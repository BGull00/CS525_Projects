{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " - Bryson Gullett\n",
    " - Robert Schaffer\n",
    " - Matthew Dixson\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import optimizers\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ebf08e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95a42",
   "metadata": {},
   "source": [
    "### Please Use Markdown\n",
    "> for markdown, see here: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2493f996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)          [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_54 (Lambda)             (None, 80)           0           ['input_55[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_108 (Embedding)      (None, 80, 256)      256000      ['input_55[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_109 (Embedding)      (None, 80, 256)      20480       ['lambda_54[0][0]']              \n",
      "                                                                                                  \n",
      " add_358 (Add)                  (None, 80, 256)      0           ['embedding_108[0][0]',          \n",
      "                                                                  'embedding_109[0][0]']          \n",
      "                                                                                                  \n",
      " multi_head_attention_152 (Mult  (None, 80, 256)     4364        ['add_358[0][0]',                \n",
      " iHeadAttention)                                                  'add_358[0][0]']                \n",
      "                                                                                                  \n",
      " add_359 (Add)                  (None, 80, 256)      0           ['multi_head_attention_152[0][0]'\n",
      "                                                                 , 'add_358[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_304 (Layer  (None, 80, 256)     512         ['add_359[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_358 (Dense)              (None, 80, 256)      65792       ['layer_normalization_304[0][0]']\n",
      "                                                                                                  \n",
      " dense_359 (Dense)              (None, 80, 256)      65792       ['dense_358[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_152 (Dropout)          (None, 80, 256)      0           ['dense_359[0][0]']              \n",
      "                                                                                                  \n",
      " add_360 (Add)                  (None, 80, 256)      0           ['dropout_152[0][0]',            \n",
      "                                                                  'layer_normalization_304[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_305 (Layer  (None, 80, 256)     512         ['add_360[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_360 (Dense)              (None, 80, 1000)     257000      ['layer_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 670,452\n",
      "Trainable params: 670,452\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, rate=0.1):\n",
    "        #initailize variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        #LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        #Use the rate variable for the dropout layers and remember to use two dense layers\n",
    "        #See assignment and its figures for more details.\n",
    "        multihead_atten = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.num_heads, dropout=self.rate)(inputs, inputs, use_causal_mask=True)\n",
    "        add1 = layers.Add()([multihead_atten, inputs])\n",
    "        layer_norm1 = layers.LayerNormalization(epsilon=1e-6)(add1)\n",
    "        dense1 = layers.Dense(self.ff_dim, activation='relu')(layer_norm1)\n",
    "        dense2 = layers.Dense(self.ff_dim, activation='relu')(dense1)\n",
    "        dropout1 = layers.Dropout(self.rate)(dense2)\n",
    "        add2 = layers.Add()([dropout1, layer_norm1])\n",
    "        layer_norm2 = layers.LayerNormalization(epsilon=1e-6)(add2)\n",
    "\n",
    "        return layer_norm2\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to enocde positions\n",
    "        #add (1) and (2) and return the layer\n",
    "        positions = layers.Lambda(lambda x: tf.multiply(tf.ones_like(x, dtype='int32'), tf.range(self.maxlen)))(inputs)\n",
    "        token_embeddings = layers.Embedding(self.vocab_size, self.embed_dim, input_length=self.maxlen)(inputs)\n",
    "        position_embeddings = layers.Embedding(self.maxlen, self.embed_dim, input_length=self.maxlen)(positions)\n",
    "\n",
    "        add = layers.Add()([token_embeddings, position_embeddings])\n",
    "        return add\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        inputs = keras.Input(shape=(self.maxlen,))\n",
    "        embedding_layer = self.EmbeddingLayer(inputs)\n",
    "        prev_layer = embedding_layer\n",
    "        for _ in range(self.num_blocks):\n",
    "            transformer_block = self.TransformerBlock(prev_layer)\n",
    "            prev_layer = transformer_block\n",
    "        final_dense = layers.Dense(self.vocab_size, activation='softmax')(prev_layer)\n",
    "        model = keras.Model(inputs=inputs, outputs=final_dense)\n",
    "        self.opt = optimizers.Adam(learning_rate=self.rate)\n",
    "        self.loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(loss=keras.losses.SparseCategoricalCrossentropy, optimizer=self.opt)\n",
    "        return model\n",
    "    \n",
    "my_model = TransformerModel(1000)\n",
    "print(my_model.create_model().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "227111a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.text = ''\n",
    "        self.len = len\n",
    "        with open(filename) as fin:\n",
    "            for line in fin:\n",
    "                self.text += line\n",
    "\n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.text = re.sub(r'[^\\w\\s]', ' ', self.text)\n",
    "        self.text = re.sub(r' +', ' ', self.text)\n",
    "        self.text = re.sub(r'â', '', self.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        self.text = self.text.split()\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.vocab = np.append(self.vocab, ['PAD'])\n",
    "        self.vocab_nums = [i for i in range(len(self.vocab))]\n",
    "        self.vocab_dict = dict(zip(self.vocab, self.vocab_nums))\n",
    "        \n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(int(len(self.text)/self.len)+1):\n",
    "            sequence_X = []\n",
    "            sequence_Y = []\n",
    "            for j in range(self.len):\n",
    "                if i*self.len+j < len(self.text):\n",
    "                    sequence_X.append(self.vocab_dict[self.text[i*self.len+j]])\n",
    "                else:\n",
    "                    sequence_X.append(self.vocab_dict['PAD'])\n",
    "                if i*self.len+j+1 < len(self.text):\n",
    "                    #y = np.zeros(len(self.vocab))\n",
    "                    #y[self.vocab_dict[self.text[i*self.len+j+1]]] = 1\n",
    "                    #y = [0 if self.vocab_dict[self.text[i*self.len+j+1]] != k else 1 for k in range(len(self.vocab))]\n",
    "                    sequence_Y.append(self.vocab_dict[self.text[i*self.len+j+1]])\n",
    "                else:\n",
    "                    #y = np.zeros(len(self.vocab))\n",
    "                    #y[self.vocab_dict['PAD']] = 1\n",
    "                    #y = [0 if self.vocab_dict['PAD'] != k else 1 for k in range(len(self.vocab))]\n",
    "                    sequence_Y.append([self.vocab_dict['PAD']])\n",
    "            X.append(sequence_X)\n",
    "            Y.append(sequence_Y)\n",
    "        return X, Y, self.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ffe1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab, vocab_dict):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100, seq_len=80):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        model_input = [self.vocab_dict['PAD'] for _ in range(seq_len)]\n",
    "        \n",
    "        #Text is the output of generate_text()\n",
    "        text = start_string\n",
    "        #Tokenize string to encode into integers\n",
    "        start_string = start_string.split()\n",
    "        #Get starting index for first word generated\n",
    "        next_index = len(start_string)\n",
    "        #Generate model input\n",
    "        for i in range(len(start_string)):\n",
    "            model_input[i] = self.vocab_dict[start_string[i]]\n",
    "        \n",
    "        #Generate text\n",
    "        for i in range(num_generate):\n",
    "            prediction = self.model.predict(np.array([model_input]), verbose=0)\n",
    "            # print(len(prediction))\n",
    "            # print(len(prediction[0]))\n",
    "            # print(len(prediction[0][0]))\n",
    "            word_index = np.argmax(prediction[0][next_index])\n",
    "            next_index += 1\n",
    "            if word_index >= len(self.vocab):\n",
    "                print('Invalid word index: ', word_index, \"Valid max word index = \", len(self.vocab))\n",
    "                word_index = self.vocab_dict['PAD']\n",
    "            if next_index < seq_len:\n",
    "                model_input[next_index] = word_index\n",
    "            else:\n",
    "                model_input.pop(0)\n",
    "                model_input.append(word_index)\n",
    "                next_index -= 1\n",
    "          \n",
    "            next_word = self.vocab[word_index]\n",
    "            text += ' ' + next_word\n",
    "            # print(model_input)\n",
    "        return text\n",
    "    \n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text = ''\n",
    "        for _ in range(num_generate):\n",
    "            word = self.vocab[rand.randrange(len(self.vocab))]\n",
    "            text += word + ' '\n",
    "            if word == 'PAD':\n",
    "                break\n",
    "        return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2de790-3a54-4723-a0f9-956d8485fb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "i read a crowd of people tail hang soft sake carathon\n",
      "here nothing sitting freely wid \n"
     ]
    }
   ],
   "source": [
    "data = DataSet('beatles.txt', 80)\n",
    "_, _, vocab = data.create_dataset()\n",
    "transformer = TransformerModel(len(vocab))\n",
    "model = transformer.create_model()\n",
    "text = GenerateText(model, vocab, data.vocab_dict)\n",
    "out = text.generate_text(\"i read a crowd of people\", num_generate=5)\n",
    "print(out)\n",
    "out = text.generate_random_text(num_generate=5)\n",
    "print(out)\n",
    "# print(len(out.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "# Used Keras' example on how to write a training loop\n",
    "# https://keras.io/guides/writing_a_training_loop_from_scratch\n",
    "def train_model(model, vocab, X, Y, epochs=50):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(X, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(Y, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "       \n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "        # model.fit(x,y)\n",
    "        text = GenerateText(model, vocab, data.vocab_dict)\n",
    "        out = text.generate_random_text(num_generate=10)\n",
    "        print(f\"Epoch {i}\\n\\tloss: {loss_value}\\n\\tText: {out}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "021a66e6",
   "metadata": {},
   "source": [
    "## Setup input data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb1fa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataSet('beatles.txt', 80)\n",
    "x, y, vocab = data.create_dataset()\n",
    "X = []\n",
    "for e in x:\n",
    "    X.append(np.asarray(e).astype(np.int64))\n",
    "X = np.asarray(X)\n",
    "\n",
    "# Some elements of y are lists of size 1, convert to int\n",
    "for i in range(452):\n",
    "    for j in range(80):\n",
    "        if isinstance(y[i][j], list):\n",
    "            y[i][j] = y[i][j][0]\n",
    "\n",
    "Y = np.asarray(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d4d35d1",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9162f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Documents\\notes\\525\\CS525_Projects\\.venv\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tloss: 7.912838935852051\n",
      "\tText: quit faster memories lasted bei butted latches annoyed eht sang \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x24c85caf3d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model1 = transformer.create_model()\n",
    "train_model(model1, vocab, X, Y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33a35877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Documents\\notes\\525\\CS525_Projects\\.venv\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tloss: 7.910845756530762\n",
      "\tText: starts choking died bride wonder wrote obscene bb friends nimmst \n",
      "Epoch 1\n",
      "\tloss: 7.70962381362915\n",
      "\tText: hendersons wear al drinkin modern deny wings single hilton saying \n",
      "Epoch 2\n",
      "\tloss: 7.502559661865234\n",
      "\tText: matter taxman cloud beneath barber hat brand thing charity right \n",
      "Epoch 3\n",
      "\tloss: 7.278773307800293\n",
      "\tText: lasts chance ohh bei diamonds oscar noticed many trim too \n",
      "Epoch 4\n",
      "\tloss: 7.0340576171875\n",
      "\tText: cake jamboree mao strawberry killer tides jar grin country disappointment \n",
      "Epoch 5\n",
      "\tloss: 6.771049499511719\n",
      "\tText: savoy fool ah sgt inquire towering sleeps played lasts runs \n",
      "Epoch 6\n",
      "\tloss: 6.498688220977783\n",
      "\tText: live lear maybe short girlfriend hammer part skirts mending grass \n",
      "Epoch 7\n",
      "\tloss: 6.23117208480835\n",
      "\tText: junior corporation delay water feeling repeat miss forgotten sweat having \n",
      "Epoch 8\n",
      "\tloss: 5.9867401123046875\n",
      "\tText: bible yeh spent anyone tho darlin greet trade sho joke \n",
      "Epoch 9\n",
      "\tloss: 5.781358242034912\n",
      "\tText: old believing somewhere ends ensemble screw way misery pum walrus \n",
      "Epoch 10\n",
      "\tloss: 5.6239166259765625\n",
      "\tText: acquainted held re behind cicce possessions fooling share tres pretty \n",
      "Epoch 11\n",
      "\tloss: 5.510250091552734\n",
      "\tText: audience ringing game giving manage military meet tracks cat special \n",
      "Epoch 12\n",
      "\tloss: 5.4274420738220215\n",
      "\tText: oceanchild dirt knickers tracks mean t cool monday paramucho childrens \n",
      "Epoch 13\n",
      "\tloss: 5.360130310058594\n",
      "\tText: as tan none fighting wusste backed not horses put who \n",
      "Epoch 14\n",
      "\tloss: 5.2956862449646\n",
      "\tText: sax white possessions nighttime behave silently bigger hoo before ballad \n",
      "Epoch 15\n",
      "\tloss: 5.224884033203125\n",
      "\tText: bop p one father wild a sax guess clinging isle \n",
      "Epoch 16\n",
      "\tloss: 5.14393424987793\n",
      "\tText: jones rolling threw 3 lonely california sits vera days evolution \n",
      "Epoch 17\n",
      "\tloss: 5.052042484283447\n",
      "\tText: road doctor haven breaking trail 9 zu honey moonlight canary \n",
      "Epoch 18\n",
      "\tloss: 4.952402591705322\n",
      "\tText: closing ridin sweeping slumbers letter big slowly cut cicce bit \n",
      "Epoch 19\n",
      "\tloss: 4.850385665893555\n",
      "\tText: portrait sometimes ourselves stream puts lime newspapers eiffel piggy raincoats \n",
      "Epoch 20\n",
      "\tloss: 4.752315521240234\n",
      "\tText: van hammer mckenzie kindly replace keeps meaningless determined taxman earn \n",
      "Epoch 21\n",
      "\tloss: 4.66228723526001\n",
      "\tText: chuck water together strawberry beautiful sergeant n steal ring and \n",
      "Epoch 22\n",
      "\tloss: 4.580846309661865\n",
      "\tText: ya appointment hold burst roam best liebt young everywhere hay \n",
      "Epoch 23\n",
      "\tloss: 4.505717754364014\n",
      "\tText: station spoil parking reprise story sh room pies known away \n",
      "Epoch 24\n",
      "\tloss: 4.433038711547852\n",
      "\tText: feeling fragrant stare eyes brand uh hab awake fare for \n",
      "Epoch 25\n",
      "\tloss: 4.360690593719482\n",
      "\tText: new missing tear note vain sin hoop put yi dich \n",
      "Epoch 26\n",
      "\tloss: 4.288003444671631\n",
      "\tText: hourglass low filled word answer tenderly miss marshmellow hear flowing \n",
      "Epoch 27\n",
      "\tloss: 4.215482234954834\n",
      "\tText: year reprise monday motor birds wanted summersets inner sigh surrender \n",
      "Epoch 28\n",
      "\tloss: 4.144730091094971\n",
      "\tText: caught beauty melting just cooking rob locked tart coca spinnin \n",
      "Epoch 29\n",
      "\tloss: 4.076777935028076\n",
      "\tText: drive savoy duty unhappy wonder valentine luck sounds fussing peaches \n",
      "Epoch 30\n",
      "\tloss: 4.012264251708984\n",
      "\tText: job vanish submarine card plain surprise ours treatin possessing sorry \n",
      "Epoch 31\n",
      "\tloss: 3.951326608657837\n",
      "\tText: one feel arizona naaa soft boom imperfect hela quite world \n",
      "Epoch 32\n",
      "\tloss: 3.8934056758880615\n",
      "\tText: verstand maybe day liverpool treat suburban closed sh play fun \n",
      "Epoch 33\n",
      "\tloss: 3.838711738586426\n",
      "\tText: missed much fix northern robert below oan land oan fields \n",
      "Epoch 34\n",
      "\tloss: 3.7864933013916016\n",
      "\tText: could unfold self glimpse loved ah seeing nice gehen michelle \n",
      "Epoch 35\n",
      "\tloss: 3.7367589473724365\n",
      "\tText: suns faces hollywood forks living pick kind wrote determined heard \n",
      "Epoch 36\n",
      "\tloss: 3.6894798278808594\n",
      "\tText: mirrors gimme sung tremember place lying grass resting voice cup \n",
      "Epoch 37\n",
      "\tloss: 3.6441807746887207\n",
      "\tText: free wouldn loud knowing rest hey crowd froh from indicate \n",
      "Epoch 38\n",
      "\tloss: 3.6011385917663574\n",
      "\tText: skip themselves bootlace every north calling work joan prudence crabalocker \n",
      "Epoch 39\n",
      "\tloss: 3.5600500106811523\n",
      "\tText: number calling girlfriend film 8 planned quando celebrated gib quite \n",
      "Epoch 40\n",
      "\tloss: 3.520803451538086\n",
      "\tText: beatin joker lazy laugh named grin melting fly soap earned \n",
      "Epoch 41\n",
      "\tloss: 3.483356475830078\n",
      "\tText: mist ok sincere knowing cicce break while short share miles \n",
      "Epoch 42\n",
      "\tloss: 3.4475879669189453\n",
      "\tText: phone train exactly marvel sold summer smiles fill earned eye \n",
      "Epoch 43\n",
      "\tloss: 3.413656711578369\n",
      "\tText: understands huh listening first clown every raleigh solitude madly hewr \n",
      "Epoch 44\n",
      "\tloss: 3.381136178970337\n",
      "\tText: got bells amore southampton she carat magic duke deep light \n",
      "Epoch 45\n",
      "\tloss: 3.3501205444335938\n",
      "\tText: drank sweat pay lightning liebt shining getter country treasure would \n",
      "Epoch 46\n",
      "\tloss: 3.319983720779419\n",
      "\tText: no girls danny tides chuck solution pleasure anyway by nay \n",
      "Epoch 47\n",
      "\tloss: 3.290924549102783\n",
      "\tText: madam begged doctor tricks ago turnstyle float lock seconds grass \n",
      "Epoch 48\n",
      "\tloss: 3.262723922729492\n",
      "\tText: saved lacking more helps free feeling solltest summersets naaa bbbbb \n",
      "Epoch 49\n",
      "\tloss: 3.2357068061828613\n",
      "\tText: lived collapsed spain glimpse christ dream cracker turned travelling best \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x24c76fd3c40>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model50 = transformer.create_model()\n",
    "train_model(model50, vocab, X, Y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9382e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Documents\\notes\\525\\CS525_Projects\\.venv\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tloss: 7.927589416503906\n",
      "\tText: whacking magic mad his shines ballad h oan key deine \n",
      "Epoch 1\n",
      "\tloss: 7.720039367675781\n",
      "\tText: guess mystery one faces ginger habit songs pornographic their ticket \n",
      "Epoch 2\n",
      "\tloss: 7.510406970977783\n",
      "\tText: eating void slither snide spending waits count overnight piggies gear \n",
      "Epoch 3\n",
      "\tloss: 7.287895679473877\n",
      "\tText: doctor nowhere tasting than pepper oan moving 8 certainly spoil \n",
      "Epoch 4\n",
      "\tloss: 7.047283172607422\n",
      "\tText: clothes off job desmond nanananaaa letters fierce stone hog compares \n",
      "Epoch 5\n",
      "\tloss: 6.790369987487793\n",
      "\tText: deep night trouble talking max ba movement named front ahead \n",
      "Epoch 6\n",
      "\tloss: 6.526035308837891\n",
      "\tText: is arise kitchen pleasure show slowly hard bells acts touched \n",
      "Epoch 7\n",
      "\tloss: 6.268245697021484\n",
      "\tText: ready symphony people tube bungalow photograph cause anyway star playin \n",
      "Epoch 8\n",
      "\tloss: 6.033290386199951\n",
      "\tText: forgive junior rything hendersons aren shown einer bien fool oceanchild \n",
      "Epoch 9\n",
      "\tloss: 5.834432601928711\n",
      "\tText: hair red ridin asleep rug marigold realize nun rainy his \n",
      "Epoch 10\n",
      "\tloss: 5.6771368980407715\n",
      "\tText: molly coca saying lear grow faster winding unfair mean clue \n",
      "Epoch 11\n",
      "\tloss: 5.558474540710449\n",
      "\tText: wusste dirty symphony customer lazy more sit declare couple crying \n",
      "Epoch 12\n",
      "\tloss: 5.469470977783203\n",
      "\tText: diamonds suburban throws flirt sailed prudence woman late sat joan \n",
      "Epoch 13\n",
      "\tloss: 5.397334098815918\n",
      "\tText: came hurt them eagle military babe da fountain sally your \n",
      "Epoch 14\n",
      "\tloss: 5.329227447509766\n",
      "\tText: gown sounds dose playing shoulders bist horse tried slept missed \n",
      "Epoch 15\n",
      "\tloss: 5.254952907562256\n",
      "\tText: yes PAD \n",
      "Epoch 16\n",
      "\tloss: 5.16913366317749\n",
      "\tText: cause d finds jay meadows les rich acquainted hill holiday \n",
      "Epoch 17\n",
      "\tloss: 5.0718560218811035\n",
      "\tText: toe grandchildren unless bathroom worse teacher affection afternoon alerted aaaah \n",
      "Epoch 18\n",
      "\tloss: 4.96692419052124\n",
      "\tText: spend ate mine birds same out puts noticed past glimmering \n",
      "Epoch 19\n",
      "\tloss: 4.860507965087891\n",
      "\tText: mi are nurse count penny slow another glimpse moonlight whoah \n",
      "Epoch 20\n",
      "\tloss: 4.759275913238525\n",
      "\tText: photograph nothin truffle tracks store certainly john wine ol warning \n",
      "Epoch 21\n",
      "\tloss: 4.667078971862793\n",
      "\tText: these sin good wild patiently doris wife rockin breaking coca \n",
      "Epoch 22\n",
      "\tloss: 4.5846381187438965\n",
      "\tText: shoot laugh lived loud dove course shelf freely ourselves closer \n",
      "Epoch 23\n",
      "\tloss: 4.509365558624268\n",
      "\tText: mooning old vanish controlled lonely 6 floor sight lives built \n",
      "Epoch 24\n",
      "\tloss: 4.437147617340088\n",
      "\tText: shines inquire discovered gideon bullfrog teen invitation compares hi drew \n",
      "Epoch 25\n",
      "\tloss: 4.364691734313965\n",
      "\tText: happinness smoke walked obscene green older sergeant pulled drew thick \n",
      "Epoch 26\n",
      "\tloss: 4.291043281555176\n",
      "\tText: fish tantalize stepping revolution piece desert reach deliver doesn skip \n",
      "Epoch 27\n",
      "\tloss: 4.216854095458984\n",
      "\tText: bible tree meaning ought everyday challenge though dawn piece walk \n",
      "Epoch 28\n",
      "\tloss: 4.143906593322754\n",
      "\tText: rose nanananaaa hill make strawberry forgive childrens billy duty bring \n",
      "Epoch 29\n",
      "\tloss: 4.074206829071045\n",
      "\tText: falling public mayayake robbing lasts 2 again smoke valentine daniel \n",
      "Epoch 30\n",
      "\tloss: 4.008603096008301\n",
      "\tText: naughty moon first jojo vienna imagine someday wiping together knows \n",
      "Epoch 31\n",
      "\tloss: 3.9472224712371826\n",
      "\tText: quarter station flowers danced tight lots dove doesn hot want \n",
      "Epoch 32\n",
      "\tloss: 3.8893563747406006\n",
      "\tText: saxon teacher marry bedroom we warning north coat existence 17 \n",
      "Epoch 33\n",
      "\tloss: 3.8341727256774902\n",
      "\tText: wond pass naaa inner warnin shady annoyed rob glass middle \n",
      "Epoch 34\n",
      "\tloss: 3.781078815460205\n",
      "\tText: splendid lingers made smokers ryone shadow orange majesty acorns doesn \n",
      "Epoch 35\n",
      "\tloss: 3.7301883697509766\n",
      "\tText: walked desert hoop hung ow because misses bible flows eat \n",
      "Epoch 36\n",
      "\tloss: 3.6815011501312256\n",
      "\tText: kave photograph older pulled unfold understand hog elephants message mornin \n",
      "Epoch 37\n",
      "\tloss: 3.63552188873291\n",
      "\tText: obla nature dressing heat sunny fears doris whole winding game \n",
      "Epoch 38\n",
      "\tloss: 3.5923430919647217\n",
      "\tText: learning thinking postcards hour somewhere inspiration poppies gum number change \n",
      "Epoch 39\n",
      "\tloss: 3.5516531467437744\n",
      "\tText: fbi calling dressing seen ene looking together gotta march am \n",
      "Epoch 40\n",
      "\tloss: 3.513270854949951\n",
      "\tText: sincere none sho likes bells leave husband wisdom glaubst lazy \n",
      "Epoch 41\n",
      "\tloss: 3.476231575012207\n",
      "\tText: ma kids superior supposed dann welcome peter lullabye kircaldy bible \n",
      "Epoch 42\n",
      "\tloss: 3.4406588077545166\n",
      "\tText: scarlet cos pennies having speak england folks carathon judge jackboots \n",
      "Epoch 43\n",
      "\tloss: 3.4060745239257812\n",
      "\tText: showdown saving girlfriend sold wine looked house woldn luck problems \n",
      "Epoch 44\n",
      "\tloss: 3.372921943664551\n",
      "\tText: raccoon understand me spoon crawling rent hour drift paramucho trivialities \n",
      "Epoch 45\n",
      "\tloss: 3.3411409854888916\n",
      "\tText: told robbin wouldn quarter flirt course policemen grin moments just \n",
      "Epoch 46\n",
      "\tloss: 3.310877799987793\n",
      "\tText: beside ensemble hankerchief park pink holding helps spent beggin too \n",
      "Epoch 47\n",
      "\tloss: 3.281883478164673\n",
      "\tText: down pretend local sorrow bin spoon yet already party breakfast \n",
      "Epoch 48\n",
      "\tloss: 3.253870725631714\n",
      "\tText: savoy om keep lock understood wondering fast changing appointment discreetly \n",
      "Epoch 49\n",
      "\tloss: 3.2268550395965576\n",
      "\tText: return thursday ways bought watchin summer cha weaving middle man \n",
      "Epoch 50\n",
      "\tloss: 3.2005367279052734\n",
      "\tText: nighttime could keep ceiling surprise bbc welcome greatest blues jackboots \n",
      "Epoch 51\n",
      "\tloss: 3.1751770973205566\n",
      "\tText: bigger teen fish cap street happiness message shall turned self \n",
      "Epoch 52\n",
      "\tloss: 3.1507070064544678\n",
      "\tText: melody different tight ok proceeded everybody lived fiddle played ask \n",
      "Epoch 53\n",
      "\tloss: 3.1270205974578857\n",
      "\tText: badly six assure inspiration jones unkind burning closing billy polythene \n",
      "Epoch 54\n",
      "\tloss: 3.104038953781128\n",
      "\tText: portrait suburban street when use dying appointment r grabbed rich \n",
      "Epoch 55\n",
      "\tloss: 3.08151912689209\n",
      "\tText: peep introduce good comes matchbox feeling crime days street bacon \n",
      "Epoch 56\n",
      "\tloss: 3.059859275817871\n",
      "\tText: crabalocker floor inverted stands meanwhile entschuldigst school screw um faces \n",
      "Epoch 57\n",
      "\tloss: 3.0384769439697266\n",
      "\tText: cruel komm knife top knives quit around much stared shore \n",
      "Epoch 58\n",
      "\tloss: 3.0178802013397217\n",
      "\tText: fed begins road earth note learn worries much sein its \n",
      "Epoch 59\n",
      "\tloss: 2.9977214336395264\n",
      "\tText: schoner marmalade while nobody losing yard happened fare keeping maybe \n",
      "Epoch 60\n",
      "\tloss: 2.9779040813446045\n",
      "\tText: eternally mornin dry performs ease fiddle recall montelimat aaaah blindly \n",
      "Epoch 61\n",
      "\tloss: 2.9584875106811523\n",
      "\tText: gas sending wond grin rise yet driver bright wouldn northern \n",
      "Epoch 62\n",
      "\tloss: 2.9393882751464844\n",
      "\tText: end school searchin thinking lagoon sea ours quietly cia shoes \n",
      "Epoch 63\n",
      "\tloss: 2.9206507205963135\n",
      "\tText: like jewellers sincere yeht dancer love bridge twenty stick recall \n",
      "Epoch 64\n",
      "\tloss: 2.9023783206939697\n",
      "\tText: beggin could cry jungle hung sea gumboot to weather grow \n",
      "Epoch 65\n",
      "\tloss: 2.8843002319335938\n",
      "\tText: each liverpool lingers laughing metaphysical sign am cool fi obscene \n",
      "Epoch 66\n",
      "\tloss: 2.866525411605835\n",
      "\tText: waters sgt lit sung stepping sometimes eggman rather sixty road \n",
      "Epoch 67\n",
      "\tloss: 2.849020481109619\n",
      "\tText: rectify jar sympathize buried cream freu instead jackboots billy lot \n",
      "Epoch 68\n",
      "\tloss: 2.8315322399139404\n",
      "\tText: fears named american buried style discreetly shouts partner tuesday southampton \n",
      "Epoch 69\n",
      "\tloss: 2.8141987323760986\n",
      "\tText: lead earn letters message liebt hilton clue style returning seat \n",
      "Epoch 70\n",
      "\tloss: 2.79740047454834\n",
      "\tText: majesty beethoven dressing money evermore lead imagine close burst comfort \n",
      "Epoch 71\n",
      "\tloss: 2.7804818153381348\n",
      "\tText: famous va sit coming meanwhile bath plan boys would lazy \n",
      "Epoch 72\n",
      "\tloss: 2.763956308364868\n",
      "\tText: ran rocking often celebrate screw mighty wing heat national carousel \n",
      "Epoch 73\n",
      "\tloss: 2.747629404067993\n",
      "\tText: doubt perverted we sin va parking grabbed yeah yeh staring \n",
      "Epoch 74\n",
      "\tloss: 2.731334686279297\n",
      "\tText: pilchard spend shows below wait board paul float shelf needed \n",
      "Epoch 75\n",
      "\tloss: 2.7152459621429443\n",
      "\tText: windy getter so ear medicine sweeter imprisoned rita fill faded \n",
      "Epoch 76\n",
      "\tloss: 2.699254274368286\n",
      "\tText: madly denis gonna reach worth yeh nights dich voice homing \n",
      "Epoch 77\n",
      "\tloss: 2.683345079421997\n",
      "\tText: indicate golden warst queen opaque canary fed success wing ready \n",
      "Epoch 78\n",
      "\tloss: 2.6676392555236816\n",
      "\tText: mac wet dancin hula boy eht kitchen nation command drinkin \n",
      "Epoch 79\n",
      "\tloss: 2.652081251144409\n",
      "\tText: prized care across verzeiht moments invitation creeps ways ok telephone \n",
      "Epoch 80\n",
      "\tloss: 2.63666033744812\n",
      "\tText: barber unkind dizzy way lips piano amore telephone semoc multicoloured \n",
      "Epoch 81\n",
      "\tloss: 2.6212117671966553\n",
      "\tText: turned sha submarine unkind tres done snide sooner temperature attractively \n",
      "Epoch 82\n",
      "\tloss: 2.605879545211792\n",
      "\tText: woman wants wonder lightning soon won takes diamonds house hollywood \n",
      "Epoch 83\n",
      "\tloss: 2.5907583236694336\n",
      "\tText: multicoloured heavy vienna bridge true happy insane scratch silent wait \n",
      "Epoch 84\n",
      "\tloss: 2.5754356384277344\n",
      "\tText: appreciate shuop west boyfriend hab parted meaning cloud telephone tell \n",
      "Epoch 85\n",
      "\tloss: 2.5606887340545654\n",
      "\tText: crabalocker spoil que introduce illusion an matchbox isn always called \n",
      "Epoch 86\n",
      "\tloss: 2.5457751750946045\n",
      "\tText: wouldn everything duty vienna style hab give shoeshine know va \n",
      "Epoch 87\n",
      "\tloss: 2.5309739112854004\n",
      "\tText: ago audience ignorance frown hog line screen new foolin warm \n",
      "Epoch 88\n",
      "\tloss: 2.516364812850952\n",
      "\tText: heel our named expert smiles finally sometimes 3 spread yeht \n",
      "Epoch 89\n",
      "\tloss: 2.5017757415771484\n",
      "\tText: movin trail 15 match eh shouts thousand den lords unkind \n",
      "Epoch 90\n",
      "\tloss: 2.4873695373535156\n",
      "\tText: comes shoot hour blew edison location take blame questo makes \n",
      "Epoch 91\n",
      "\tloss: 2.4726691246032715\n",
      "\tText: stealing inquire driver shelf hendersons ought burns nicht stupid wore \n",
      "Epoch 92\n",
      "\tloss: 2.45839786529541\n",
      "\tText: trade rains cast son daddy preparation putting touch brown lifting \n",
      "Epoch 93\n",
      "\tloss: 2.4442737102508545\n",
      "\tText: sympathize mi featuring kindness steal ee pulled nie disappear postcard \n",
      "Epoch 94\n",
      "\tloss: 2.429964542388916\n",
      "\tText: beginning greet raincoats couple dreadful ceiling chances secret lie canite \n",
      "Epoch 95\n",
      "\tloss: 2.4159274101257324\n",
      "\tText: find huh days spinal snide chance submarine bells shelter bring \n",
      "Epoch 96\n",
      "\tloss: 2.402017593383789\n",
      "\tText: lifting obscene breaks picture pies mean what stairs first soul \n",
      "Epoch 97\n",
      "\tloss: 2.3879153728485107\n",
      "\tText: buried undertake evolution harm lancashire loved smile preparation rybody deceive \n",
      "Epoch 98\n",
      "\tloss: 2.3739378452301025\n",
      "\tText: moving kitchen playroom lip nature worm green farm sucks ry \n",
      "Epoch 99\n",
      "\tloss: 2.360086441040039\n",
      "\tText: breakfast silent long destruction digging girls blackburn glow lied leaves \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x24c93d266a0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model100 = transformer.create_model()\n",
    "train_model(model100, vocab, X, Y, epochs=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09e5780f",
   "metadata": {},
   "source": [
    "## Overtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerModel(len(vocab))\n",
    "model500 = transformer.create_model()\n",
    "train_model(model500, vocab, X, Y, epochs=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fcd9aad",
   "metadata": {},
   "source": [
    "## Test with different starting phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "566cebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: a day in the life\n",
      "\t1 Epoch: died note monkey celebrate corner hurry bellyful singer sky backdoor \n",
      "\t50 Epochs: bedroom wonder drinkin penetrate drive ould warnin asking madly shoe \n",
      "\t100 Epochs: wave married rent forgotten mustard lagoon oh says al postcard \n",
      "Phrase: hello world\n",
      "\t1 Epoch: shoot let cows eyes plat vienna wink yellow homeward forget \n",
      "\t50 Epochs: friend walk hills soul seems being ll queue biding cicce \n",
      "\t100 Epochs: busy stars teen 1 then nothin tuesday yet strange watching \n",
      "Phrase: la la la\n",
      "\t1 Epoch: jonah used equipped denis a walrus almost three blue flies \n",
      "\t50 Epochs: joke cannot lit stairs di bootlace clowns habit returned ground \n",
      "\t100 Epochs: already writing slaggers singing poop low winging roller doch sleepy \n"
     ]
    }
   ],
   "source": [
    "\n",
    "phrases = [\n",
    "    \"a day in the life\",\n",
    "    \"hello world\",\n",
    "    # \"I\",\n",
    "    \"la la la\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    text1 = GenerateText(model1, vocab, data.vocab_dict)\n",
    "    out1 = text1.generate_random_text(num_generate=10)\n",
    "    \n",
    "    text50 = GenerateText(model50, vocab, data.vocab_dict)\n",
    "    out50 = text50.generate_random_text(num_generate=10)\n",
    "    \n",
    "    text100 = GenerateText(model50, vocab, data.vocab_dict)\n",
    "    out100 = text100.generate_random_text(num_generate=10)    \n",
    "    print(f\"Phrase: {phrase}\")\n",
    "    print(f\"\\t1 Epoch: {out1}\")\n",
    "    print(f\"\\t50 Epochs: {out50}\")\n",
    "    print(f\"\\t100 Epochs: {out100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e23b663014ca37592679400f38d6d04e3fc85e5c6f63651f80341cade9e63d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
