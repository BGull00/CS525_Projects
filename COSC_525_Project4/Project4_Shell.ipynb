{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " - Bryson Gullett\n",
    " - Robert Schaffer\n",
    " - Matthew Dixson\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import optimizers\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ebf08e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95a42",
   "metadata": {},
   "source": [
    "### Please Use Markdown\n",
    "> for markdown, see here: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2493f996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 80)           0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 80, 256)      256000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 80, 256)      20480       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 80, 256)      0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 80, 256)     4364        ['add[0][0]',                    \n",
      " dAttention)                                                      'add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 80, 256)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 80, 256)     512         ['add_1[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 80, 256)      65792       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 80, 256)      65792       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 80, 256)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 80, 256)      0           ['dropout[0][0]',                \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 80, 256)     512         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 80, 1000)     257000      ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 670,452\n",
      "Trainable params: 670,452\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, rate=0.1):\n",
    "        #initailize variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        #LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        #Use the rate variable for the dropout layers and remember to use two dense layers\n",
    "        #See assignment and its figures for more details.\n",
    "        multihead_atten = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.num_heads, dropout=self.rate)(inputs, inputs, use_causal_mask=True)\n",
    "        add1 = layers.Add()([multihead_atten, inputs])\n",
    "        layer_norm1 = layers.LayerNormalization(epsilon=1e-6)(add1)\n",
    "        dense1 = layers.Dense(self.ff_dim, activation='relu')(layer_norm1)\n",
    "        dense2 = layers.Dense(self.ff_dim, activation='relu')(dense1)\n",
    "        dropout1 = layers.Dropout(self.rate)(dense2)\n",
    "        add2 = layers.Add()([dropout1, layer_norm1])\n",
    "        layer_norm2 = layers.LayerNormalization(epsilon=1e-6)(add2)\n",
    "\n",
    "        return layer_norm2\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to enocde positions\n",
    "        #add (1) and (2) and return the layer\n",
    "        positions = layers.Lambda(lambda x: tf.multiply(tf.ones_like(x, dtype='int32'), tf.range(self.maxlen)))(inputs)\n",
    "        token_embeddings = layers.Embedding(self.vocab_size, self.embed_dim, input_length=self.maxlen)(inputs)\n",
    "        position_embeddings = layers.Embedding(self.maxlen, self.embed_dim, input_length=self.maxlen)(positions)\n",
    "\n",
    "        add = layers.Add()([token_embeddings, position_embeddings])\n",
    "        return add\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        inputs = keras.Input(shape=(self.maxlen,))\n",
    "        embedding_layer = self.EmbeddingLayer(inputs)\n",
    "        prev_layer = embedding_layer\n",
    "        for _ in range(self.num_blocks):\n",
    "            transformer_block = self.TransformerBlock(prev_layer)\n",
    "            prev_layer = transformer_block\n",
    "        final_dense = layers.Dense(self.vocab_size, activation='softmax')(prev_layer)\n",
    "        model = keras.Model(inputs=inputs, outputs=final_dense)\n",
    "        self.opt = optimizers.Adam(learning_rate=self.rate)\n",
    "        self.loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(loss=keras.losses.SparseCategoricalCrossentropy, optimizer=self.opt)\n",
    "        return model\n",
    "    \n",
    "my_model = TransformerModel(1000)\n",
    "print(my_model.create_model().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227111a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.text = ''\n",
    "        self.len = len\n",
    "        with open(filename) as fin:\n",
    "            for line in fin:\n",
    "                self.text += line\n",
    "\n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.text = re.sub(r'[^\\w\\s]', ' ', self.text)\n",
    "        self.text = re.sub(r' +', ' ', self.text)\n",
    "        self.text = re.sub(r'â', '', self.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        self.text = self.text.split()\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.vocab = np.append(self.vocab, ['PAD'])\n",
    "        self.vocab_nums = [i for i in range(len(self.vocab))]\n",
    "        self.vocab_dict = dict(zip(self.vocab, self.vocab_nums))\n",
    "        \n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(int(len(self.text)/self.len)):\n",
    "            sequence_X = []\n",
    "            sequence_Y = []\n",
    "            for j in range(self.len):\n",
    "                if i*self.len+j < len(self.text):\n",
    "                    sequence_X.append(self.vocab_dict[self.text[i*self.len+j]])\n",
    "                else:\n",
    "                    pass\n",
    "                    #sequence_X.append(self.vocab_dict['PAD'])\n",
    "                if i*self.len+j+1 < len(self.text):\n",
    "                    #y = np.zeros(len(self.vocab))\n",
    "                    #y[self.vocab_dict[self.text[i*self.len+j+1]]] = 1\n",
    "                    #y = [0 if self.vocab_dict[self.text[i*self.len+j+1]] != k else 1 for k in range(len(self.vocab))]\n",
    "                    sequence_Y.append(self.vocab_dict[self.text[i*self.len+j+1]])\n",
    "                else:\n",
    "                    #y = np.zeros(len(self.vocab))\n",
    "                    #y[self.vocab_dict['PAD']] = 1\n",
    "                    #y = [0 if self.vocab_dict['PAD'] != k else 1 for k in range(len(self.vocab))]\n",
    "                    #sequence_Y.append([self.vocab_dict['PAD']])\n",
    "                    pass\n",
    "            X.append(sequence_X)\n",
    "            Y.append(sequence_Y)\n",
    "        return X, Y, self.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffe1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab, vocab_dict):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100, seq_len=80):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        model_input = [self.vocab_dict['PAD'] for _ in range(seq_len)]\n",
    "        \n",
    "        #Text is the output of generate_text()\n",
    "        text = start_string\n",
    "        #Tokenize string to encode into integers\n",
    "        start_string = start_string.split()\n",
    "        #Get starting index for first word generated\n",
    "        next_index = len(start_string)\n",
    "        #Generate model input\n",
    "        for i in range(len(start_string)):\n",
    "            model_input[i] = self.vocab_dict[start_string[i]]\n",
    "        \n",
    "        #Generate text\n",
    "        for i in range(num_generate):\n",
    "            prediction = self.model.predict(np.array([model_input]), verbose=0)\n",
    "            # print(len(prediction))\n",
    "            # print(len(prediction[0]))\n",
    "            # print(len(prediction[0][0]))\n",
    "            word_index = np.argmax(prediction[0][next_index])\n",
    "            next_index += 1\n",
    "            if word_index >= len(self.vocab):\n",
    "                print('Invalid word index: ', word_index, \"Valid max word index = \", len(self.vocab))\n",
    "                word_index = self.vocab_dict['PAD']\n",
    "            if next_index < seq_len:\n",
    "                model_input[next_index] = word_index\n",
    "            else:\n",
    "                model_input.pop(0)\n",
    "                model_input.append(word_index)\n",
    "                next_index -= 1\n",
    "          \n",
    "            next_word = self.vocab[word_index]\n",
    "            text += ' ' + next_word\n",
    "            # print(model_input)\n",
    "        return text\n",
    "    \n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text = ''\n",
    "        for _ in range(num_generate):\n",
    "            word = self.vocab[rand.randrange(len(self.vocab))]\n",
    "            text += word + ' '\n",
    "            if word == 'PAD':\n",
    "                break\n",
    "        return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2de790-3a54-4723-a0f9-956d8485fb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read a crowd of people edgar anymore eagle carry you\n",
      "holiday most playin jay friend \n"
     ]
    }
   ],
   "source": [
    "data = DataSet('beatles.txt', 80)\n",
    "_, _, vocab = data.create_dataset()\n",
    "transformer = TransformerModel(len(vocab))\n",
    "model = transformer.create_model()\n",
    "text = GenerateText(model, vocab, data.vocab_dict)\n",
    "out = text.generate_text(\"i read a crowd of people\", num_generate=5)\n",
    "print(out)\n",
    "out = text.generate_random_text(num_generate=5)\n",
    "print(out)\n",
    "# print(len(out.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b59dd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "# Used Keras' example on how to write a training loop\n",
    "# https://keras.io/guides/writing_a_training_loop_from_scratch\n",
    "def train_model(model, vocab, X, Y, epochs=50):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(X, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(Y, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "       \n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "        # model.fit(x,y)\n",
    "        text = GenerateText(model, vocab, data.vocab_dict)\n",
    "        out = text.generate_text('hello world', 50)\n",
    "        #out = text.generate_random_text(num_generate=10)\n",
    "        print(f\"Epoch {i}\\n\\tloss: {loss_value}\\n\\tText: {out}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a66e6",
   "metadata": {},
   "source": [
    "## Setup input data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1fa030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = DataSet('beatles.txt', 80)\n",
    "x, y, vocab = data.create_dataset()\n",
    "X = []\n",
    "for e in x:\n",
    "    X.append(np.asarray(e).astype(np.int64))\n",
    "X = np.asarray(X)\n",
    "\n",
    "# Some elements of y are lists of size 1, convert to int\n",
    "for i in range(451):\n",
    "    for j in range(80):\n",
    "        if isinstance(y[i][j], list):\n",
    "            y[i][j] = y[i][j][0]\n",
    "\n",
    "Y = np.asarray(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d35d1",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9162f6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\School\\UTK_SPRING_2023\\Deep_Learning\\cs425-Env\\Lib\\site-packages\\keras\\backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tloss: 7.928467750549316\n",
      "\tText: hello world tear cares seven spinnin fly needs questo sits aaaaaahhhhhh finds wine cup almost strange king lady vanish come takes ears spinnin yard k fighting heavy asleep flat southampton lotta come buried darn bags king breakfast discreetly sung where alerted newspapers cola discreetly getting fill blind letter hog comfort dinner as\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1339b45e4d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model1 = transformer.create_model()\n",
    "train_model(model1, vocab, X, Y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a35877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tloss: 7.935318946838379\n",
      "\tText: hello world starched helping such kept ya joan mon drive worked resign saw faces discreetly higher you said forgotten arms peter kind out styes you bin ich change sail mundo parted could surely girlfriend u licks deeper who silent here reply lear wings mother lips spinal talked hab butterflies noticed six carve\n",
      "Epoch 1\n",
      "\tloss: 7.726151943206787\n",
      "\tText: hello world back nose song obladi pies swim views thrill bundle upset peanuts faces from of you said forgotten arms peter round out styes you help to oh to oh speaking public i a kiss still l who i my road you still arriving knives morning tall you re voices mojo leisure\n",
      "Epoch 2\n",
      "\tloss: 7.513859748840332\n",
      "\tText: hello world back in i saw when i at becomes looked up na waves kiss yes you yes you still look so something i a our einer change i a problems i i could does people hung i i my you i a PAD back had a paperback other will i m\n",
      "Epoch 3\n",
      "\tloss: 7.288165092468262\n",
      "\tText: hello world back in i m i i m you i a i at i my you i m i a long out a i m in i to oh to work i could i a i a i my you i a i m i m you re it i m\n",
      "Epoch 4\n",
      "\tloss: 7.043977737426758\n",
      "\tText: hello world back i m i i i m i m i a i m i a i m i m you you i i m you i i m i could i i m i i m i m i m you i m i m you you you i m\n",
      "Epoch 5\n",
      "\tloss: 6.782829761505127\n",
      "\tText: hello world i m i m i i m i m i a i m i m i m i m you you i i m you i i i a i i i m i i m i m i m you i i m you i a i i m\n",
      "Epoch 6\n",
      "\tloss: 6.51298189163208\n",
      "\tText: hello world i m i m i i i m i a i i i i i i m i m you you i i m you i i i i i i i m i i i i i i m you i i m i m you i i m\n",
      "Epoch 7\n",
      "\tloss: 6.248287677764893\n",
      "\tText: hello world i i m i i i i i i i i i i i i i m i i m i i i i i i i i i i i i m i i i i i i i i i i m i i i m i m\n",
      "Epoch 8\n",
      "\tloss: 6.0060014724731445\n",
      "\tText: hello world i i i i i i i i i i i i i i i i m i i i m i i i i i i i i i i i i i i i i i i i i i i i i i i i i m\n",
      "Epoch 9\n",
      "\tloss: 5.801344394683838\n",
      "\tText: hello world i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "Epoch 10\n",
      "\tloss: 5.641796112060547\n",
      "\tText: hello world i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "Epoch 11\n",
      "\tloss: 5.525032997131348\n",
      "\tText: hello world i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "Epoch 12\n",
      "\tloss: 5.439590930938721\n",
      "\tText: hello world i i i i i i i i i i i i i i i i m i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i m\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model50 = transformer.create_model()\n",
    "train_model(model50, vocab, X, Y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9382e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = TransformerModel(len(vocab), rate=.001)\n",
    "model100 = transformer.create_model()\n",
    "train_model(model100, vocab, X, Y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5780f",
   "metadata": {},
   "source": [
    "## Overtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd1710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = TransformerModel(len(vocab))\n",
    "model500 = transformer.create_model()\n",
    "train_model(model500, vocab, X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd9aad",
   "metadata": {},
   "source": [
    "## Test with different starting phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cebb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "phrases = [\n",
    "    \"a day in the life\",\n",
    "    \"hello world\",\n",
    "    # \"I\",\n",
    "    \"la la la\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    text1 = GenerateText(model1, vocab, data.vocab_dict)\n",
    "    out1 = text1.generate_random_text(num_generate=10)\n",
    "    \n",
    "    text50 = GenerateText(model50, vocab, data.vocab_dict)\n",
    "    out50 = text50.generate_random_text(num_generate=10)\n",
    "    \n",
    "    text100 = GenerateText(model50, vocab, data.vocab_dict)\n",
    "    out100 = text100.generate_random_text(num_generate=10)    \n",
    "    print(f\"Phrase: {phrase}\")\n",
    "    print(f\"\\t1 Epoch: {out1}\")\n",
    "    print(f\"\\t50 Epochs: {out50}\")\n",
    "    print(f\"\\t100 Epochs: {out100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741de32a-bc14-4561-a178-f602c429d191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e23b663014ca37592679400f38d6d04e3fc85e5c6f63651f80341cade9e63d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
