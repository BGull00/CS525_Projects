{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c315b-3d4d-4ce6-a701-97c8ad865c4f",
   "metadata": {},
   "source": [
    "# Problem Introduction\n",
    "    The goal of this project was to get familiarized with building neural networks that have varying architectures using available python libraries and to experiment with the networks’ success on a dataset. The dataset that will be used is a modified version of the FairFace dataset (https://github.com/joojs/fairface) which contains 86,744 training images, 10,954 validation images, and the classifications for race, age, and gender. These images have been reduced to grayscale and resized to 32 x 32 pixels. The image inputs are normalized with Min-Max scaling and use Categorical Cross-Entropy as the loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc860b-fe4d-499e-a9ac-d6d6eaf3ab88",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d534170-9810-428e-88d0-548a53d7a670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tensor imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import optimizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#Pillow Imports\n",
    "from PIL import Image\n",
    "\n",
    "#Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Sci_Kit Imports\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a76739-643e-4988-85e9-a87595c11300",
   "metadata": {},
   "source": [
    "### Load TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a827b0-2156-4e05-b80b-1e41d15d8926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "#log_folder = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b827b-e676-490f-ad1f-17efbdda881d",
   "metadata": {},
   "source": [
    "# Task 1 Introduction\n",
    "    Task 1 uses a fully connected feed forward neural network with three hidden layers: 1024 neurons with hyperbolic tangent activation functions, 512 neurons with sigmoid activation functions, and 100 neurons with rectified linear activation functions. This network architecture will be used to attempt to classify the images into the age and race categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c861cc-e233-4588-8377-de39bea2ebcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createFeedFoward(inputShape, outputSize, lr):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(1024, input_shape=inputShape, activation='tanh'))\n",
    "    model.add(layers.Dense(512, activation='sigmoid'))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(outputSize, activation='softmax'))\n",
    "    opt = optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c0199d-d2a8-44a4-a4a2-fc0bb0c4cf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createXY(imgPath, labelFile, num):\n",
    "    #Create and normalize X\n",
    "    X = []\n",
    "    for i in range(num):\n",
    "        fileName = imgPath + str(i+1) + '.jpg'\n",
    "        img = Image.open(fileName)\n",
    "        X.append(list(img.getdata()))\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    #Get labels\n",
    "    label_df = pd.read_csv(labelFile)\n",
    "    \n",
    "    #Find unique labels and output size\n",
    "    age_labels = label_df['age'].unique()\n",
    "    race_labels = label_df['race'].unique()\n",
    "    \n",
    "    #Create Binary y arrays\n",
    "    lb_age = LabelBinarizer(sparse_output=False)\n",
    "    lb_race = LabelBinarizer(sparse_output=False)\n",
    "    lb_age.fit(age_labels)\n",
    "    lb_race.fit(race_labels)\n",
    "    y_age = list(label_df['age'].head(num))\n",
    "    y_race = list(label_df['race'].head(num))\n",
    "    y_age = lb_age.transform(y_age)\n",
    "    y_race = lb_race.transform(y_race)\n",
    "    \n",
    "    \n",
    "    return X, y_age, y_race, age_labels, race_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c9b9a8-0d3b-4adc-9323-0b2aa7cf4309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printResults(predictions, labels, trueLabels):\n",
    "    print(len(predictions))\n",
    "    print(len(predictions[1]))\n",
    "    print(len(labels))\n",
    "    print(len(trueLabels))\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"Label: \" + trueLabels[i])\n",
    "        for j in range(len(labels)):\n",
    "            print(\"{:12}: {:10.2f}%\".format(labels[j], (predictions[i][j] * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae9e112-bcf9-4f1a-9023-f274f066475f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getMax(values):\n",
    "    maxes = [np.argmax(val) for val in values ]\n",
    "    return maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03d6754-82d0-46fc-9056-27d73c78867c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createCallbacks(log_folder):\n",
    "    callbacks = [TensorBoard(log_dir=log_folder,\n",
    "                         histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True,\n",
    "                         update_freq='epoch',\n",
    "                         profile_batch=2,\n",
    "                         embeddings_freq=1)]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0cc615c-9cd8-4a43-9099-28eab987c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_age_train, y_race_train, age_labels, race_labels = createXY('project3_COSC525/train/', 'project3_COSC525/fairface_label_train.csv', 86744)\n",
    "X_test, y_age_test, y_race_test, _ , _ = createXY('project3_COSC525/val/', 'project3_COSC525/fairface_label_val.csv', 10954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96317c5-cbb6-4451-ad0e-99a9171cb5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def taskOne(X_train, y_train, X_test, y_test, lr, numEpochs, batchSize, log_folder):\n",
    "    model = createFeedFoward((1024,), len(y_train[0]), lr)\n",
    "    callbacks = createCallbacks(log_folder)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=numEpochs, batch_size=batchSize, callbacks=callbacks)\n",
    "    y_true = getMax(y_test)\n",
    "    y_pred = getMax(model.predict(X_test))\n",
    "    eval = tf.keras.metrics.Accuracy()\n",
    "    eval.update_state(y_true, y_pred)\n",
    "    print('Accuracy: ', eval.result().numpy())\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b209cd-8c69-47f8-bcd4-fb452047f933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.8739 - accuracy: 0.2927 - val_loss: 1.8516 - val_accuracy: 0.3015\n",
      "Epoch 2/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8522 - accuracy: 0.2963 - val_loss: 1.8447 - val_accuracy: 0.3013\n",
      "Epoch 3/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8419 - accuracy: 0.2971 - val_loss: 1.8314 - val_accuracy: 0.3048\n",
      "Epoch 4/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8321 - accuracy: 0.2985 - val_loss: 1.8194 - val_accuracy: 0.3057\n",
      "Epoch 5/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.8213 - accuracy: 0.3019 - val_loss: 1.8108 - val_accuracy: 0.3127\n",
      "Epoch 6/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.8113 - accuracy: 0.3045 - val_loss: 1.8024 - val_accuracy: 0.3116\n",
      "Epoch 7/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8016 - accuracy: 0.3087 - val_loss: 1.7893 - val_accuracy: 0.3171\n",
      "Epoch 8/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7925 - accuracy: 0.3108 - val_loss: 1.7857 - val_accuracy: 0.3183\n",
      "Epoch 9/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7838 - accuracy: 0.3148 - val_loss: 1.7709 - val_accuracy: 0.3267\n",
      "Epoch 10/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7753 - accuracy: 0.3182 - val_loss: 1.7672 - val_accuracy: 0.3286\n",
      "Epoch 11/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7677 - accuracy: 0.3200 - val_loss: 1.7569 - val_accuracy: 0.3287\n",
      "Epoch 12/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7605 - accuracy: 0.3227 - val_loss: 1.7492 - val_accuracy: 0.3315\n",
      "Epoch 13/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7531 - accuracy: 0.3251 - val_loss: 1.7417 - val_accuracy: 0.3328\n",
      "Epoch 14/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7468 - accuracy: 0.3261 - val_loss: 1.7366 - val_accuracy: 0.3358\n",
      "Epoch 15/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7411 - accuracy: 0.3284 - val_loss: 1.7339 - val_accuracy: 0.3354\n",
      "Epoch 16/150\n",
      "868/868 [==============================] - 9s 11ms/step - loss: 1.7354 - accuracy: 0.3298 - val_loss: 1.7273 - val_accuracy: 0.3334\n",
      "Epoch 17/150\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.7303 - accuracy: 0.3320 - val_loss: 1.7277 - val_accuracy: 0.3370\n",
      "Epoch 18/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7255 - accuracy: 0.3335 - val_loss: 1.7195 - val_accuracy: 0.3364\n",
      "Epoch 19/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7211 - accuracy: 0.3350 - val_loss: 1.7121 - val_accuracy: 0.3408\n",
      "Epoch 20/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7168 - accuracy: 0.3360 - val_loss: 1.7153 - val_accuracy: 0.3370\n",
      "Epoch 21/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7131 - accuracy: 0.3375 - val_loss: 1.7123 - val_accuracy: 0.3398\n",
      "Epoch 22/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7097 - accuracy: 0.3381 - val_loss: 1.7093 - val_accuracy: 0.3457\n",
      "Epoch 23/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7068 - accuracy: 0.3383 - val_loss: 1.6973 - val_accuracy: 0.3481\n",
      "Epoch 24/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7034 - accuracy: 0.3389 - val_loss: 1.7167 - val_accuracy: 0.3388\n",
      "Epoch 25/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7003 - accuracy: 0.3407 - val_loss: 1.6912 - val_accuracy: 0.3490\n",
      "Epoch 26/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6974 - accuracy: 0.3416 - val_loss: 1.6892 - val_accuracy: 0.3502\n",
      "Epoch 27/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6942 - accuracy: 0.3432 - val_loss: 1.6854 - val_accuracy: 0.3503\n",
      "Epoch 28/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6922 - accuracy: 0.3431 - val_loss: 1.6840 - val_accuracy: 0.3497\n",
      "Epoch 29/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6894 - accuracy: 0.3445 - val_loss: 1.6881 - val_accuracy: 0.3477\n",
      "Epoch 30/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6868 - accuracy: 0.3437 - val_loss: 1.6791 - val_accuracy: 0.3503\n",
      "Epoch 31/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6851 - accuracy: 0.3454 - val_loss: 1.6883 - val_accuracy: 0.3542\n",
      "Epoch 32/150\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6819 - accuracy: 0.3475 - val_loss: 1.6783 - val_accuracy: 0.3515\n",
      "Epoch 33/150\n",
      "868/868 [==============================] - 9s 11ms/step - loss: 1.6802 - accuracy: 0.3472 - val_loss: 1.6904 - val_accuracy: 0.3427\n",
      "Epoch 34/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6776 - accuracy: 0.3482 - val_loss: 1.6736 - val_accuracy: 0.3575\n",
      "Epoch 35/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6755 - accuracy: 0.3493 - val_loss: 1.6880 - val_accuracy: 0.3485\n",
      "Epoch 36/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6734 - accuracy: 0.3501 - val_loss: 1.6693 - val_accuracy: 0.3547\n",
      "Epoch 37/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6721 - accuracy: 0.3515 - val_loss: 1.6713 - val_accuracy: 0.3536\n",
      "Epoch 38/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6694 - accuracy: 0.3503 - val_loss: 1.6654 - val_accuracy: 0.3598\n",
      "Epoch 39/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6675 - accuracy: 0.3518 - val_loss: 1.6789 - val_accuracy: 0.3502\n",
      "Epoch 40/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6654 - accuracy: 0.3516 - val_loss: 1.6663 - val_accuracy: 0.3567\n",
      "Epoch 41/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6635 - accuracy: 0.3524 - val_loss: 1.6608 - val_accuracy: 0.3591\n",
      "Epoch 42/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6620 - accuracy: 0.3534 - val_loss: 1.6647 - val_accuracy: 0.3590\n",
      "Epoch 43/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6602 - accuracy: 0.3543 - val_loss: 1.6647 - val_accuracy: 0.3529\n",
      "Epoch 44/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6585 - accuracy: 0.3541 - val_loss: 1.6750 - val_accuracy: 0.3538\n",
      "Epoch 45/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6567 - accuracy: 0.3559 - val_loss: 1.6612 - val_accuracy: 0.3605\n",
      "Epoch 46/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6556 - accuracy: 0.3568 - val_loss: 1.6519 - val_accuracy: 0.3622\n",
      "Epoch 47/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6532 - accuracy: 0.3560 - val_loss: 1.6634 - val_accuracy: 0.3567\n",
      "Epoch 48/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6519 - accuracy: 0.3571 - val_loss: 1.6692 - val_accuracy: 0.3599\n",
      "Epoch 49/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6505 - accuracy: 0.3570 - val_loss: 1.6504 - val_accuracy: 0.3630\n",
      "Epoch 50/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6491 - accuracy: 0.3573 - val_loss: 1.6502 - val_accuracy: 0.3631\n",
      "Epoch 51/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6470 - accuracy: 0.3595 - val_loss: 1.6495 - val_accuracy: 0.3633\n",
      "Epoch 52/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6456 - accuracy: 0.3588 - val_loss: 1.6452 - val_accuracy: 0.3659\n",
      "Epoch 53/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6444 - accuracy: 0.3587 - val_loss: 1.6593 - val_accuracy: 0.3596\n",
      "Epoch 54/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6430 - accuracy: 0.3594 - val_loss: 1.6440 - val_accuracy: 0.3651\n",
      "Epoch 55/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6417 - accuracy: 0.3603 - val_loss: 1.6917 - val_accuracy: 0.3397\n",
      "Epoch 56/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6394 - accuracy: 0.3606 - val_loss: 1.6447 - val_accuracy: 0.3607\n",
      "Epoch 57/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6383 - accuracy: 0.3619 - val_loss: 1.6399 - val_accuracy: 0.3644\n",
      "Epoch 58/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6370 - accuracy: 0.3627 - val_loss: 1.6660 - val_accuracy: 0.3562\n",
      "Epoch 59/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6357 - accuracy: 0.3607 - val_loss: 1.6731 - val_accuracy: 0.3545\n",
      "Epoch 60/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6338 - accuracy: 0.3636 - val_loss: 1.6525 - val_accuracy: 0.3648\n",
      "Epoch 61/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6333 - accuracy: 0.3641 - val_loss: 1.6449 - val_accuracy: 0.3640\n",
      "Epoch 62/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6317 - accuracy: 0.3646 - val_loss: 1.6374 - val_accuracy: 0.3686\n",
      "Epoch 63/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6307 - accuracy: 0.3642 - val_loss: 1.6459 - val_accuracy: 0.3633\n",
      "Epoch 64/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6292 - accuracy: 0.3648 - val_loss: 1.6864 - val_accuracy: 0.3506\n",
      "Epoch 65/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6286 - accuracy: 0.3653 - val_loss: 1.6407 - val_accuracy: 0.3607\n",
      "Epoch 66/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6272 - accuracy: 0.3650 - val_loss: 1.6413 - val_accuracy: 0.3633\n",
      "Epoch 67/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6256 - accuracy: 0.3658 - val_loss: 1.6311 - val_accuracy: 0.3705\n",
      "Epoch 68/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6246 - accuracy: 0.3669 - val_loss: 1.6405 - val_accuracy: 0.3630\n",
      "Epoch 69/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6239 - accuracy: 0.3666 - val_loss: 1.6375 - val_accuracy: 0.3681\n",
      "Epoch 70/150\n",
      "868/868 [==============================] - 9s 11ms/step - loss: 1.6226 - accuracy: 0.3681 - val_loss: 1.6349 - val_accuracy: 0.3683\n",
      "Epoch 71/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6215 - accuracy: 0.3677 - val_loss: 1.6799 - val_accuracy: 0.3403\n",
      "Epoch 72/150\n",
      "868/868 [==============================] - 9s 10ms/step - loss: 1.6202 - accuracy: 0.3677 - val_loss: 1.6328 - val_accuracy: 0.3666\n",
      "Epoch 73/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6197 - accuracy: 0.3690 - val_loss: 1.6281 - val_accuracy: 0.3708\n",
      "Epoch 74/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6183 - accuracy: 0.3674 - val_loss: 1.6436 - val_accuracy: 0.3525\n",
      "Epoch 75/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6174 - accuracy: 0.3692 - val_loss: 1.7423 - val_accuracy: 0.3231\n",
      "Epoch 76/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6159 - accuracy: 0.3693 - val_loss: 1.6316 - val_accuracy: 0.3694\n",
      "Epoch 77/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6151 - accuracy: 0.3696 - val_loss: 1.6447 - val_accuracy: 0.3625\n",
      "Epoch 78/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6147 - accuracy: 0.3690 - val_loss: 1.6353 - val_accuracy: 0.3670\n",
      "Epoch 79/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6131 - accuracy: 0.3705 - val_loss: 1.6376 - val_accuracy: 0.3710\n",
      "Epoch 80/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6126 - accuracy: 0.3695 - val_loss: 1.6209 - val_accuracy: 0.3722\n",
      "Epoch 81/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6114 - accuracy: 0.3698 - val_loss: 1.6370 - val_accuracy: 0.3601\n",
      "Epoch 82/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6099 - accuracy: 0.3714 - val_loss: 1.6406 - val_accuracy: 0.3646\n",
      "Epoch 83/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6092 - accuracy: 0.3706 - val_loss: 1.6368 - val_accuracy: 0.3623\n",
      "Epoch 84/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6084 - accuracy: 0.3710 - val_loss: 1.6461 - val_accuracy: 0.3590\n",
      "Epoch 85/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6070 - accuracy: 0.3730 - val_loss: 1.6192 - val_accuracy: 0.3723\n",
      "Epoch 86/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6061 - accuracy: 0.3733 - val_loss: 1.6191 - val_accuracy: 0.3733\n",
      "Epoch 87/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6048 - accuracy: 0.3723 - val_loss: 1.6224 - val_accuracy: 0.3749\n",
      "Epoch 88/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6041 - accuracy: 0.3727 - val_loss: 1.6266 - val_accuracy: 0.3732\n",
      "Epoch 89/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6036 - accuracy: 0.3733 - val_loss: 1.6552 - val_accuracy: 0.3605\n",
      "Epoch 90/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6014 - accuracy: 0.3739 - val_loss: 1.6415 - val_accuracy: 0.3615\n",
      "Epoch 91/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6011 - accuracy: 0.3732 - val_loss: 1.6241 - val_accuracy: 0.3746\n",
      "Epoch 92/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6006 - accuracy: 0.3743 - val_loss: 1.6224 - val_accuracy: 0.3680\n",
      "Epoch 93/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5998 - accuracy: 0.3744 - val_loss: 1.6247 - val_accuracy: 0.3708\n",
      "Epoch 94/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5989 - accuracy: 0.3768 - val_loss: 1.6200 - val_accuracy: 0.3712\n",
      "Epoch 95/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5971 - accuracy: 0.3746 - val_loss: 1.6169 - val_accuracy: 0.3705\n",
      "Epoch 96/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5967 - accuracy: 0.3746 - val_loss: 1.6161 - val_accuracy: 0.3747\n",
      "Epoch 97/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5961 - accuracy: 0.3754 - val_loss: 1.6196 - val_accuracy: 0.3760\n",
      "Epoch 98/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5949 - accuracy: 0.3753 - val_loss: 1.6207 - val_accuracy: 0.3627\n",
      "Epoch 99/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5951 - accuracy: 0.3762 - val_loss: 1.6277 - val_accuracy: 0.3718\n",
      "Epoch 100/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5940 - accuracy: 0.3761 - val_loss: 1.6628 - val_accuracy: 0.3435\n",
      "Epoch 101/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5931 - accuracy: 0.3768 - val_loss: 1.6324 - val_accuracy: 0.3716\n",
      "Epoch 102/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5918 - accuracy: 0.3780 - val_loss: 1.6139 - val_accuracy: 0.3737\n",
      "Epoch 103/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5902 - accuracy: 0.3772 - val_loss: 1.6203 - val_accuracy: 0.3726\n",
      "Epoch 104/150\n",
      "868/868 [==============================] - 9s 11ms/step - loss: 1.5902 - accuracy: 0.3766 - val_loss: 1.6162 - val_accuracy: 0.3720\n",
      "Epoch 105/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5893 - accuracy: 0.3779 - val_loss: 1.6060 - val_accuracy: 0.3793\n",
      "Epoch 106/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5882 - accuracy: 0.3793 - val_loss: 1.6245 - val_accuracy: 0.3638\n",
      "Epoch 107/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5874 - accuracy: 0.3784 - val_loss: 1.6207 - val_accuracy: 0.3699\n",
      "Epoch 108/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5875 - accuracy: 0.3781 - val_loss: 1.6066 - val_accuracy: 0.3784\n",
      "Epoch 109/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5860 - accuracy: 0.3785 - val_loss: 1.6147 - val_accuracy: 0.3653\n",
      "Epoch 110/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5854 - accuracy: 0.3789 - val_loss: 1.6032 - val_accuracy: 0.3791\n",
      "Epoch 111/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5857 - accuracy: 0.3781 - val_loss: 1.6083 - val_accuracy: 0.3785\n",
      "Epoch 112/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5835 - accuracy: 0.3809 - val_loss: 1.6148 - val_accuracy: 0.3713\n",
      "Epoch 113/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5831 - accuracy: 0.3803 - val_loss: 1.6132 - val_accuracy: 0.3754\n",
      "Epoch 114/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5818 - accuracy: 0.3798 - val_loss: 1.5996 - val_accuracy: 0.3805\n",
      "Epoch 115/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5822 - accuracy: 0.3809 - val_loss: 1.6260 - val_accuracy: 0.3769\n",
      "Epoch 116/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5808 - accuracy: 0.3804 - val_loss: 1.6607 - val_accuracy: 0.3557\n",
      "Epoch 117/150\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.5800 - accuracy: 0.3799 - val_loss: 1.6103 - val_accuracy: 0.3726\n",
      "Epoch 118/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5783 - accuracy: 0.3811 - val_loss: 1.6179 - val_accuracy: 0.3768\n",
      "Epoch 119/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5778 - accuracy: 0.3800 - val_loss: 1.6084 - val_accuracy: 0.3748\n",
      "Epoch 120/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5768 - accuracy: 0.3816 - val_loss: 1.5963 - val_accuracy: 0.3827\n",
      "Epoch 121/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5767 - accuracy: 0.3822 - val_loss: 1.6129 - val_accuracy: 0.3647\n",
      "Epoch 122/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5748 - accuracy: 0.3826 - val_loss: 1.6179 - val_accuracy: 0.3624\n",
      "Epoch 123/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5747 - accuracy: 0.3817 - val_loss: 1.6208 - val_accuracy: 0.3738\n",
      "Epoch 124/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5747 - accuracy: 0.3823 - val_loss: 1.5987 - val_accuracy: 0.3810\n",
      "Epoch 125/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5728 - accuracy: 0.3829 - val_loss: 1.6163 - val_accuracy: 0.3681\n",
      "Epoch 126/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5730 - accuracy: 0.3819 - val_loss: 1.5988 - val_accuracy: 0.3771\n",
      "Epoch 127/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5713 - accuracy: 0.3838 - val_loss: 1.5995 - val_accuracy: 0.3815\n",
      "Epoch 128/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5703 - accuracy: 0.3826 - val_loss: 1.6154 - val_accuracy: 0.3720\n",
      "Epoch 129/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5686 - accuracy: 0.3835 - val_loss: 1.6083 - val_accuracy: 0.3717\n",
      "Epoch 130/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5692 - accuracy: 0.3844 - val_loss: 1.6035 - val_accuracy: 0.3813\n",
      "Epoch 131/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5695 - accuracy: 0.3833 - val_loss: 1.6183 - val_accuracy: 0.3743\n",
      "Epoch 132/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5679 - accuracy: 0.3846 - val_loss: 1.6222 - val_accuracy: 0.3664\n",
      "Epoch 133/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5675 - accuracy: 0.3839 - val_loss: 1.6315 - val_accuracy: 0.3638\n",
      "Epoch 134/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5675 - accuracy: 0.3843 - val_loss: 1.6046 - val_accuracy: 0.3756\n",
      "Epoch 135/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5649 - accuracy: 0.3858 - val_loss: 1.5974 - val_accuracy: 0.3860\n",
      "Epoch 136/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5651 - accuracy: 0.3837 - val_loss: 1.6105 - val_accuracy: 0.3800\n",
      "Epoch 137/150\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.5637 - accuracy: 0.3855 - val_loss: 1.5966 - val_accuracy: 0.3824\n",
      "Epoch 138/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5632 - accuracy: 0.3858 - val_loss: 1.6416 - val_accuracy: 0.3672\n",
      "Epoch 139/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5628 - accuracy: 0.3841 - val_loss: 1.5947 - val_accuracy: 0.3819\n",
      "Epoch 140/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5617 - accuracy: 0.3865 - val_loss: 1.6168 - val_accuracy: 0.3754\n",
      "Epoch 141/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5609 - accuracy: 0.3857 - val_loss: 1.6080 - val_accuracy: 0.3754\n",
      "Epoch 142/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5596 - accuracy: 0.3872 - val_loss: 1.5899 - val_accuracy: 0.3864\n",
      "Epoch 143/150\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.5592 - accuracy: 0.3868 - val_loss: 1.5885 - val_accuracy: 0.3871\n",
      "Epoch 144/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5584 - accuracy: 0.3877 - val_loss: 1.6037 - val_accuracy: 0.3810\n",
      "Epoch 145/150\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.5579 - accuracy: 0.3877 - val_loss: 1.6414 - val_accuracy: 0.3668\n",
      "Epoch 146/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5569 - accuracy: 0.3871 - val_loss: 1.6142 - val_accuracy: 0.3777\n",
      "Epoch 147/150\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.5563 - accuracy: 0.3870 - val_loss: 1.6005 - val_accuracy: 0.3784\n",
      "Epoch 148/150\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.5563 - accuracy: 0.3879 - val_loss: 1.5882 - val_accuracy: 0.3852\n",
      "Epoch 149/150\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.5540 - accuracy: 0.3878 - val_loss: 1.6349 - val_accuracy: 0.3652\n",
      "Epoch 150/150\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.5547 - accuracy: 0.3877 - val_loss: 1.5991 - val_accuracy: 0.3792\n",
      "343/343 [==============================] - 1s 3ms/step\n",
      "Accuracy:  0.3792222\n",
      "[[  53    1   52   68   23    2    0    0    0]\n",
      " [   1   66  829  115  144   13    4    9    0]\n",
      " [   4   34 2484   64  638   37   23   16    0]\n",
      " [  16   47  578  595   90   14   10    6    0]\n",
      " [   2   12 1388   51  767   63   32   15    0]\n",
      " [   3   10  624   23  539   89   50   15    0]\n",
      " [   0    5  292   15  307   70   76   31    0]\n",
      " [   0    0  125    8   94   27   43   24    0]\n",
      " [   1    2   44    3   34    5   10   19    0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d7544f45293182de\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d7544f45293182de\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6020;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskOne(X_train, y_age_train, X_test, y_age_test, 0.005, 150, 100, log_folder)\n",
    "%tensorboard --logdir logs --port=6020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94111ab0-5240-40db-b44c-413ea52e76ea",
   "metadata": {},
   "source": [
    "# Task 1 Results (Age)\n",
    "    The fully connected network used for classifying the age was trained over 200 epochs with a learning rate of 0.005 and a batch size of 100. The overall accuracy of the classifier was 37.92%, while not great, is to be expected from a shallow network with relatively small images. The epoch-loss graph shows that the training loss was still falling but the validation loss was starting to taper out. This indicates that further training would not improve the overall performance of the network. The confusion matrix for this network shows that it never predicted any person as being over the age of 70. This could indicate an imbalance in the dataset used.\n",
    "### Epoch-Loss Graph:\n",
    "![Epoch-Loss Graph](graphs/task1_age_epoch_loss.PNG)\n",
    "### Epoch-Accuracy Graph:\n",
    "![Epoch-Accuracy Graph](graphs/task1_age_epoch_accuracy.PNG)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfa19edb-0725-4dbf-9bc5-eb81ecd8fa9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "868/868 [==============================] - 13s 15ms/step - loss: 1.9279 - accuracy: 0.1919 - val_loss: 1.9167 - val_accuracy: 0.1976\n",
      "Epoch 2/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.9080 - accuracy: 0.2111 - val_loss: 1.9050 - val_accuracy: 0.2175\n",
      "Epoch 3/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8979 - accuracy: 0.2239 - val_loss: 1.8961 - val_accuracy: 0.2253\n",
      "Epoch 4/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8893 - accuracy: 0.2324 - val_loss: 1.8885 - val_accuracy: 0.2341\n",
      "Epoch 5/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8821 - accuracy: 0.2391 - val_loss: 1.8816 - val_accuracy: 0.2461\n",
      "Epoch 6/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8756 - accuracy: 0.2455 - val_loss: 1.8753 - val_accuracy: 0.2460\n",
      "Epoch 7/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8698 - accuracy: 0.2509 - val_loss: 1.8698 - val_accuracy: 0.2583\n",
      "Epoch 8/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8648 - accuracy: 0.2543 - val_loss: 1.8648 - val_accuracy: 0.2554\n",
      "Epoch 9/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8602 - accuracy: 0.2575 - val_loss: 1.8603 - val_accuracy: 0.2601\n",
      "Epoch 10/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.8558 - accuracy: 0.2615 - val_loss: 1.8567 - val_accuracy: 0.2641\n",
      "Epoch 11/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8518 - accuracy: 0.2646 - val_loss: 1.8520 - val_accuracy: 0.2633\n",
      "Epoch 12/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8481 - accuracy: 0.2665 - val_loss: 1.8489 - val_accuracy: 0.2726\n",
      "Epoch 13/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.8445 - accuracy: 0.2692 - val_loss: 1.8455 - val_accuracy: 0.2739\n",
      "Epoch 14/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8411 - accuracy: 0.2710 - val_loss: 1.8417 - val_accuracy: 0.2789\n",
      "Epoch 15/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8379 - accuracy: 0.2734 - val_loss: 1.8382 - val_accuracy: 0.2731\n",
      "Epoch 16/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8347 - accuracy: 0.2750 - val_loss: 1.8353 - val_accuracy: 0.2804\n",
      "Epoch 17/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8317 - accuracy: 0.2770 - val_loss: 1.8318 - val_accuracy: 0.2783\n",
      "Epoch 18/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8287 - accuracy: 0.2789 - val_loss: 1.8295 - val_accuracy: 0.2817\n",
      "Epoch 19/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8259 - accuracy: 0.2806 - val_loss: 1.8260 - val_accuracy: 0.2806\n",
      "Epoch 20/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8229 - accuracy: 0.2824 - val_loss: 1.8240 - val_accuracy: 0.2843\n",
      "Epoch 21/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.8202 - accuracy: 0.2838 - val_loss: 1.8205 - val_accuracy: 0.2835\n",
      "Epoch 22/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8174 - accuracy: 0.2848 - val_loss: 1.8186 - val_accuracy: 0.2887\n",
      "Epoch 23/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.8148 - accuracy: 0.2865 - val_loss: 1.8157 - val_accuracy: 0.2860\n",
      "Epoch 24/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.8121 - accuracy: 0.2889 - val_loss: 1.8140 - val_accuracy: 0.2886\n",
      "Epoch 25/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.8095 - accuracy: 0.2907 - val_loss: 1.8106 - val_accuracy: 0.2884\n",
      "Epoch 26/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8068 - accuracy: 0.2920 - val_loss: 1.8077 - val_accuracy: 0.2914\n",
      "Epoch 27/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8042 - accuracy: 0.2931 - val_loss: 1.8061 - val_accuracy: 0.2916\n",
      "Epoch 28/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.8016 - accuracy: 0.2949 - val_loss: 1.8027 - val_accuracy: 0.2954\n",
      "Epoch 29/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7989 - accuracy: 0.2968 - val_loss: 1.8000 - val_accuracy: 0.2951\n",
      "Epoch 30/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.7963 - accuracy: 0.2979 - val_loss: 1.7978 - val_accuracy: 0.2953\n",
      "Epoch 31/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7936 - accuracy: 0.2994 - val_loss: 1.7947 - val_accuracy: 0.2974\n",
      "Epoch 32/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7908 - accuracy: 0.3013 - val_loss: 1.7925 - val_accuracy: 0.2997\n",
      "Epoch 33/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7881 - accuracy: 0.3032 - val_loss: 1.7905 - val_accuracy: 0.3016\n",
      "Epoch 34/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7854 - accuracy: 0.3053 - val_loss: 1.7872 - val_accuracy: 0.2981\n",
      "Epoch 35/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7826 - accuracy: 0.3057 - val_loss: 1.7848 - val_accuracy: 0.3020\n",
      "Epoch 36/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7798 - accuracy: 0.3074 - val_loss: 1.7830 - val_accuracy: 0.3024\n",
      "Epoch 37/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7770 - accuracy: 0.3087 - val_loss: 1.7793 - val_accuracy: 0.3054\n",
      "Epoch 38/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7741 - accuracy: 0.3106 - val_loss: 1.7765 - val_accuracy: 0.3079\n",
      "Epoch 39/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7713 - accuracy: 0.3116 - val_loss: 1.7748 - val_accuracy: 0.3085\n",
      "Epoch 40/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7683 - accuracy: 0.3141 - val_loss: 1.7718 - val_accuracy: 0.3049\n",
      "Epoch 41/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7655 - accuracy: 0.3154 - val_loss: 1.7677 - val_accuracy: 0.3105\n",
      "Epoch 42/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7626 - accuracy: 0.3162 - val_loss: 1.7653 - val_accuracy: 0.3106\n",
      "Epoch 43/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7597 - accuracy: 0.3181 - val_loss: 1.7625 - val_accuracy: 0.3172\n",
      "Epoch 44/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7568 - accuracy: 0.3203 - val_loss: 1.7599 - val_accuracy: 0.3192\n",
      "Epoch 45/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7536 - accuracy: 0.3215 - val_loss: 1.7582 - val_accuracy: 0.3104\n",
      "Epoch 46/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7508 - accuracy: 0.3234 - val_loss: 1.7544 - val_accuracy: 0.3203\n",
      "Epoch 47/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7479 - accuracy: 0.3245 - val_loss: 1.7513 - val_accuracy: 0.3176\n",
      "Epoch 48/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7449 - accuracy: 0.3267 - val_loss: 1.7485 - val_accuracy: 0.3196\n",
      "Epoch 49/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7421 - accuracy: 0.3271 - val_loss: 1.7462 - val_accuracy: 0.3160\n",
      "Epoch 50/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7391 - accuracy: 0.3293 - val_loss: 1.7433 - val_accuracy: 0.3255\n",
      "Epoch 51/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7363 - accuracy: 0.3299 - val_loss: 1.7412 - val_accuracy: 0.3266\n",
      "Epoch 52/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7335 - accuracy: 0.3316 - val_loss: 1.7384 - val_accuracy: 0.3296\n",
      "Epoch 53/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7306 - accuracy: 0.3333 - val_loss: 1.7360 - val_accuracy: 0.3217\n",
      "Epoch 54/200\n",
      "868/868 [==============================] - 9s 11ms/step - loss: 1.7280 - accuracy: 0.3343 - val_loss: 1.7331 - val_accuracy: 0.3281\n",
      "Epoch 55/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.7254 - accuracy: 0.3345 - val_loss: 1.7297 - val_accuracy: 0.3307\n",
      "Epoch 56/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7227 - accuracy: 0.3362 - val_loss: 1.7283 - val_accuracy: 0.3317\n",
      "Epoch 57/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7201 - accuracy: 0.3369 - val_loss: 1.7262 - val_accuracy: 0.3322\n",
      "Epoch 58/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7174 - accuracy: 0.3386 - val_loss: 1.7239 - val_accuracy: 0.3344\n",
      "Epoch 59/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.7149 - accuracy: 0.3384 - val_loss: 1.7203 - val_accuracy: 0.3368\n",
      "Epoch 60/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7124 - accuracy: 0.3413 - val_loss: 1.7179 - val_accuracy: 0.3360\n",
      "Epoch 61/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.7100 - accuracy: 0.3418 - val_loss: 1.7158 - val_accuracy: 0.3400\n",
      "Epoch 62/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7079 - accuracy: 0.3419 - val_loss: 1.7145 - val_accuracy: 0.3412\n",
      "Epoch 63/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.7055 - accuracy: 0.3429 - val_loss: 1.7118 - val_accuracy: 0.3349\n",
      "Epoch 64/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7032 - accuracy: 0.3439 - val_loss: 1.7095 - val_accuracy: 0.3376\n",
      "Epoch 65/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.7012 - accuracy: 0.3448 - val_loss: 1.7074 - val_accuracy: 0.3385\n",
      "Epoch 66/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6991 - accuracy: 0.3451 - val_loss: 1.7065 - val_accuracy: 0.3416\n",
      "Epoch 67/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6969 - accuracy: 0.3466 - val_loss: 1.7035 - val_accuracy: 0.3402\n",
      "Epoch 68/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6950 - accuracy: 0.3467 - val_loss: 1.7027 - val_accuracy: 0.3426\n",
      "Epoch 69/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6931 - accuracy: 0.3484 - val_loss: 1.6999 - val_accuracy: 0.3434\n",
      "Epoch 70/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6913 - accuracy: 0.3489 - val_loss: 1.6984 - val_accuracy: 0.3413\n",
      "Epoch 71/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6895 - accuracy: 0.3499 - val_loss: 1.6968 - val_accuracy: 0.3427\n",
      "Epoch 72/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6877 - accuracy: 0.3501 - val_loss: 1.6964 - val_accuracy: 0.3432\n",
      "Epoch 73/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6860 - accuracy: 0.3515 - val_loss: 1.6934 - val_accuracy: 0.3433\n",
      "Epoch 74/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6844 - accuracy: 0.3522 - val_loss: 1.6916 - val_accuracy: 0.3447\n",
      "Epoch 75/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6828 - accuracy: 0.3523 - val_loss: 1.6900 - val_accuracy: 0.3471\n",
      "Epoch 76/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6811 - accuracy: 0.3531 - val_loss: 1.6891 - val_accuracy: 0.3453\n",
      "Epoch 77/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6796 - accuracy: 0.3533 - val_loss: 1.6875 - val_accuracy: 0.3457\n",
      "Epoch 78/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6782 - accuracy: 0.3536 - val_loss: 1.6871 - val_accuracy: 0.3461\n",
      "Epoch 79/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6769 - accuracy: 0.3550 - val_loss: 1.6853 - val_accuracy: 0.3474\n",
      "Epoch 80/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6755 - accuracy: 0.3545 - val_loss: 1.6847 - val_accuracy: 0.3493\n",
      "Epoch 81/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6740 - accuracy: 0.3557 - val_loss: 1.6820 - val_accuracy: 0.3478\n",
      "Epoch 82/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6729 - accuracy: 0.3562 - val_loss: 1.6809 - val_accuracy: 0.3503\n",
      "Epoch 83/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6716 - accuracy: 0.3569 - val_loss: 1.6802 - val_accuracy: 0.3527\n",
      "Epoch 84/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6704 - accuracy: 0.3571 - val_loss: 1.6788 - val_accuracy: 0.3520\n",
      "Epoch 85/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6691 - accuracy: 0.3575 - val_loss: 1.6789 - val_accuracy: 0.3529\n",
      "Epoch 86/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6680 - accuracy: 0.3576 - val_loss: 1.6775 - val_accuracy: 0.3512\n",
      "Epoch 87/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6670 - accuracy: 0.3592 - val_loss: 1.6763 - val_accuracy: 0.3530\n",
      "Epoch 88/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6657 - accuracy: 0.3595 - val_loss: 1.6767 - val_accuracy: 0.3550\n",
      "Epoch 89/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6648 - accuracy: 0.3599 - val_loss: 1.6748 - val_accuracy: 0.3549\n",
      "Epoch 90/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6635 - accuracy: 0.3601 - val_loss: 1.6736 - val_accuracy: 0.3532\n",
      "Epoch 91/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6627 - accuracy: 0.3606 - val_loss: 1.6728 - val_accuracy: 0.3515\n",
      "Epoch 92/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6617 - accuracy: 0.3611 - val_loss: 1.6700 - val_accuracy: 0.3541\n",
      "Epoch 93/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6608 - accuracy: 0.3616 - val_loss: 1.6694 - val_accuracy: 0.3554\n",
      "Epoch 94/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6599 - accuracy: 0.3620 - val_loss: 1.6720 - val_accuracy: 0.3590\n",
      "Epoch 95/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6589 - accuracy: 0.3626 - val_loss: 1.6681 - val_accuracy: 0.3560\n",
      "Epoch 96/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6579 - accuracy: 0.3633 - val_loss: 1.6677 - val_accuracy: 0.3593\n",
      "Epoch 97/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6572 - accuracy: 0.3632 - val_loss: 1.6658 - val_accuracy: 0.3578\n",
      "Epoch 98/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6565 - accuracy: 0.3634 - val_loss: 1.6661 - val_accuracy: 0.3553\n",
      "Epoch 99/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6555 - accuracy: 0.3637 - val_loss: 1.6645 - val_accuracy: 0.3615\n",
      "Epoch 100/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6545 - accuracy: 0.3648 - val_loss: 1.6637 - val_accuracy: 0.3559\n",
      "Epoch 101/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6539 - accuracy: 0.3647 - val_loss: 1.6628 - val_accuracy: 0.3569\n",
      "Epoch 102/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6529 - accuracy: 0.3651 - val_loss: 1.6631 - val_accuracy: 0.3616\n",
      "Epoch 103/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6523 - accuracy: 0.3653 - val_loss: 1.6623 - val_accuracy: 0.3569\n",
      "Epoch 104/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6514 - accuracy: 0.3665 - val_loss: 1.6618 - val_accuracy: 0.3553\n",
      "Epoch 105/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6508 - accuracy: 0.3667 - val_loss: 1.6610 - val_accuracy: 0.3601\n",
      "Epoch 106/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6499 - accuracy: 0.3667 - val_loss: 1.6597 - val_accuracy: 0.3626\n",
      "Epoch 107/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6494 - accuracy: 0.3669 - val_loss: 1.6607 - val_accuracy: 0.3607\n",
      "Epoch 108/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6486 - accuracy: 0.3663 - val_loss: 1.6582 - val_accuracy: 0.3618\n",
      "Epoch 109/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6478 - accuracy: 0.3677 - val_loss: 1.6604 - val_accuracy: 0.3614\n",
      "Epoch 110/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6471 - accuracy: 0.3686 - val_loss: 1.6602 - val_accuracy: 0.3595\n",
      "Epoch 111/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6464 - accuracy: 0.3687 - val_loss: 1.6576 - val_accuracy: 0.3614\n",
      "Epoch 112/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6458 - accuracy: 0.3694 - val_loss: 1.6560 - val_accuracy: 0.3656\n",
      "Epoch 113/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6452 - accuracy: 0.3689 - val_loss: 1.6548 - val_accuracy: 0.3625\n",
      "Epoch 114/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6443 - accuracy: 0.3694 - val_loss: 1.6549 - val_accuracy: 0.3645\n",
      "Epoch 115/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6438 - accuracy: 0.3700 - val_loss: 1.6544 - val_accuracy: 0.3637\n",
      "Epoch 116/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6431 - accuracy: 0.3707 - val_loss: 1.6528 - val_accuracy: 0.3646\n",
      "Epoch 117/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6426 - accuracy: 0.3712 - val_loss: 1.6536 - val_accuracy: 0.3622\n",
      "Epoch 118/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6421 - accuracy: 0.3708 - val_loss: 1.6520 - val_accuracy: 0.3618\n",
      "Epoch 119/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6412 - accuracy: 0.3721 - val_loss: 1.6527 - val_accuracy: 0.3629\n",
      "Epoch 120/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6406 - accuracy: 0.3714 - val_loss: 1.6531 - val_accuracy: 0.3666\n",
      "Epoch 121/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6401 - accuracy: 0.3720 - val_loss: 1.6522 - val_accuracy: 0.3619\n",
      "Epoch 122/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6395 - accuracy: 0.3720 - val_loss: 1.6507 - val_accuracy: 0.3632\n",
      "Epoch 123/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6388 - accuracy: 0.3728 - val_loss: 1.6502 - val_accuracy: 0.3661\n",
      "Epoch 124/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6382 - accuracy: 0.3729 - val_loss: 1.6486 - val_accuracy: 0.3653\n",
      "Epoch 125/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6375 - accuracy: 0.3736 - val_loss: 1.6486 - val_accuracy: 0.3674\n",
      "Epoch 126/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6372 - accuracy: 0.3740 - val_loss: 1.6474 - val_accuracy: 0.3669\n",
      "Epoch 127/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6365 - accuracy: 0.3739 - val_loss: 1.6473 - val_accuracy: 0.3667\n",
      "Epoch 128/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6360 - accuracy: 0.3739 - val_loss: 1.6467 - val_accuracy: 0.3668\n",
      "Epoch 129/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6355 - accuracy: 0.3742 - val_loss: 1.6472 - val_accuracy: 0.3644\n",
      "Epoch 130/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6349 - accuracy: 0.3746 - val_loss: 1.6454 - val_accuracy: 0.3674\n",
      "Epoch 131/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6343 - accuracy: 0.3754 - val_loss: 1.6452 - val_accuracy: 0.3667\n",
      "Epoch 132/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6337 - accuracy: 0.3758 - val_loss: 1.6459 - val_accuracy: 0.3717\n",
      "Epoch 133/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6334 - accuracy: 0.3755 - val_loss: 1.6455 - val_accuracy: 0.3659\n",
      "Epoch 134/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6325 - accuracy: 0.3756 - val_loss: 1.6436 - val_accuracy: 0.3709\n",
      "Epoch 135/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6323 - accuracy: 0.3756 - val_loss: 1.6436 - val_accuracy: 0.3686\n",
      "Epoch 136/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6316 - accuracy: 0.3767 - val_loss: 1.6442 - val_accuracy: 0.3685\n",
      "Epoch 137/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6310 - accuracy: 0.3778 - val_loss: 1.6418 - val_accuracy: 0.3695\n",
      "Epoch 138/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6305 - accuracy: 0.3762 - val_loss: 1.6425 - val_accuracy: 0.3702\n",
      "Epoch 139/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6301 - accuracy: 0.3768 - val_loss: 1.6413 - val_accuracy: 0.3709\n",
      "Epoch 140/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6295 - accuracy: 0.3781 - val_loss: 1.6415 - val_accuracy: 0.3726\n",
      "Epoch 141/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6291 - accuracy: 0.3778 - val_loss: 1.6401 - val_accuracy: 0.3678\n",
      "Epoch 142/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6286 - accuracy: 0.3772 - val_loss: 1.6390 - val_accuracy: 0.3712\n",
      "Epoch 143/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6279 - accuracy: 0.3778 - val_loss: 1.6404 - val_accuracy: 0.3680\n",
      "Epoch 144/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6273 - accuracy: 0.3784 - val_loss: 1.6412 - val_accuracy: 0.3684\n",
      "Epoch 145/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6269 - accuracy: 0.3786 - val_loss: 1.6385 - val_accuracy: 0.3707\n",
      "Epoch 146/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6265 - accuracy: 0.3792 - val_loss: 1.6394 - val_accuracy: 0.3733\n",
      "Epoch 147/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6261 - accuracy: 0.3799 - val_loss: 1.6378 - val_accuracy: 0.3731\n",
      "Epoch 148/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6257 - accuracy: 0.3791 - val_loss: 1.6370 - val_accuracy: 0.3730\n",
      "Epoch 149/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6250 - accuracy: 0.3790 - val_loss: 1.6380 - val_accuracy: 0.3761\n",
      "Epoch 150/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6245 - accuracy: 0.3802 - val_loss: 1.6375 - val_accuracy: 0.3712\n",
      "Epoch 151/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6241 - accuracy: 0.3797 - val_loss: 1.6367 - val_accuracy: 0.3711\n",
      "Epoch 152/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6233 - accuracy: 0.3801 - val_loss: 1.6359 - val_accuracy: 0.3723\n",
      "Epoch 153/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6227 - accuracy: 0.3810 - val_loss: 1.6349 - val_accuracy: 0.3748\n",
      "Epoch 154/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6223 - accuracy: 0.3813 - val_loss: 1.6349 - val_accuracy: 0.3722\n",
      "Epoch 155/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6219 - accuracy: 0.3805 - val_loss: 1.6340 - val_accuracy: 0.3741\n",
      "Epoch 156/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6214 - accuracy: 0.3812 - val_loss: 1.6349 - val_accuracy: 0.3771\n",
      "Epoch 157/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6210 - accuracy: 0.3807 - val_loss: 1.6344 - val_accuracy: 0.3722\n",
      "Epoch 158/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6205 - accuracy: 0.3812 - val_loss: 1.6336 - val_accuracy: 0.3746\n",
      "Epoch 159/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6198 - accuracy: 0.3817 - val_loss: 1.6325 - val_accuracy: 0.3753\n",
      "Epoch 160/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6195 - accuracy: 0.3814 - val_loss: 1.6322 - val_accuracy: 0.3737\n",
      "Epoch 161/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6190 - accuracy: 0.3831 - val_loss: 1.6318 - val_accuracy: 0.3726\n",
      "Epoch 162/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6186 - accuracy: 0.3824 - val_loss: 1.6323 - val_accuracy: 0.3731\n",
      "Epoch 163/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6181 - accuracy: 0.3829 - val_loss: 1.6309 - val_accuracy: 0.3731\n",
      "Epoch 164/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6174 - accuracy: 0.3825 - val_loss: 1.6334 - val_accuracy: 0.3696\n",
      "Epoch 165/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6171 - accuracy: 0.3827 - val_loss: 1.6315 - val_accuracy: 0.3750\n",
      "Epoch 166/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6165 - accuracy: 0.3831 - val_loss: 1.6333 - val_accuracy: 0.3741\n",
      "Epoch 167/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6162 - accuracy: 0.3838 - val_loss: 1.6296 - val_accuracy: 0.3757\n",
      "Epoch 168/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6157 - accuracy: 0.3835 - val_loss: 1.6291 - val_accuracy: 0.3757\n",
      "Epoch 169/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6152 - accuracy: 0.3829 - val_loss: 1.6284 - val_accuracy: 0.3775\n",
      "Epoch 170/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6148 - accuracy: 0.3842 - val_loss: 1.6286 - val_accuracy: 0.3795\n",
      "Epoch 171/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6143 - accuracy: 0.3845 - val_loss: 1.6275 - val_accuracy: 0.3748\n",
      "Epoch 172/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6138 - accuracy: 0.3842 - val_loss: 1.6270 - val_accuracy: 0.3768\n",
      "Epoch 173/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6133 - accuracy: 0.3849 - val_loss: 1.6276 - val_accuracy: 0.3789\n",
      "Epoch 174/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6129 - accuracy: 0.3853 - val_loss: 1.6266 - val_accuracy: 0.3762\n",
      "Epoch 175/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6122 - accuracy: 0.3845 - val_loss: 1.6278 - val_accuracy: 0.3748\n",
      "Epoch 176/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6118 - accuracy: 0.3851 - val_loss: 1.6250 - val_accuracy: 0.3775\n",
      "Epoch 177/200\n",
      "868/868 [==============================] - 12s 14ms/step - loss: 1.6113 - accuracy: 0.3865 - val_loss: 1.6256 - val_accuracy: 0.3766\n",
      "Epoch 178/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6109 - accuracy: 0.3863 - val_loss: 1.6246 - val_accuracy: 0.3785\n",
      "Epoch 179/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6104 - accuracy: 0.3865 - val_loss: 1.6244 - val_accuracy: 0.3790\n",
      "Epoch 180/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6100 - accuracy: 0.3866 - val_loss: 1.6252 - val_accuracy: 0.3793\n",
      "Epoch 181/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6096 - accuracy: 0.3867 - val_loss: 1.6248 - val_accuracy: 0.3786\n",
      "Epoch 182/200\n",
      "868/868 [==============================] - 10s 11ms/step - loss: 1.6090 - accuracy: 0.3874 - val_loss: 1.6241 - val_accuracy: 0.3802\n",
      "Epoch 183/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6086 - accuracy: 0.3862 - val_loss: 1.6234 - val_accuracy: 0.3821\n",
      "Epoch 184/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6084 - accuracy: 0.3873 - val_loss: 1.6301 - val_accuracy: 0.3763\n",
      "Epoch 185/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6077 - accuracy: 0.3873 - val_loss: 1.6256 - val_accuracy: 0.3764\n",
      "Epoch 186/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6073 - accuracy: 0.3875 - val_loss: 1.6288 - val_accuracy: 0.3787\n",
      "Epoch 187/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6068 - accuracy: 0.3874 - val_loss: 1.6223 - val_accuracy: 0.3805\n",
      "Epoch 188/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6064 - accuracy: 0.3876 - val_loss: 1.6218 - val_accuracy: 0.3821\n",
      "Epoch 189/200\n",
      "868/868 [==============================] - 10s 12ms/step - loss: 1.6060 - accuracy: 0.3878 - val_loss: 1.6218 - val_accuracy: 0.3793\n",
      "Epoch 190/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6056 - accuracy: 0.3874 - val_loss: 1.6213 - val_accuracy: 0.3803\n",
      "Epoch 191/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6051 - accuracy: 0.3873 - val_loss: 1.6207 - val_accuracy: 0.3806\n",
      "Epoch 192/200\n",
      "868/868 [==============================] - 11s 12ms/step - loss: 1.6044 - accuracy: 0.3888 - val_loss: 1.6208 - val_accuracy: 0.3819\n",
      "Epoch 193/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6040 - accuracy: 0.3885 - val_loss: 1.6224 - val_accuracy: 0.3798\n",
      "Epoch 194/200\n",
      "868/868 [==============================] - 12s 13ms/step - loss: 1.6037 - accuracy: 0.3883 - val_loss: 1.6196 - val_accuracy: 0.3814\n",
      "Epoch 195/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6032 - accuracy: 0.3897 - val_loss: 1.6183 - val_accuracy: 0.3819\n",
      "Epoch 196/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6028 - accuracy: 0.3896 - val_loss: 1.6197 - val_accuracy: 0.3808\n",
      "Epoch 197/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6024 - accuracy: 0.3895 - val_loss: 1.6208 - val_accuracy: 0.3806\n",
      "Epoch 198/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6019 - accuracy: 0.3890 - val_loss: 1.6178 - val_accuracy: 0.3818\n",
      "Epoch 199/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6013 - accuracy: 0.3897 - val_loss: 1.6171 - val_accuracy: 0.3826\n",
      "Epoch 200/200\n",
      "868/868 [==============================] - 11s 13ms/step - loss: 1.6008 - accuracy: 0.3889 - val_loss: 1.6185 - val_accuracy: 0.3824\n",
      "343/343 [==============================] - 1s 4ms/step\n",
      "Accuracy:  0.38241738\n",
      "[[1001   44  188   52   28   29  214]\n",
      " [ 114  820  108   75   17   89  327]\n",
      " [ 306   70  648  138   42   32  280]\n",
      " [ 202  173  282  277   65   55  569]\n",
      " [ 159   78  219  143  130   13  467]\n",
      " [ 228  405  186  118   17  159  302]\n",
      " [ 259  191  197  159   80   45 1154]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-990cd3e9be46e157\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-990cd3e9be46e157\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6021;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskOne(X_train, y_race_train, X_test, y_race_test, 0.001, 200, 100, log_folder)\n",
    "%tensorboard --logdir logs --port=6021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383aa1a-ccc4-44e2-b47f-3123b4b1b0b4",
   "metadata": {},
   "source": [
    "# Task 1 Results (Race)\n",
    "    The fully connected network used for classifying the race was trained over 200 epochs with a learning rate of 0.001 and a batch size of 100. The learning rate decrease with the increase in the number of epochs was an attempt to get a higher accuracy out of the network. However, the network’s overall accuracy was 38.24%. Increasing the number of epochs even further could possibly increase the accuracy since the accuracy and loss graphs had not started to taper off, but it is doubtful that the accuracy would improve significantly with this network.\n",
    "### Epoch-Loss Graph:\n",
    "![Epoch-Loss Graph](graphs/task1_race_epoch_loss.PNG)\n",
    "### Epoch-Accuracy Graph:\n",
    "![Epoch-Accuracy Graph](graphs/task1_race_epoch_accuracy.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4e457-4953-446a-9dcc-e98c3e9ff5a2",
   "metadata": {},
   "source": [
    "# Task 2 Introduction\n",
    "    Task 2 uses a small convolutional network comprised of three hidden layers: a convolution layer using rectified linear activation functions with 40 feature detectors and a 5x5 kernel size, a max pooling layer with a 2x2 pooling size, and a fully connected layer with 100 neurons using rectified linear activation functions. This network architecture will be used to attempt to classify the images into the age and race categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bb7ac59-dfae-4919-885d-9d06b9ae0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CNN network model\n",
    "def createSmallCNN(inputShape, outputSize, lr):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(40, (5, 5), activation='relu', input_shape=inputShape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(outputSize, activation='softmax'))\n",
    "    opt = optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b8778fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized data\n",
    "#X_train, y_age_train, y_race_train, age_labels, race_labels = createXY('project3_COSC525/train/', 'project3_COSC525/fairface_label_train.csv', 10000)\n",
    "#X_test, y_age_test, y_race_test, _ , _ = createXY('project3_COSC525/val/', 'project3_COSC525/fairface_label_val.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd313e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task two; train and evaluate CNN for given train/test data\n",
    "def taskTwo(X_train, y_train, X_test, y_test, lr, numEpochs, batchSize, log_folder):\n",
    "    model = createSmallCNN((32, 32, 1), len(y_train[0]), lr)\n",
    "    callbacks = createCallbacks(log_folder)\n",
    "    model.fit(np.reshape(X_train, (X_train.shape[0], 32, 32)), y_train, validation_data=(np.reshape(X_test, (X_test.shape[0], 32, 32)), y_test), epochs=numEpochs, batch_size=batchSize, callbacks=callbacks)\n",
    "    y_true = getMax(y_test)\n",
    "    y_pred = getMax(model.predict(np.reshape(X_test, (X_test.shape[0], 32, 32))))\n",
    "    eval = tf.keras.metrics.Accuracy()\n",
    "    eval.update_state(y_true, y_pred)\n",
    "    print('Accuracy: ', eval.result().numpy())\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb3ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.9016 - accuracy: 0.2922 - val_loss: 1.8670 - val_accuracy: 0.3059\n",
      "Epoch 2/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.8628 - accuracy: 0.2991 - val_loss: 1.8501 - val_accuracy: 0.3054\n",
      "Epoch 3/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.8472 - accuracy: 0.3022 - val_loss: 1.8349 - val_accuracy: 0.3023\n",
      "Epoch 4/200\n",
      "868/868 [==============================] - 25s 28ms/step - loss: 1.8321 - accuracy: 0.3027 - val_loss: 1.8198 - val_accuracy: 0.3087\n",
      "Epoch 5/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.8178 - accuracy: 0.3047 - val_loss: 1.8088 - val_accuracy: 0.3097\n",
      "Epoch 6/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.8054 - accuracy: 0.3059 - val_loss: 1.7894 - val_accuracy: 0.3138\n",
      "Epoch 7/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.7950 - accuracy: 0.3085 - val_loss: 1.7820 - val_accuracy: 0.3163\n",
      "Epoch 8/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.7857 - accuracy: 0.3118 - val_loss: 1.7768 - val_accuracy: 0.3182\n",
      "Epoch 9/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.7766 - accuracy: 0.3151 - val_loss: 1.7667 - val_accuracy: 0.3245\n",
      "Epoch 10/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.7678 - accuracy: 0.3170 - val_loss: 1.7571 - val_accuracy: 0.3280\n",
      "Epoch 11/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.7591 - accuracy: 0.3209 - val_loss: 1.7457 - val_accuracy: 0.3309\n",
      "Epoch 12/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.7499 - accuracy: 0.3249 - val_loss: 1.7364 - val_accuracy: 0.3334\n",
      "Epoch 13/200\n",
      "868/868 [==============================] - 26s 31ms/step - loss: 1.7409 - accuracy: 0.3279 - val_loss: 1.7281 - val_accuracy: 0.3387\n",
      "Epoch 14/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.7317 - accuracy: 0.3332 - val_loss: 1.7223 - val_accuracy: 0.3351\n",
      "Epoch 15/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.7225 - accuracy: 0.3357 - val_loss: 1.7222 - val_accuracy: 0.3432\n",
      "Epoch 16/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.7136 - accuracy: 0.3397 - val_loss: 1.7145 - val_accuracy: 0.3464\n",
      "Epoch 17/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.7049 - accuracy: 0.3411 - val_loss: 1.7050 - val_accuracy: 0.3436\n",
      "Epoch 18/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.6974 - accuracy: 0.3449 - val_loss: 1.6910 - val_accuracy: 0.3475\n",
      "Epoch 19/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.6891 - accuracy: 0.3477 - val_loss: 1.6813 - val_accuracy: 0.3612\n",
      "Epoch 20/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.6818 - accuracy: 0.3490 - val_loss: 1.6793 - val_accuracy: 0.3550\n",
      "Epoch 21/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6749 - accuracy: 0.3531 - val_loss: 1.6753 - val_accuracy: 0.3488\n",
      "Epoch 22/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6683 - accuracy: 0.3533 - val_loss: 1.6645 - val_accuracy: 0.3633\n",
      "Epoch 23/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.6623 - accuracy: 0.3563 - val_loss: 1.6554 - val_accuracy: 0.3643\n",
      "Epoch 24/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.6563 - accuracy: 0.3583 - val_loss: 1.6547 - val_accuracy: 0.3638\n",
      "Epoch 25/200\n",
      "868/868 [==============================] - 26s 29ms/step - loss: 1.6511 - accuracy: 0.3601 - val_loss: 1.6563 - val_accuracy: 0.3606\n",
      "Epoch 26/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6460 - accuracy: 0.3621 - val_loss: 1.6422 - val_accuracy: 0.3723\n",
      "Epoch 27/200\n",
      "868/868 [==============================] - 25s 28ms/step - loss: 1.6407 - accuracy: 0.3640 - val_loss: 1.6421 - val_accuracy: 0.3690\n",
      "Epoch 28/200\n",
      "868/868 [==============================] - 26s 29ms/step - loss: 1.6363 - accuracy: 0.3657 - val_loss: 1.6398 - val_accuracy: 0.3684\n",
      "Epoch 29/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6311 - accuracy: 0.3668 - val_loss: 1.6320 - val_accuracy: 0.3733\n",
      "Epoch 30/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.6266 - accuracy: 0.3687 - val_loss: 1.6354 - val_accuracy: 0.3691\n",
      "Epoch 31/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6222 - accuracy: 0.3699 - val_loss: 1.6422 - val_accuracy: 0.3658\n",
      "Epoch 32/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.6186 - accuracy: 0.3710 - val_loss: 1.6317 - val_accuracy: 0.3749\n",
      "Epoch 33/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6140 - accuracy: 0.3730 - val_loss: 1.6167 - val_accuracy: 0.3770\n",
      "Epoch 34/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6099 - accuracy: 0.3752 - val_loss: 1.6176 - val_accuracy: 0.3734\n",
      "Epoch 35/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6062 - accuracy: 0.3760 - val_loss: 1.6204 - val_accuracy: 0.3744\n",
      "Epoch 36/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.6023 - accuracy: 0.3773 - val_loss: 1.6094 - val_accuracy: 0.3783\n",
      "Epoch 37/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5983 - accuracy: 0.3783 - val_loss: 1.6037 - val_accuracy: 0.3838\n",
      "Epoch 38/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5945 - accuracy: 0.3792 - val_loss: 1.6005 - val_accuracy: 0.3851\n",
      "Epoch 39/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.5910 - accuracy: 0.3804 - val_loss: 1.6112 - val_accuracy: 0.3782\n",
      "Epoch 40/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5870 - accuracy: 0.3816 - val_loss: 1.5989 - val_accuracy: 0.3805\n",
      "Epoch 41/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5834 - accuracy: 0.3835 - val_loss: 1.5928 - val_accuracy: 0.3847\n",
      "Epoch 42/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5799 - accuracy: 0.3835 - val_loss: 1.5891 - val_accuracy: 0.3869\n",
      "Epoch 43/200\n",
      "868/868 [==============================] - 29s 33ms/step - loss: 1.5766 - accuracy: 0.3856 - val_loss: 1.5943 - val_accuracy: 0.3834\n",
      "Epoch 44/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.5727 - accuracy: 0.3872 - val_loss: 1.5822 - val_accuracy: 0.3861\n",
      "Epoch 45/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.5696 - accuracy: 0.3873 - val_loss: 1.5800 - val_accuracy: 0.3900\n",
      "Epoch 46/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5661 - accuracy: 0.3889 - val_loss: 1.5754 - val_accuracy: 0.3919\n",
      "Epoch 47/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5634 - accuracy: 0.3896 - val_loss: 1.5759 - val_accuracy: 0.3926\n",
      "Epoch 48/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5595 - accuracy: 0.3902 - val_loss: 1.5708 - val_accuracy: 0.3935\n",
      "Epoch 49/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5561 - accuracy: 0.3915 - val_loss: 1.5703 - val_accuracy: 0.3902\n",
      "Epoch 50/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5526 - accuracy: 0.3923 - val_loss: 1.5811 - val_accuracy: 0.3750\n",
      "Epoch 51/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5498 - accuracy: 0.3932 - val_loss: 1.5644 - val_accuracy: 0.3916\n",
      "Epoch 52/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.5460 - accuracy: 0.3952 - val_loss: 1.5724 - val_accuracy: 0.3916\n",
      "Epoch 53/200\n",
      "868/868 [==============================] - 28s 33ms/step - loss: 1.5431 - accuracy: 0.3962 - val_loss: 1.5595 - val_accuracy: 0.3959\n",
      "Epoch 54/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5402 - accuracy: 0.3974 - val_loss: 1.5625 - val_accuracy: 0.3935\n",
      "Epoch 55/200\n",
      "868/868 [==============================] - 26s 31ms/step - loss: 1.5370 - accuracy: 0.3988 - val_loss: 1.5554 - val_accuracy: 0.3959\n",
      "Epoch 56/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5340 - accuracy: 0.3999 - val_loss: 1.5539 - val_accuracy: 0.3953\n",
      "Epoch 57/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5304 - accuracy: 0.4000 - val_loss: 1.5573 - val_accuracy: 0.3901\n",
      "Epoch 58/200\n",
      "868/868 [==============================] - 28s 33ms/step - loss: 1.5274 - accuracy: 0.4025 - val_loss: 1.5753 - val_accuracy: 0.3831\n",
      "Epoch 59/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5249 - accuracy: 0.4033 - val_loss: 1.5455 - val_accuracy: 0.3975\n",
      "Epoch 60/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5215 - accuracy: 0.4037 - val_loss: 1.5425 - val_accuracy: 0.3987\n",
      "Epoch 61/200\n",
      "868/868 [==============================] - 29s 33ms/step - loss: 1.5190 - accuracy: 0.4045 - val_loss: 1.5401 - val_accuracy: 0.4022\n",
      "Epoch 62/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.5159 - accuracy: 0.4057 - val_loss: 1.5443 - val_accuracy: 0.3931\n",
      "Epoch 63/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.5132 - accuracy: 0.4060 - val_loss: 1.5354 - val_accuracy: 0.4009\n",
      "Epoch 64/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.5100 - accuracy: 0.4074 - val_loss: 1.5366 - val_accuracy: 0.4043\n",
      "Epoch 65/200\n",
      "868/868 [==============================] - 25s 28ms/step - loss: 1.5071 - accuracy: 0.4090 - val_loss: 1.5321 - val_accuracy: 0.3995\n",
      "Epoch 66/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.5043 - accuracy: 0.4085 - val_loss: 1.5294 - val_accuracy: 0.4071\n",
      "Epoch 67/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.5012 - accuracy: 0.4095 - val_loss: 1.5364 - val_accuracy: 0.4027\n",
      "Epoch 68/200\n",
      "868/868 [==============================] - 23s 27ms/step - loss: 1.4990 - accuracy: 0.4110 - val_loss: 1.5378 - val_accuracy: 0.3952\n",
      "Epoch 69/200\n",
      "868/868 [==============================] - 24s 27ms/step - loss: 1.4960 - accuracy: 0.4121 - val_loss: 1.5254 - val_accuracy: 0.4011\n",
      "Epoch 70/200\n",
      "868/868 [==============================] - 24s 27ms/step - loss: 1.4936 - accuracy: 0.4126 - val_loss: 1.5244 - val_accuracy: 0.4076\n",
      "Epoch 71/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4908 - accuracy: 0.4122 - val_loss: 1.5267 - val_accuracy: 0.3973\n",
      "Epoch 72/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4882 - accuracy: 0.4145 - val_loss: 1.5309 - val_accuracy: 0.3982\n",
      "Epoch 73/200\n",
      "868/868 [==============================] - 23s 27ms/step - loss: 1.4851 - accuracy: 0.4153 - val_loss: 1.5178 - val_accuracy: 0.4096\n",
      "Epoch 74/200\n",
      "868/868 [==============================] - 24s 27ms/step - loss: 1.4829 - accuracy: 0.4161 - val_loss: 1.5195 - val_accuracy: 0.4071\n",
      "Epoch 75/200\n",
      "868/868 [==============================] - 23s 27ms/step - loss: 1.4802 - accuracy: 0.4177 - val_loss: 1.5178 - val_accuracy: 0.4087\n",
      "Epoch 76/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4772 - accuracy: 0.4183 - val_loss: 1.5160 - val_accuracy: 0.4092\n",
      "Epoch 77/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4749 - accuracy: 0.4199 - val_loss: 1.5093 - val_accuracy: 0.4106\n",
      "Epoch 78/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4727 - accuracy: 0.4195 - val_loss: 1.5138 - val_accuracy: 0.4092\n",
      "Epoch 79/200\n",
      "868/868 [==============================] - 23s 27ms/step - loss: 1.4696 - accuracy: 0.4210 - val_loss: 1.5063 - val_accuracy: 0.4105\n",
      "Epoch 80/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4675 - accuracy: 0.4220 - val_loss: 1.5170 - val_accuracy: 0.4065\n",
      "Epoch 81/200\n",
      "868/868 [==============================] - 26s 31ms/step - loss: 1.4644 - accuracy: 0.4230 - val_loss: 1.5072 - val_accuracy: 0.4124\n",
      "Epoch 82/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4621 - accuracy: 0.4247 - val_loss: 1.4980 - val_accuracy: 0.4146\n",
      "Epoch 83/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4595 - accuracy: 0.4241 - val_loss: 1.4999 - val_accuracy: 0.4122\n",
      "Epoch 84/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4573 - accuracy: 0.4254 - val_loss: 1.5020 - val_accuracy: 0.4109\n",
      "Epoch 85/200\n",
      "868/868 [==============================] - 25s 28ms/step - loss: 1.4545 - accuracy: 0.4270 - val_loss: 1.5003 - val_accuracy: 0.4085\n",
      "Epoch 86/200\n",
      "868/868 [==============================] - 24s 28ms/step - loss: 1.4524 - accuracy: 0.4273 - val_loss: 1.4940 - val_accuracy: 0.4118\n",
      "Epoch 87/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4498 - accuracy: 0.4284 - val_loss: 1.5047 - val_accuracy: 0.4098\n",
      "Epoch 88/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4473 - accuracy: 0.4291 - val_loss: 1.4909 - val_accuracy: 0.4147\n",
      "Epoch 89/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4447 - accuracy: 0.4305 - val_loss: 1.4931 - val_accuracy: 0.4112\n",
      "Epoch 90/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.4420 - accuracy: 0.4313 - val_loss: 1.4937 - val_accuracy: 0.4076\n",
      "Epoch 91/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4394 - accuracy: 0.4317 - val_loss: 1.4870 - val_accuracy: 0.4151\n",
      "Epoch 92/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.4375 - accuracy: 0.4325 - val_loss: 1.4863 - val_accuracy: 0.4173\n",
      "Epoch 93/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4350 - accuracy: 0.4331 - val_loss: 1.5062 - val_accuracy: 0.4004\n",
      "Epoch 94/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4323 - accuracy: 0.4343 - val_loss: 1.4829 - val_accuracy: 0.4188\n",
      "Epoch 95/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4300 - accuracy: 0.4351 - val_loss: 1.4865 - val_accuracy: 0.4184\n",
      "Epoch 96/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4273 - accuracy: 0.4356 - val_loss: 1.4884 - val_accuracy: 0.4112\n",
      "Epoch 97/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4249 - accuracy: 0.4376 - val_loss: 1.4763 - val_accuracy: 0.4195\n",
      "Epoch 98/200\n",
      "868/868 [==============================] - 25s 28ms/step - loss: 1.4227 - accuracy: 0.4385 - val_loss: 1.4858 - val_accuracy: 0.4176\n",
      "Epoch 99/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4203 - accuracy: 0.4385 - val_loss: 1.4824 - val_accuracy: 0.4178\n",
      "Epoch 100/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4177 - accuracy: 0.4393 - val_loss: 1.4738 - val_accuracy: 0.4172\n",
      "Epoch 101/200\n",
      "868/868 [==============================] - 25s 29ms/step - loss: 1.4152 - accuracy: 0.4418 - val_loss: 1.4760 - val_accuracy: 0.4187\n",
      "Epoch 102/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4131 - accuracy: 0.4418 - val_loss: 1.4720 - val_accuracy: 0.4164\n",
      "Epoch 103/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4103 - accuracy: 0.4427 - val_loss: 1.4754 - val_accuracy: 0.4130\n",
      "Epoch 104/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4076 - accuracy: 0.4425 - val_loss: 1.4678 - val_accuracy: 0.4196\n",
      "Epoch 105/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4053 - accuracy: 0.4430 - val_loss: 1.4667 - val_accuracy: 0.4154\n",
      "Epoch 106/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.4032 - accuracy: 0.4451 - val_loss: 1.4741 - val_accuracy: 0.4196\n",
      "Epoch 107/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.4009 - accuracy: 0.4442 - val_loss: 1.4667 - val_accuracy: 0.4187\n",
      "Epoch 108/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.3979 - accuracy: 0.4471 - val_loss: 1.4788 - val_accuracy: 0.4119\n",
      "Epoch 109/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.3960 - accuracy: 0.4462 - val_loss: 1.4656 - val_accuracy: 0.4192\n",
      "Epoch 110/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3935 - accuracy: 0.4485 - val_loss: 1.4635 - val_accuracy: 0.4180\n",
      "Epoch 111/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.3910 - accuracy: 0.4491 - val_loss: 1.4592 - val_accuracy: 0.4217\n",
      "Epoch 112/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.3887 - accuracy: 0.4503 - val_loss: 1.4588 - val_accuracy: 0.4211\n",
      "Epoch 113/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.3863 - accuracy: 0.4520 - val_loss: 1.4731 - val_accuracy: 0.4170\n",
      "Epoch 114/200\n",
      "868/868 [==============================] - 26s 31ms/step - loss: 1.3845 - accuracy: 0.4514 - val_loss: 1.4707 - val_accuracy: 0.4204\n",
      "Epoch 115/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3813 - accuracy: 0.4529 - val_loss: 1.4739 - val_accuracy: 0.4181\n",
      "Epoch 116/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3791 - accuracy: 0.4546 - val_loss: 1.4617 - val_accuracy: 0.4201\n",
      "Epoch 117/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3769 - accuracy: 0.4546 - val_loss: 1.4539 - val_accuracy: 0.4212\n",
      "Epoch 118/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.3749 - accuracy: 0.4548 - val_loss: 1.4722 - val_accuracy: 0.4068\n",
      "Epoch 119/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3723 - accuracy: 0.4561 - val_loss: 1.4547 - val_accuracy: 0.4240\n",
      "Epoch 120/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.3705 - accuracy: 0.4575 - val_loss: 1.4505 - val_accuracy: 0.4233\n",
      "Epoch 121/200\n",
      "868/868 [==============================] - 26s 31ms/step - loss: 1.3677 - accuracy: 0.4582 - val_loss: 1.4499 - val_accuracy: 0.4244\n",
      "Epoch 122/200\n",
      "868/868 [==============================] - 27s 32ms/step - loss: 1.3653 - accuracy: 0.4591 - val_loss: 1.4574 - val_accuracy: 0.4228\n",
      "Epoch 123/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3627 - accuracy: 0.4609 - val_loss: 1.4509 - val_accuracy: 0.4198\n",
      "Epoch 124/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.3609 - accuracy: 0.4623 - val_loss: 1.4614 - val_accuracy: 0.4185\n",
      "Epoch 125/200\n",
      "868/868 [==============================] - 26s 30ms/step - loss: 1.3581 - accuracy: 0.4622 - val_loss: 1.4516 - val_accuracy: 0.4232\n",
      "Epoch 126/200\n",
      "868/868 [==============================] - 28s 32ms/step - loss: 1.3568 - accuracy: 0.4629 - val_loss: 1.4450 - val_accuracy: 0.4254\n",
      "Epoch 127/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3538 - accuracy: 0.4635 - val_loss: 1.4512 - val_accuracy: 0.4250\n",
      "Epoch 128/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3522 - accuracy: 0.4645 - val_loss: 1.4585 - val_accuracy: 0.4168\n",
      "Epoch 129/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3492 - accuracy: 0.4659 - val_loss: 1.4440 - val_accuracy: 0.4236\n",
      "Epoch 130/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3471 - accuracy: 0.4669 - val_loss: 1.4464 - val_accuracy: 0.4230\n",
      "Epoch 131/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3450 - accuracy: 0.4662 - val_loss: 1.4483 - val_accuracy: 0.4245\n",
      "Epoch 132/200\n",
      "868/868 [==============================] - 27s 31ms/step - loss: 1.3428 - accuracy: 0.4679 - val_loss: 1.4446 - val_accuracy: 0.4198\n",
      "Epoch 133/200\n",
      "701/868 [=======================>......] - ETA: 4s - loss: 1.3388 - accuracy: 0.4713"
     ]
    }
   ],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskTwo(X_train, y_age_train, X_test, y_age_test, 0.003, 200, 100, log_folder)\n",
    "%tensorboard --logdir logs --port=6022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskTwo(X_train, y_race_train, X_test, y_race_test, 0.003, 120, 100, log_folder)\n",
    "%tensorboard --logdir logs --port=6023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e3f14-890a-4ffa-a3e0-7d5be46ed633",
   "metadata": {},
   "source": [
    "## Task 3: Your own Convolutional Neural Network\n",
    "1. Build another convolutional neural network, where you choose all the parameters to see if you can get a higher accuracy.\n",
    "2. Using Min-Max scaling to scale the training dataset and using the same Min and Max values from the training set scale the test dataset ( X−Xmin/Xmax−Xmin ).\n",
    "3. Using mini-batch gradient descent to optimize the loss function: “categorical cross-entropy” on the training dataset. Please record the loss value for each of the epochs and create an epoch-loss plot and an accuracy-loss plot for both the training and validation set.\n",
    "4. Report the following:\n",
    "    - Final classification accuracy.\n",
    "    - The n-class confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af4e16-49e9-4ed8-8518-b4152c545e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CNN network model\n",
    "def createOwnCNN(inputShape, outputSize, lr):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=inputShape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(outputSize, activation='softmax'))\n",
    "    opt = optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized data\n",
    "#X_train, y_age_train, y_race_train, age_labels, race_labels = createXY('project3_COSC525/train/', 'project3_COSC525/fairface_label_train.csv', 10000)\n",
    "#X_test, y_age_test, y_race_test, _ , _ = createXY('project3_COSC525/val/', 'project3_COSC525/fairface_label_val.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task three; train and evaluate CNN for given train/test data\n",
    "def taskThree(X_train, y_train, X_test, y_test, lr, numEpochs, batchSize, log_folder):\n",
    "    model = createOwnCNN((32, 32, 1), len(y_train[0]), lr)\n",
    "    callbacks = createCallbacks(log_folder)\n",
    "    model.fit(np.reshape(X_train, (X_train.shape[0], 32, 32)), y_train, validation_data=(np.reshape(X_test, (X_test.shape[0], 32, 32)), y_test), epochs=numEpochs, batch_size=batchSize, callbacks=callbacks)\n",
    "    y_true = getMax(y_test)\n",
    "    y_pred = getMax(model.predict(np.reshape(X_test, (X_test.shape[0], 32, 32))))\n",
    "    eval = tf.keras.metrics.Accuracy()\n",
    "    eval.update_state(y_true, y_pred)\n",
    "    print('Accuracy: ', eval.result().numpy())\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(c_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45165262",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskThree(X_train, y_age_train, X_test, y_age_test, 0.001, 120, 100, log_folder)\n",
    "%tensorboard --logdir logs --port=6024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskThree(X_train, y_race_train, X_test, y_race_test, 0.001, 120, 100, log_folder\n",
    "%tensorboard --logdir logs --port=6025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0a435-0e8d-4b79-80b7-a1dcc3500a93",
   "metadata": {},
   "source": [
    "## Task 4: Your own Convolutional Neural Network on both Tasks Simultaneously\n",
    "1. Build another convolutional neural network, where you try and classify both tasks with a single network. After your flatten layer have two more fully connected layers for each “branch”. Note that in order to do so you will not be able to use the Sequential model.\n",
    "2. Using Min-Max scaling to scale the training dataset and using the same Min and Max values from the training set scale the test dataset ( X−Xmin/Xmax−Xmin ).\n",
    "3. Using mini-batch gradient descent to optimize the loss function: “categorical cross-entropy” on the training dataset. Please record the loss value for each of the epochs and create an epoch-loss plot and an accuracy-loss plot for both the training and validation set.\n",
    "4. Report the following:\n",
    "    - Final classification accuracy.\n",
    "    - The n-class confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34adc1f7-cadb-4126-a36e-6fbf4abd54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CNN network model\n",
    "def createOwnCNNTwoTasks(inputShape, outputSizes, lr):\n",
    "    input = layers.Input(shape = inputShape)\n",
    "    conv1 = layers.Conv2D(32, (3, 3), activation='relu', name='conv1')(input)\n",
    "    max1 = layers.MaxPooling2D((2, 2), name='max1')(conv1)\n",
    "    conv2 = layers.Conv2D(64, (3, 3), activation='relu', name='conv2')(max1)\n",
    "    max2 = layers.MaxPooling2D((2, 2), name='max2')(conv2)\n",
    "    flatten = layers.Flatten()(max2)\n",
    "\n",
    "    # Branch 1\n",
    "    fc11 = layers.Dense(100, activation='relu', name='fc11')(flatten)\n",
    "    fc12 = layers.Dense(outputSizes[0], activation='softmax', name='fc12')(fc11)\n",
    "\n",
    "    # Branch 2\n",
    "    fc21 = layers.Dense(100, activation='relu', name='fc21')(flatten)\n",
    "    fc22 = layers.Dense(outputSizes[1], activation='softmax', name='fc22')(fc21)\n",
    "\n",
    "    # Concatenate output of branches\n",
    "    output = layers.concatenate([fc12, fc22])\n",
    "    model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    opt = optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized data\n",
    "#X_train, y_age_train, y_race_train, age_labels, race_labels = createXY('project3_COSC525/train/', 'project3_COSC525/fairface_label_train.csv', 10000)\n",
    "#X_test, y_age_test, y_race_test, _ , _ = createXY('project3_COSC525/val/', 'project3_COSC525/fairface_label_val.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task four; train and evaluate CNN for given train/test data\n",
    "def taskFour(X_train, y_trains, X_test, y_tests, lr, numEpochs, batchSize, log_folder):\n",
    "    callbacks = createCallbacks(log_folder)\n",
    "    model = createOwnCNNTwoTasks((32, 32, 1), (len(y_trains[0][0]), len(y_trains[1][0])), lr)\n",
    "    model.fit(np.reshape(X_train, (X_train.shape[0], 32, 32)), np.concatenate((y_trains[0], y_trains[1]), axis=1), validation_data=(np.reshape(X_test, (X_test.shape[0], 32, 32)), np.concatenate((y_tests[0], y_tests[1]), axis=1)), epochs=numEpochs, batch_size=batchSize, callbacks=callbacks)\n",
    "    y_true = getMax(np.concatenate((y_tests[0], y_tests[1]), axis=1))\n",
    "    y_pred = getMax(model.predict(np.reshape(X_test, (X_test.shape[0], 32, 32))))\n",
    "    eval = tf.keras.metrics.Accuracy()\n",
    "    eval.update_state(y_true, y_pred)\n",
    "    print('Accuracy: ', eval.result().numpy())\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "taskFour(X_train, (y_age_train, y_race_train), X_test, (y_age_test, y_race_test), 0.001, 120, 100)\n",
    "%tensorboard --logdir logs --port=6026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8e48d-e509-40f8-9938-44e9106634fd",
   "metadata": {},
   "source": [
    "## Task 5: Variational Auto Encoder (COSC 525 only)\n",
    "1. Build a variational autoencoder with the following specifications (in this one you have a little more flexibility):\n",
    "    - Should have at least two convolution layers in the encoder and 2 deconvolution layers in the decoder.\n",
    "    - Latent dimension should be at least 5.\n",
    "    - Loss should be either MSE or binary cross entropy.\n",
    "2. Using Min-Max scaling to scale the training dataset and using the same Min and Max values from the training set scale the test dataset ( X−Xmin/Xmax−Xmin ).\n",
    "3. Using mini-batch gradient descent to optimize the loss function on the training dataset. Please record the loss value for each of the epochs and create an epoch-loss plot and an accuracy-loss plot for both the training and validation set.\n",
    "4. Qualitatively evaluate your model by generating a set of faces by randomly choosing 10 latent vectors and presenting the resulting images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6477f",
   "metadata": {},
   "source": [
    "VAE code adapted from https://keras.io/examples/generative/vae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef84de-003b-4121-b3eb-1bbe48339d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "latent_dim = 15\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(32, 32, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(2 * 2 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((2, 2, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35903b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational AutoEncoder model class\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train VAE on all face images\n",
    "X_train, _, _, _, _ = createXY('project3_COSC525/train/', 'project3_COSC525/fairface_label_train.csv', 'age', 10000)\n",
    "X_test, _, _ , _, _ = createXY('project3_COSC525/val/', 'project3_COSC525/fairface_label_val.csv', 'age', 1000)\n",
    "all_faces = np.concatenate([X_train, X_test], axis=0)\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0005))\n",
    "vae.fit(np.reshape(all_faces, (11000, 32, 32, 1)), epochs=10000, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cdc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generating some (non-random) faces\n",
    "preds = vae.encoder.predict(np.reshape(all_faces, (10, 32, 32, 1)))[0]\n",
    "# gen = vae.decoder.predict(np.reshape(np.mean(preds, axis=0), (1, 5)))[0]\n",
    "gen = vae.decoder.predict(np.reshape(preds[4], (1, latent_dim)))[0]\n",
    "gen = np.reshape(gen, (32, 32))\n",
    "im = Image.fromarray(np.uint8(gen * 255), 'L')\n",
    "im.save(\"test.png\")\n",
    "comp_im = Image.open(\"project3_COSC525/train/5.jpg\")\n",
    "comp_im.save(\"test_comp.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e23b663014ca37592679400f38d6d04e3fc85e5c6f63651f80341cade9e63d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
