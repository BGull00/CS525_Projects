{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from art.attacks.evasion import PixelAttack\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from onePixelAttack import loadAttackData\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def preprocess_image_input(input_images):\n",
    "  input_images = input_images.astype('float32')\n",
    "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
    "  return output_ims\n",
    "\n",
    "def feature_extractor(inputs):\n",
    "    feature_extractor = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n",
    "                                                              include_top=False,\n",
    "                                                              weights='imagenet')(inputs)\n",
    "    return feature_extractor\n",
    "\n",
    "def classifier(inputs):\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
    "    return x\n",
    "\n",
    "def final_model(inputs):\n",
    "    resize = tf.keras.layers.UpSampling2D(size=(7,7))(inputs)\n",
    "    resnet_feature_extractor = feature_extractor(resize)\n",
    "    classification_output = classifier(resnet_feature_extractor)\n",
    "    return classification_output\n",
    "\n",
    "def define_compile_model():\n",
    "  inputs = tf.keras.layers.Input(shape=(32,32,3))\n",
    "  classification_output = final_model(inputs) \n",
    "  model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n",
    "  model.compile(optimizer='SGD', \n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics = ['accuracy']) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels) , (validation_images, validation_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "train_X = preprocess_image_input(training_images)\n",
    "valid_X = preprocess_image_input(validation_images)\n",
    "resnet = define_compile_model()\n",
    "resnet = tf.keras.models.load_model('./trainedModel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics I used in a previous ML class\n",
    "def analysis(model, X, Y, classes=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']):\n",
    "    ypred = np.argmax(model.predict(X), axis=1)\n",
    "    cm = confusion_matrix(Y, ypred)\n",
    "    cr = classification_report(Y, ypred)\n",
    "    print('Test Statistics:', cr, sep='\\n', end='\\n\\n\\n')\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    sns.heatmap(cm, linewidth=0.5, annot=cm, xticklabels=classes, yticklabels=classes)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_attacked, X_unattacked, y = loadAttackData('attacked.imgs')\n",
    "X_attacked = np.asarray(X_attacked) / 255   # Scale RGB values between 0 and 1.0\n",
    "X_unattacked = np.asarray(X_unattacked) / 255   # Scale RGB values between 0 and 1.0\n",
    "X_attacked_test = X_attacked[0:int(len(X_attacked)/3)]\n",
    "X_attacked_train = X_attacked[int(len(X_attacked)/3):]\n",
    "X_unattacked_test = X_unattacked[0:int(len(X_unattacked)/3)]\n",
    "X_unattacked_train = X_unattacked[int(len(X_unattacked)/3):]\n",
    "y_test = np.asarray(y[0:int(len(y)/3)])\n",
    "y_train = np.asarray(y[int(len(y)/3):])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Resnet model with unattacked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 48s 2s/step\n",
      "Test Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.96      0.74       103\n",
      "           1       0.95      0.83      0.88        88\n",
      "           2       0.82      0.78      0.80       103\n",
      "           3       0.60      0.77      0.67        91\n",
      "           4       0.89      0.75      0.81       102\n",
      "           5       0.94      0.53      0.68        86\n",
      "           6       0.90      0.82      0.86       104\n",
      "           7       0.75      0.88      0.81        94\n",
      "           8       0.94      0.80      0.86       110\n",
      "           9       0.96      0.86      0.91       107\n",
      "\n",
      "    accuracy                           0.80       988\n",
      "   macro avg       0.83      0.80      0.80       988\n",
      "weighted avg       0.84      0.80      0.81       988\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis(resnet, X_unattacked_train*255, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Resnet with Attacked Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 24s 2s/step\n",
      "Test Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.90      0.45        50\n",
      "           1       0.81      0.73      0.77        63\n",
      "           2       0.29      0.47      0.36        43\n",
      "           3       0.33      0.46      0.39        46\n",
      "           4       0.50      0.10      0.17        50\n",
      "           5       0.56      0.38      0.45        40\n",
      "           6       0.82      0.17      0.28        53\n",
      "           7       0.74      0.51      0.60        49\n",
      "           8       0.56      0.45      0.50        44\n",
      "           9       0.82      0.56      0.67        55\n",
      "\n",
      "    accuracy                           0.48       493\n",
      "   macro avg       0.57      0.47      0.46       493\n",
      "weighted avg       0.59      0.48      0.47       493\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis(resnet, X_attacked_test*255, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code for counter adversarial models (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_conv_denoising_autoencoder(input_shape, should_log = False):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='same', activation = 'relu', input_shape = input_shape))\n",
    "    model.add(layers.BatchNormalization()) # H, W, 16\n",
    "    model.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 2, padding='same', activation = 'relu'))\n",
    "    model.add(layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization()) # H/2, W/2, 32\n",
    "    model.add(layers.Conv2DTranspose(32, kernel_size = 3, activation=\"relu\", strides=2, padding=\"same\"))\n",
    "    model.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization()) # H, W, 16\n",
    "    model.add(layers.Conv2D(filters = input_shape[2], kernel_size = 1, strides = 1, padding='same', activation = 'sigmoid'))\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.01, decay_steps = 500, decay_rate = 0.5)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), loss='mean_squared_error')\n",
    "\n",
    "    if should_log:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "''' Create denoising autoencoder model that uses fully connected layers '''\n",
    "def create_fully_conn_denoising_autoencoder(input_shape, should_log = False):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    input_size = 1\n",
    "    for val in input_shape:\n",
    "        input_size *= val\n",
    "\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units = 128, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(units = input_size, activation = 'sigmoid'))\n",
    "    model.add(layers.Reshape(input_shape))\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.001, decay_steps = 1000, decay_rate = 0.5)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), loss='mean_squared_error')\n",
    "\n",
    "    if should_log:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "''' Create generative adversarial denoising autoencoder model that uses fully convolutional layers in the\n",
    "autoencoder followed by a discriminator that also uses convolutional layers '''\n",
    "def create_GAN_denoising_autoencoder(input_shape, should_log = False):\n",
    "\n",
    "    autoencoder = Sequential()\n",
    "    autoencoder.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='same', activation = 'relu', input_shape = input_shape))\n",
    "    autoencoder.add(layers.BatchNormalization()) # H, W, 16\n",
    "    autoencoder.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 2, padding='same', activation = 'relu'))\n",
    "    autoencoder.add(layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "    autoencoder.add(layers.BatchNormalization()) # H/2, W/2, 32\n",
    "    autoencoder.add(layers.Conv2DTranspose(32, kernel_size = 3, activation=\"relu\", strides=2, padding=\"same\"))\n",
    "    autoencoder.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "    autoencoder.add(layers.BatchNormalization()) # H, W, 16\n",
    "    autoencoder.add(layers.Conv2D(filters = input_shape[2], kernel_size = 1, strides = 1, padding='same', activation = 'sigmoid'))\n",
    "\n",
    "    if should_log:\n",
    "        autoencoder.summary()\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding='same', activation = layers.LeakyReLU(), input_shape = input_shape))\n",
    "    discriminator.add(layers.BatchNormalization()) # H, W, 16\n",
    "    discriminator.add(layers.Dropout(0.5))\n",
    "    discriminator.add(layers.Conv2D(filters = 16, kernel_size = 3, strides = 2, padding='same', activation = layers.LeakyReLU()))\n",
    "    discriminator.add(layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding='same', activation = layers.LeakyReLU()))\n",
    "    discriminator.add(layers.BatchNormalization()) # H/2, W/2, 32\n",
    "    discriminator.add(layers.Dropout(0.5))\n",
    "    discriminator.add(layers.Flatten())\n",
    "    discriminator.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    if should_log:\n",
    "        discriminator.summary()\n",
    "\n",
    "    return autoencoder, discriminator\n",
    "\n",
    "\n",
    "''' Function used to calculate loss values for generative adversarial denoising autoencoder model '''\n",
    "def get_GAN_denoising_autoencoder_losses(unattacked_images, denoised_images, denoised_disc_output, unattacked_disc_output, mse, binary_cross_entropy):\n",
    "\n",
    "    # Calculate autoencoder loss (sum of MSE and binary cross entropy from discriminator being fooled\n",
    "    # into thinking denoised images are unattacked)\n",
    "    mse_autoencoder_loss = mse(unattacked_images, denoised_images)\n",
    "    discriminator_output_loss = binary_cross_entropy(tf.ones_like(denoised_disc_output), denoised_disc_output)\n",
    "    autoencoder_loss = 75 * mse_autoencoder_loss + discriminator_output_loss\n",
    "\n",
    "    # Calculate discriminator loss (sum of binary cross entropy from correctly classifying unattacked\n",
    "    # images as unattacked and from correctly classifying denoised images as denoised)\n",
    "    unattacked_discriminator_loss = binary_cross_entropy(tf.ones_like(unattacked_disc_output), unattacked_disc_output)\n",
    "    denoised_discriminator_loss = binary_cross_entropy(tf.zeros_like(denoised_disc_output), denoised_disc_output)\n",
    "    discriminator_loss = unattacked_discriminator_loss + denoised_discriminator_loss\n",
    "\n",
    "    return autoencoder_loss, discriminator_loss\n",
    "\n",
    "\n",
    "''' Function used by TensorFlow to train the generative adversarial denoising autoencoder for a single step;\n",
    "credit: https://www.tensorflow.org/tutorials/generative/dcgan '''\n",
    "@tf.function\n",
    "def train_GAN_denoising_autoencoder_step(attacked_images, unattacked_images, autoencoder, discriminator, mse, binary_cross_entropy, autoencoder_optimizer, discriminator_optimizer):\n",
    "\n",
    "    # Perform training step\n",
    "    with tf.GradientTape() as autoencoder_tape, tf.GradientTape() as discriminator_tape:\n",
    "        denoised_images = autoencoder(attacked_images, training=True)\n",
    "\n",
    "        denoised_disc_output = discriminator(denoised_images, training=True)\n",
    "        unattacked_disc_output = discriminator(unattacked_images, training=True)\n",
    "\n",
    "        autoencoder_loss, discriminator_loss = get_GAN_denoising_autoencoder_losses(unattacked_images, denoised_images, denoised_disc_output, unattacked_disc_output, mse, binary_cross_entropy)\n",
    "\n",
    "    gradients_of_discriminator = discriminator_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    gradients_of_autoencoder = autoencoder_tape.gradient(autoencoder_loss, autoencoder.trainable_variables)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients_of_autoencoder, autoencoder.trainable_variables))\n",
    "\n",
    "\n",
    "''' Function used to train generative adversarial denoising autoencoder model;\n",
    "credit: https://www.tensorflow.org/tutorials/generative/dcgan '''\n",
    "def train_GAN_denoising_autoencoder(attacked_images_train, unattacked_images_train, attacked_images_val, unattacked_images_val, num_epochs, batch_size, autoencoder, discriminator):\n",
    "\n",
    "    # Init loss functions from Keras\n",
    "    mse = keras.losses.MeanSquaredError()\n",
    "    binary_cross_entropy = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    # Init Adam optimizers for both autoencoder and generator\n",
    "    autoencoder_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    # Calculate number of batches per epoch\n",
    "    num_batches_per_epoch = int(attacked_images_train.shape[0] / batch_size)\n",
    "\n",
    "    # Train for given number of epochs\n",
    "    autoencoder_losses_train = []\n",
    "    discriminator_losses_train = []\n",
    "    autoencoder_losses_val = []\n",
    "    discriminator_losses_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print('Starting epoch ' + str(epoch+1) + '/' + str(num_epochs))\n",
    "        \n",
    "        # Train one step for each batch in a single epoch\n",
    "        for _ in range(num_batches_per_epoch):\n",
    "\n",
    "            # Randomly select batch of batch_size samples from training data\n",
    "            batch_rand_img_inds = np.random.randint(0, attacked_images_train.shape[0], batch_size)\n",
    "            attacked_images_batch = attacked_images_train[batch_rand_img_inds]\n",
    "            unattacked_images_batch = unattacked_images_train[batch_rand_img_inds]\n",
    "\n",
    "            # Perform training step on current batch of attacked and unattacked images\n",
    "            train_GAN_denoising_autoencoder_step(attacked_images_batch, unattacked_images_batch, autoencoder, discriminator, mse, binary_cross_entropy, autoencoder_optimizer, discriminator_optimizer)\n",
    "            \n",
    "        # Calculate and print losses for the epoch for all training data\n",
    "        denoised_images_train = autoencoder(attacked_images_train, training = False)\n",
    "        denoised_disc_output_train = discriminator(denoised_images_train, training = False)\n",
    "        unattacked_disc_output_train = discriminator(unattacked_images_train, training = False)\n",
    "        autoencoder_loss_train, discriminator_loss_train = get_GAN_denoising_autoencoder_losses(unattacked_images_train, denoised_images_train, denoised_disc_output_train, unattacked_disc_output_train, mse, binary_cross_entropy)\n",
    "\n",
    "        print('    autoencoder_loss_train: ' + str(float(autoencoder_loss_train)) + ' - discriminator_loss_train: ' + str(float(discriminator_loss_train)))\n",
    "        autoencoder_losses_train.append(autoencoder_loss_train)\n",
    "        discriminator_losses_train.append(discriminator_loss_train)\n",
    "\n",
    "        # Calculate and print losses for the epoch for all validation data\n",
    "        denoised_images_val = autoencoder(attacked_images_val, training = False)\n",
    "        denoised_disc_output_val = discriminator(denoised_images_val, training = False)\n",
    "        unattacked_disc_output_val = discriminator(unattacked_images_val, training = False)\n",
    "        autoencoder_loss_val, discriminator_loss_val = get_GAN_denoising_autoencoder_losses(unattacked_images_val, denoised_images_val, denoised_disc_output_val, unattacked_disc_output_val, mse, binary_cross_entropy)\n",
    "\n",
    "        print('    autoencoder_loss_val: ' + str(float(autoencoder_loss_val)) + ' - discriminator_loss_val: ' + str(float(discriminator_loss_val)))\n",
    "        autoencoder_losses_val.append(autoencoder_loss_val)\n",
    "        discriminator_losses_val.append(discriminator_loss_val)\n",
    "    \n",
    "    return autoencoder_losses_train, discriminator_losses_train, autoencoder_losses_val, discriminator_losses_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 3s 406ms/step - loss: 0.0548 - val_loss: 0.0914\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 1s 363ms/step - loss: 0.0172 - val_loss: 0.2186\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 1s 366ms/step - loss: 0.0137 - val_loss: 0.2282\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 1s 369ms/step - loss: 0.0113 - val_loss: 0.2259\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 1s 380ms/step - loss: 0.0092 - val_loss: 0.2440\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 1s 378ms/step - loss: 0.0080 - val_loss: 0.2428\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 1s 378ms/step - loss: 0.0071 - val_loss: 0.2344\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 1s 375ms/step - loss: 0.0065 - val_loss: 0.2187\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 0.0060 - val_loss: 0.2074\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 1s 365ms/step - loss: 0.0056 - val_loss: 0.1830\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 1s 360ms/step - loss: 0.0053 - val_loss: 0.1706\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 1s 363ms/step - loss: 0.0051 - val_loss: 0.1514\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 1s 362ms/step - loss: 0.0048 - val_loss: 0.1359\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 1s 365ms/step - loss: 0.0047 - val_loss: 0.1176\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 1s 363ms/step - loss: 0.0046 - val_loss: 0.1081\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 1s 362ms/step - loss: 0.0044 - val_loss: 0.0894\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 0.0042 - val_loss: 0.0813\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 1s 365ms/step - loss: 0.0040 - val_loss: 0.0739\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 0.0039 - val_loss: 0.0652\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 0.0040 - val_loss: 0.0580\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 1s 368ms/step - loss: 0.0037 - val_loss: 0.0513\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 1s 372ms/step - loss: 0.0037 - val_loss: 0.0491\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 0.0035 - val_loss: 0.0441\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 1s 371ms/step - loss: 0.0034 - val_loss: 0.0422\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 0.0033 - val_loss: 0.0391\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 1s 375ms/step - loss: 0.0033 - val_loss: 0.0371\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 1s 379ms/step - loss: 0.0031 - val_loss: 0.0357\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 1s 381ms/step - loss: 0.0029 - val_loss: 0.0322\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 0.0028 - val_loss: 0.0327\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 0.0026 - val_loss: 0.0306\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 0.0025 - val_loss: 0.0303\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 0.0023 - val_loss: 0.0284\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 1s 372ms/step - loss: 0.0022 - val_loss: 0.0279\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 0.0021 - val_loss: 0.0292\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 1s 372ms/step - loss: 0.0022 - val_loss: 0.0281\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 1s 365ms/step - loss: 0.0020 - val_loss: 0.0290\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 1s 377ms/step - loss: 0.0018 - val_loss: 0.0280\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 0.0018 - val_loss: 0.0278\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 1s 373ms/step - loss: 0.0017 - val_loss: 0.0261\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 0.0017 - val_loss: 0.0254\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 0.0016 - val_loss: 0.0242\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 1s 390ms/step - loss: 0.0016 - val_loss: 0.0240\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 1s 375ms/step - loss: 0.0015 - val_loss: 0.0229\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 1s 375ms/step - loss: 0.0016 - val_loss: 0.0222\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 1s 375ms/step - loss: 0.0015 - val_loss: 0.0211\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 0.0015 - val_loss: 0.0197\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 0.0014 - val_loss: 0.0215\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 0.0014 - val_loss: 0.0186\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 1s 378ms/step - loss: 0.0014 - val_loss: 0.0190\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 0.0014 - val_loss: 0.0179\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 0.0013 - val_loss: 0.0170\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 0.0013 - val_loss: 0.0164\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0013 - val_loss: 0.0150\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 0.0013 - val_loss: 0.0158\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.0012 - val_loss: 0.0152\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0012 - val_loss: 0.0135\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 0.0011 - val_loss: 0.0148\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 0.0011 - val_loss: 0.0127\n",
      "Epoch 59/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 0.0011 - val_loss: 0.0134\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0011 - val_loss: 0.0125\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 0.0011 - val_loss: 0.0126\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 0.0010 - val_loss: 0.0116\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0010 - val_loss: 0.0114\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 0.0010 - val_loss: 0.0124\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 0.0011 - val_loss: 0.0102\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 0.0012 - val_loss: 0.0126\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0012 - val_loss: 0.0099\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 2s 384ms/step - loss: 0.0010 - val_loss: 0.0115\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 0.0010 - val_loss: 0.0101\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 0.0010 - val_loss: 0.0095\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0010 - val_loss: 0.0100\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 0.0011 - val_loss: 0.0084\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 9.6726e-04 - val_loss: 0.0094\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0010 - val_loss: 0.0075\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 2s 445ms/step - loss: 9.1476e-04 - val_loss: 0.0084\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 8.9369e-04 - val_loss: 0.0075\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 8.5125e-04 - val_loss: 0.0075\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 8.3773e-04 - val_loss: 0.0075\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 8.5275e-04 - val_loss: 0.0066\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 8.7560e-04 - val_loss: 0.0070\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 9.7030e-04 - val_loss: 0.0060\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 8.3506e-04 - val_loss: 0.0069\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 7.9992e-04 - val_loss: 0.0062\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 8.3343e-04 - val_loss: 0.0064\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 8.0297e-04 - val_loss: 0.0062\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 8.1185e-04 - val_loss: 0.0056\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 7.6615e-04 - val_loss: 0.0055\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 7.4758e-04 - val_loss: 0.0052\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 7.5810e-04 - val_loss: 0.0051\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 9.0884e-04 - val_loss: 0.0052\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 7.6806e-04 - val_loss: 0.0051\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 7.5525e-04 - val_loss: 0.0046\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 7.8235e-04 - val_loss: 0.0047\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 7.5977e-04 - val_loss: 0.0041\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 7.5266e-04 - val_loss: 0.0042\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 7.7093e-04 - val_loss: 0.0038\n",
      "Epoch 97/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 7.8615e-04 - val_loss: 0.0038\n",
      "Epoch 98/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 6.8871e-04 - val_loss: 0.0040\n",
      "Epoch 99/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 7.9218e-04 - val_loss: 0.0036\n",
      "Epoch 100/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 7.3659e-04 - val_loss: 0.0033\n",
      "Epoch 101/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 7.3926e-04 - val_loss: 0.0032\n",
      "Epoch 102/1000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 6.9354e-04 - val_loss: 0.0032\n",
      "Epoch 103/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 7.2341e-04 - val_loss: 0.0027\n",
      "Epoch 104/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 6.9200e-04 - val_loss: 0.0029\n",
      "Epoch 105/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 6.4077e-04 - val_loss: 0.0034\n",
      "Epoch 106/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 6.4764e-04 - val_loss: 0.0028\n",
      "Epoch 107/1000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 6.8986e-04 - val_loss: 0.0029\n",
      "Epoch 108/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 6.7728e-04 - val_loss: 0.0027\n",
      "Epoch 109/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 6.1306e-04 - val_loss: 0.0026\n",
      "Epoch 110/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 6.4931e-04 - val_loss: 0.0030\n",
      "Epoch 111/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 7.0995e-04 - val_loss: 0.0024\n",
      "Epoch 112/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 6.5860e-04 - val_loss: 0.0027\n",
      "Epoch 113/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 6.1295e-04 - val_loss: 0.0027\n",
      "Epoch 114/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 6.2614e-04 - val_loss: 0.0022\n",
      "Epoch 115/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 6.2053e-04 - val_loss: 0.0024\n",
      "Epoch 116/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 5.8331e-04 - val_loss: 0.0022\n",
      "Epoch 117/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 6.8133e-04 - val_loss: 0.0021\n",
      "Epoch 118/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 6.6024e-04 - val_loss: 0.0019\n",
      "Epoch 119/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 6.0031e-04 - val_loss: 0.0021\n",
      "Epoch 120/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 5.9677e-04 - val_loss: 0.0022\n",
      "Epoch 121/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 6.6563e-04 - val_loss: 0.0020\n",
      "Epoch 122/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 5.8226e-04 - val_loss: 0.0020\n",
      "Epoch 123/1000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 6.5459e-04 - val_loss: 0.0015\n",
      "Epoch 124/1000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 7.1930e-04 - val_loss: 0.0018\n",
      "Epoch 125/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 6.2116e-04 - val_loss: 0.0018\n",
      "Epoch 126/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 6.1028e-04 - val_loss: 0.0014\n",
      "Epoch 127/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 6.1867e-04 - val_loss: 0.0015\n",
      "Epoch 128/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 5.7920e-04 - val_loss: 0.0016\n",
      "Epoch 129/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.1763e-04 - val_loss: 0.0018\n",
      "Epoch 130/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.2889e-04 - val_loss: 0.0014\n",
      "Epoch 131/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 5.7896e-04 - val_loss: 0.0016\n",
      "Epoch 132/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.1575e-04 - val_loss: 0.0013\n",
      "Epoch 133/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.0160e-04 - val_loss: 0.0014\n",
      "Epoch 134/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.3832e-04 - val_loss: 0.0015\n",
      "Epoch 135/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 5.6479e-04 - val_loss: 0.0012\n",
      "Epoch 136/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.3311e-04 - val_loss: 0.0014\n",
      "Epoch 137/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 5.9452e-04 - val_loss: 0.0012\n",
      "Epoch 138/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.1584e-04 - val_loss: 0.0011\n",
      "Epoch 139/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 4.9017e-04 - val_loss: 0.0012\n",
      "Epoch 140/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 5.1224e-04 - val_loss: 0.0012\n",
      "Epoch 141/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 4.9963e-04 - val_loss: 0.0011\n",
      "Epoch 142/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 6.6898e-04 - val_loss: 0.0013\n",
      "Epoch 143/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.1735e-04 - val_loss: 9.9327e-04\n",
      "Epoch 144/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 5.6154e-04 - val_loss: 0.0012\n",
      "Epoch 145/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 4.9521e-04 - val_loss: 8.6121e-04\n",
      "Epoch 146/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.4641e-04 - val_loss: 0.0012\n",
      "Epoch 147/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 4.7488e-04 - val_loss: 9.0001e-04\n",
      "Epoch 148/1000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 5.6876e-04 - val_loss: 0.0011\n",
      "Epoch 149/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 5.4582e-04 - val_loss: 7.8436e-04\n",
      "Epoch 150/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.2604e-04 - val_loss: 0.0010\n",
      "Epoch 151/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 5.3597e-04 - val_loss: 9.9790e-04\n",
      "Epoch 152/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 4.8188e-04 - val_loss: 8.2463e-04\n",
      "Epoch 153/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 4.5393e-04 - val_loss: 9.4132e-04\n",
      "Epoch 154/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 5.4267e-04 - val_loss: 9.6277e-04\n",
      "Epoch 155/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.5287e-04 - val_loss: 7.9135e-04\n",
      "Epoch 156/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 5.1247e-04 - val_loss: 8.2214e-04\n",
      "Epoch 157/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 4.6664e-04 - val_loss: 8.5019e-04\n",
      "Epoch 158/1000\n",
      "4/4 [==============================] - 1s 381ms/step - loss: 4.7354e-04 - val_loss: 0.0011\n",
      "Epoch 159/1000\n",
      "4/4 [==============================] - 2s 406ms/step - loss: 4.2942e-04 - val_loss: 8.0658e-04\n",
      "Epoch 160/1000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 4.6788e-04 - val_loss: 8.3590e-04\n",
      "Epoch 161/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 4.6629e-04 - val_loss: 8.4859e-04\n",
      "Epoch 162/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 4.4396e-04 - val_loss: 0.0012\n",
      "Epoch 163/1000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 4.9587e-04 - val_loss: 7.6699e-04\n",
      "Epoch 164/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 4.3683e-04 - val_loss: 9.0483e-04\n",
      "Epoch 165/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 5.2481e-04 - val_loss: 7.0085e-04\n",
      "Epoch 166/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 4.2846e-04 - val_loss: 9.4662e-04\n",
      "Epoch 167/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 5.1223e-04 - val_loss: 6.7259e-04\n",
      "Epoch 168/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 4.7085e-04 - val_loss: 6.9333e-04\n",
      "Epoch 169/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 4.6857e-04 - val_loss: 0.0010\n",
      "Epoch 170/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 5.0996e-04 - val_loss: 9.1473e-04\n",
      "Epoch 171/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 4.3066e-04 - val_loss: 0.0010\n",
      "Epoch 172/1000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 4.6733e-04 - val_loss: 6.0767e-04\n",
      "Epoch 173/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 4.7948e-04 - val_loss: 5.8009e-04\n",
      "Epoch 174/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 4.7124e-04 - val_loss: 7.8102e-04\n",
      "Epoch 175/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 5.1848e-04 - val_loss: 7.8908e-04\n",
      "Epoch 176/1000\n",
      "4/4 [==============================] - 2s 448ms/step - loss: 4.2836e-04 - val_loss: 6.1013e-04\n",
      "Epoch 177/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 4.1970e-04 - val_loss: 6.6934e-04\n",
      "Epoch 178/1000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 4.6049e-04 - val_loss: 7.9895e-04\n",
      "Epoch 179/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 4.3963e-04 - val_loss: 7.3032e-04\n",
      "Epoch 180/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 4.0253e-04 - val_loss: 5.3964e-04\n",
      "Epoch 181/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 4.7814e-04 - val_loss: 6.9622e-04\n",
      "Epoch 182/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 4.0830e-04 - val_loss: 6.0174e-04\n",
      "Epoch 183/1000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 3.9618e-04 - val_loss: 6.9532e-04\n",
      "Epoch 184/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 4.2358e-04 - val_loss: 5.9835e-04\n",
      "Epoch 185/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.7256e-04 - val_loss: 7.4361e-04\n",
      "Epoch 186/1000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 3.9701e-04 - val_loss: 5.3418e-04\n",
      "Epoch 187/1000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 5.8972e-04 - val_loss: 6.7862e-04\n",
      "Epoch 188/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 4.6761e-04 - val_loss: 5.3615e-04\n",
      "Epoch 189/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 5.9101e-04 - val_loss: 8.5367e-04\n",
      "Epoch 190/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 4.4683e-04 - val_loss: 8.6730e-04\n",
      "Epoch 191/1000\n",
      "4/4 [==============================] - 2s 473ms/step - loss: 4.7586e-04 - val_loss: 4.8658e-04\n",
      "Epoch 192/1000\n",
      "4/4 [==============================] - 2s 439ms/step - loss: 4.2241e-04 - val_loss: 5.6043e-04\n",
      "Epoch 193/1000\n",
      "4/4 [==============================] - 2s 437ms/step - loss: 5.3370e-04 - val_loss: 6.2949e-04\n",
      "Epoch 194/1000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 4.0731e-04 - val_loss: 8.1407e-04\n",
      "Epoch 195/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 4.5054e-04 - val_loss: 4.4881e-04\n",
      "Epoch 196/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 4.0276e-04 - val_loss: 5.8011e-04\n",
      "Epoch 197/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 3.7948e-04 - val_loss: 6.1444e-04\n",
      "Epoch 198/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 5.1240e-04 - val_loss: 4.5605e-04\n",
      "Epoch 199/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.6872e-04 - val_loss: 4.3452e-04\n",
      "Epoch 200/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 4.6121e-04 - val_loss: 6.7364e-04\n",
      "Epoch 201/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 4.2666e-04 - val_loss: 5.6257e-04\n",
      "Epoch 202/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 4.5978e-04 - val_loss: 4.7102e-04\n",
      "Epoch 203/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.4190e-04 - val_loss: 4.4827e-04\n",
      "Epoch 204/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.7481e-04 - val_loss: 6.0985e-04\n",
      "Epoch 205/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 4.1597e-04 - val_loss: 5.6266e-04\n",
      "Epoch 206/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.5620e-04 - val_loss: 4.1717e-04\n",
      "Epoch 207/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.7556e-04 - val_loss: 6.4357e-04\n",
      "Epoch 208/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.6613e-04 - val_loss: 5.0248e-04\n",
      "Epoch 209/1000\n",
      "4/4 [==============================] - 1s 391ms/step - loss: 3.6926e-04 - val_loss: 4.7193e-04\n",
      "Epoch 210/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 5.0026e-04 - val_loss: 4.4035e-04\n",
      "Epoch 211/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.8321e-04 - val_loss: 4.9872e-04\n",
      "Epoch 212/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.0289e-04 - val_loss: 4.9712e-04\n",
      "Epoch 213/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.6822e-04 - val_loss: 5.2476e-04\n",
      "Epoch 214/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.1544e-04 - val_loss: 4.8716e-04\n",
      "Epoch 215/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 5.3751e-04 - val_loss: 5.0805e-04\n",
      "Epoch 216/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.9337e-04 - val_loss: 4.7392e-04\n",
      "Epoch 217/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 4.8056e-04 - val_loss: 4.5048e-04\n",
      "Epoch 218/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.8262e-04 - val_loss: 4.3079e-04\n",
      "Epoch 219/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 4.0216e-04 - val_loss: 4.7533e-04\n",
      "Epoch 220/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.2668e-04 - val_loss: 5.4603e-04\n",
      "Epoch 221/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.5282e-04 - val_loss: 6.0823e-04\n",
      "Epoch 222/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.6844e-04 - val_loss: 4.1097e-04\n",
      "Epoch 223/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 5.2727e-04 - val_loss: 4.5373e-04\n",
      "Epoch 224/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.8697e-04 - val_loss: 5.7501e-04\n",
      "Epoch 225/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 5.0201e-04 - val_loss: 4.6360e-04\n",
      "Epoch 226/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 5.2592e-04 - val_loss: 4.8538e-04\n",
      "Epoch 227/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.7229e-04 - val_loss: 4.5780e-04\n",
      "Epoch 228/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.8102e-04 - val_loss: 4.3957e-04\n",
      "Epoch 229/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.6696e-04 - val_loss: 3.8914e-04\n",
      "Epoch 230/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.1205e-04 - val_loss: 4.1661e-04\n",
      "Epoch 231/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.8754e-04 - val_loss: 5.8856e-04\n",
      "Epoch 232/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.6938e-04 - val_loss: 3.6680e-04\n",
      "Epoch 233/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.4884e-04 - val_loss: 4.8949e-04\n",
      "Epoch 234/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.9542e-04 - val_loss: 4.5625e-04\n",
      "Epoch 235/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.3458e-04 - val_loss: 3.9941e-04\n",
      "Epoch 236/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.8013e-04 - val_loss: 4.2905e-04\n",
      "Epoch 237/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.9193e-04 - val_loss: 5.1043e-04\n",
      "Epoch 238/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.5326e-04 - val_loss: 6.3998e-04\n",
      "Epoch 239/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 4.1676e-04 - val_loss: 4.5725e-04\n",
      "Epoch 240/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.2181e-04 - val_loss: 4.3160e-04\n",
      "Epoch 241/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.7153e-04 - val_loss: 5.0993e-04\n",
      "Epoch 242/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 4.9467e-04 - val_loss: 4.9755e-04\n",
      "Epoch 243/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.6953e-04 - val_loss: 4.4408e-04\n",
      "Epoch 244/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 3.3888e-04 - val_loss: 4.1804e-04\n",
      "Epoch 245/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.9417e-04 - val_loss: 3.8571e-04\n",
      "Epoch 246/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.3820e-04 - val_loss: 4.0105e-04\n",
      "Epoch 247/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.4883e-04 - val_loss: 3.5594e-04\n",
      "Epoch 248/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.8712e-04 - val_loss: 4.6931e-04\n",
      "Epoch 249/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 4.0296e-04 - val_loss: 3.8025e-04\n",
      "Epoch 250/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.8635e-04 - val_loss: 4.4156e-04\n",
      "Epoch 251/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.3180e-04 - val_loss: 5.3794e-04\n",
      "Epoch 252/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.7452e-04 - val_loss: 3.8549e-04\n",
      "Epoch 253/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.6332e-04 - val_loss: 5.3723e-04\n",
      "Epoch 254/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 4.4930e-04 - val_loss: 4.1941e-04\n",
      "Epoch 255/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.7946e-04 - val_loss: 3.9960e-04\n",
      "Epoch 256/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.2884e-04 - val_loss: 4.1108e-04\n",
      "Epoch 257/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.3818e-04 - val_loss: 4.1299e-04\n",
      "Epoch 258/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.6082e-04 - val_loss: 3.4763e-04\n",
      "Epoch 259/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.2157e-04 - val_loss: 3.8652e-04\n",
      "Epoch 260/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 4.1004e-04 - val_loss: 4.1126e-04\n",
      "Epoch 261/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 4.0767e-04 - val_loss: 3.5280e-04\n",
      "Epoch 262/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.0035e-04 - val_loss: 3.9004e-04\n",
      "Epoch 263/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 3.1692e-04 - val_loss: 3.9968e-04\n",
      "Epoch 264/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 4.2924e-04 - val_loss: 3.5325e-04\n",
      "Epoch 265/1000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 3.2676e-04 - val_loss: 3.9211e-04\n",
      "Epoch 266/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 3.3978e-04 - val_loss: 3.6922e-04\n",
      "Epoch 267/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 4.0789e-04 - val_loss: 3.2465e-04\n",
      "Epoch 268/1000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 3.4852e-04 - val_loss: 4.3139e-04\n",
      "Epoch 269/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.9470e-04 - val_loss: 4.4108e-04\n",
      "Epoch 270/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.5671e-04 - val_loss: 3.6403e-04\n",
      "Epoch 271/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.7568e-04 - val_loss: 3.6177e-04\n",
      "Epoch 272/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.4965e-04 - val_loss: 5.7683e-04\n",
      "Epoch 273/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.4321e-04 - val_loss: 3.2032e-04\n",
      "Epoch 274/1000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 4.0786e-04 - val_loss: 3.4021e-04\n",
      "Epoch 275/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 3.6780e-04 - val_loss: 3.9043e-04\n",
      "Epoch 276/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.5624e-04 - val_loss: 3.4516e-04\n",
      "Epoch 277/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.1897e-04 - val_loss: 3.9765e-04\n",
      "Epoch 278/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 3.4518e-04 - val_loss: 3.2430e-04\n",
      "Epoch 279/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.2213e-04 - val_loss: 4.1072e-04\n",
      "Epoch 280/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 3.9557e-04 - val_loss: 3.3553e-04\n",
      "Epoch 281/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.1438e-04 - val_loss: 3.4761e-04\n",
      "Epoch 282/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.1620e-04 - val_loss: 3.8501e-04\n",
      "Epoch 283/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 4.2474e-04 - val_loss: 3.3981e-04\n",
      "Epoch 284/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.7786e-04 - val_loss: 3.7684e-04\n",
      "Epoch 285/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.7091e-04 - val_loss: 3.3698e-04\n",
      "Epoch 286/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.4933e-04 - val_loss: 3.3750e-04\n",
      "Epoch 287/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.4097e-04 - val_loss: 3.7107e-04\n",
      "Epoch 288/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 4.7152e-04 - val_loss: 3.1626e-04\n",
      "Epoch 289/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.6711e-04 - val_loss: 3.5664e-04\n",
      "Epoch 290/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.1228e-04 - val_loss: 3.2924e-04\n",
      "Epoch 291/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 4.2791e-04 - val_loss: 3.9989e-04\n",
      "Epoch 292/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 3.5713e-04 - val_loss: 3.7180e-04\n",
      "Epoch 293/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.0483e-04 - val_loss: 3.4166e-04\n",
      "Epoch 294/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.9824e-04 - val_loss: 4.4400e-04\n",
      "Epoch 295/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.6137e-04 - val_loss: 3.9263e-04\n",
      "Epoch 296/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.1462e-04 - val_loss: 3.2268e-04\n",
      "Epoch 297/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.3210e-04 - val_loss: 4.0987e-04\n",
      "Epoch 298/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.2411e-04 - val_loss: 3.3240e-04\n",
      "Epoch 299/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.9476e-04 - val_loss: 3.5215e-04\n",
      "Epoch 300/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.9763e-04 - val_loss: 3.3088e-04\n",
      "Epoch 301/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.1948e-04 - val_loss: 4.0240e-04\n",
      "Epoch 302/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.7638e-04 - val_loss: 3.9740e-04\n",
      "Epoch 303/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.1563e-04 - val_loss: 4.2594e-04\n",
      "Epoch 304/1000\n",
      "4/4 [==============================] - 2s 435ms/step - loss: 3.4833e-04 - val_loss: 3.7636e-04\n",
      "Epoch 305/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.1449e-04 - val_loss: 3.3938e-04\n",
      "Epoch 306/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.6516e-04 - val_loss: 3.3532e-04\n",
      "Epoch 307/1000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 3.7747e-04 - val_loss: 3.6857e-04\n",
      "Epoch 308/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.1504e-04 - val_loss: 3.7522e-04\n",
      "Epoch 309/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 3.1125e-04 - val_loss: 3.1076e-04\n",
      "Epoch 310/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.6155e-04 - val_loss: 3.1742e-04\n",
      "Epoch 311/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.2603e-04 - val_loss: 3.4135e-04\n",
      "Epoch 312/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.4784e-04 - val_loss: 3.1879e-04\n",
      "Epoch 313/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.7809e-04 - val_loss: 3.2912e-04\n",
      "Epoch 314/1000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 2.9145e-04 - val_loss: 3.2765e-04\n",
      "Epoch 315/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 3.5712e-04 - val_loss: 2.9596e-04\n",
      "Epoch 316/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.3591e-04 - val_loss: 3.2855e-04\n",
      "Epoch 317/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 2.9516e-04 - val_loss: 3.3196e-04\n",
      "Epoch 318/1000\n",
      "4/4 [==============================] - 1s 381ms/step - loss: 3.1113e-04 - val_loss: 3.2211e-04\n",
      "Epoch 319/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.3476e-04 - val_loss: 3.1787e-04\n",
      "Epoch 320/1000\n",
      "4/4 [==============================] - 2s 437ms/step - loss: 3.0840e-04 - val_loss: 3.7883e-04\n",
      "Epoch 321/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.7935e-04 - val_loss: 3.1988e-04\n",
      "Epoch 322/1000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 3.9872e-04 - val_loss: 2.9261e-04\n",
      "Epoch 323/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.3469e-04 - val_loss: 3.2501e-04\n",
      "Epoch 324/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.1251e-04 - val_loss: 3.0604e-04\n",
      "Epoch 325/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 3.1864e-04 - val_loss: 3.2057e-04\n",
      "Epoch 326/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 3.5247e-04 - val_loss: 3.0789e-04\n",
      "Epoch 327/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.9998e-04 - val_loss: 3.2760e-04\n",
      "Epoch 328/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.3139e-04 - val_loss: 3.5533e-04\n",
      "Epoch 329/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.2213e-04 - val_loss: 3.1206e-04\n",
      "Epoch 330/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.1268e-04 - val_loss: 3.1422e-04\n",
      "Epoch 331/1000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 3.8656e-04 - val_loss: 3.5871e-04\n",
      "Epoch 332/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 2.8885e-04 - val_loss: 3.6459e-04\n",
      "Epoch 333/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 2.9045e-04 - val_loss: 3.2837e-04\n",
      "Epoch 334/1000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 3.2239e-04 - val_loss: 3.1404e-04\n",
      "Epoch 335/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 3.4896e-04 - val_loss: 3.3026e-04\n",
      "Epoch 336/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 2.9576e-04 - val_loss: 3.5999e-04\n",
      "Epoch 337/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 3.4501e-04 - val_loss: 3.5098e-04\n",
      "Epoch 338/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.3851e-04 - val_loss: 3.6194e-04\n",
      "Epoch 339/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.4671e-04 - val_loss: 2.9517e-04\n",
      "Epoch 340/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 3.1746e-04 - val_loss: 2.8992e-04\n",
      "Epoch 341/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.7033e-04 - val_loss: 3.7248e-04\n",
      "Epoch 342/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 2.9792e-04 - val_loss: 3.2381e-04\n",
      "Epoch 343/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.7085e-04 - val_loss: 3.1954e-04\n",
      "Epoch 344/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9443e-04 - val_loss: 2.9681e-04\n",
      "Epoch 345/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 3.2682e-04 - val_loss: 3.0187e-04\n",
      "Epoch 346/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.5344e-04 - val_loss: 2.9190e-04\n",
      "Epoch 347/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.1692e-04 - val_loss: 3.2033e-04\n",
      "Epoch 348/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.9208e-04 - val_loss: 3.4369e-04\n",
      "Epoch 349/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.7592e-04 - val_loss: 2.9140e-04\n",
      "Epoch 350/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.5444e-04 - val_loss: 3.3924e-04\n",
      "Epoch 351/1000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 3.5716e-04 - val_loss: 3.1095e-04\n",
      "Epoch 352/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.6488e-04 - val_loss: 2.9243e-04\n",
      "Epoch 353/1000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 3.3175e-04 - val_loss: 3.3670e-04\n",
      "Epoch 354/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.3056e-04 - val_loss: 3.1945e-04\n",
      "Epoch 355/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.1494e-04 - val_loss: 2.8197e-04\n",
      "Epoch 356/1000\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 3.6051e-04 - val_loss: 3.0370e-04\n",
      "Epoch 357/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8339e-04 - val_loss: 3.2226e-04\n",
      "Epoch 358/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.2475e-04 - val_loss: 2.8227e-04\n",
      "Epoch 359/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.3084e-04 - val_loss: 2.9657e-04\n",
      "Epoch 360/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.0218e-04 - val_loss: 3.0227e-04\n",
      "Epoch 361/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.2611e-04 - val_loss: 3.2406e-04\n",
      "Epoch 362/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 3.0610e-04 - val_loss: 3.1308e-04\n",
      "Epoch 363/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.0480e-04 - val_loss: 3.1104e-04\n",
      "Epoch 364/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.7274e-04 - val_loss: 2.8236e-04\n",
      "Epoch 365/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 3.0401e-04 - val_loss: 3.0844e-04\n",
      "Epoch 366/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 3.0900e-04 - val_loss: 3.2616e-04\n",
      "Epoch 367/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.0066e-04 - val_loss: 2.6095e-04\n",
      "Epoch 368/1000\n",
      "4/4 [==============================] - 1s 382ms/step - loss: 2.6895e-04 - val_loss: 3.3735e-04\n",
      "Epoch 369/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 3.5099e-04 - val_loss: 3.0082e-04\n",
      "Epoch 370/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6072e-04 - val_loss: 2.5935e-04\n",
      "Epoch 371/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.8039e-04 - val_loss: 3.1416e-04\n",
      "Epoch 372/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.2956e-04 - val_loss: 2.7472e-04\n",
      "Epoch 373/1000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 3.1467e-04 - val_loss: 2.9655e-04\n",
      "Epoch 374/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 3.3077e-04 - val_loss: 2.8664e-04\n",
      "Epoch 375/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7107e-04 - val_loss: 2.9013e-04\n",
      "Epoch 376/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.8530e-04 - val_loss: 2.9120e-04\n",
      "Epoch 377/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 2.7904e-04 - val_loss: 2.8911e-04\n",
      "Epoch 378/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.1282e-04 - val_loss: 2.9584e-04\n",
      "Epoch 379/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 3.7256e-04 - val_loss: 3.0988e-04\n",
      "Epoch 380/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6894e-04 - val_loss: 2.6667e-04\n",
      "Epoch 381/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.1512e-04 - val_loss: 2.8695e-04\n",
      "Epoch 382/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.0126e-04 - val_loss: 2.9113e-04\n",
      "Epoch 383/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 2.8516e-04 - val_loss: 2.8194e-04\n",
      "Epoch 384/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.6967e-04 - val_loss: 2.8960e-04\n",
      "Epoch 385/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.8311e-04 - val_loss: 2.5986e-04\n",
      "Epoch 386/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.7107e-04 - val_loss: 2.8461e-04\n",
      "Epoch 387/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.8956e-04 - val_loss: 2.9260e-04\n",
      "Epoch 388/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.8279e-04 - val_loss: 2.6110e-04\n",
      "Epoch 389/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6594e-04 - val_loss: 2.8048e-04\n",
      "Epoch 390/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8786e-04 - val_loss: 3.0721e-04\n",
      "Epoch 391/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.9689e-04 - val_loss: 2.6708e-04\n",
      "Epoch 392/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.1696e-04 - val_loss: 2.7882e-04\n",
      "Epoch 393/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8345e-04 - val_loss: 2.6958e-04\n",
      "Epoch 394/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.5798e-04 - val_loss: 2.7637e-04\n",
      "Epoch 395/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.8576e-04 - val_loss: 2.5918e-04\n",
      "Epoch 396/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.3666e-04 - val_loss: 2.9182e-04\n",
      "Epoch 397/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.5931e-04 - val_loss: 2.8604e-04\n",
      "Epoch 398/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.6900e-04 - val_loss: 2.8997e-04\n",
      "Epoch 399/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 3.1384e-04 - val_loss: 3.0502e-04\n",
      "Epoch 400/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6513e-04 - val_loss: 2.7058e-04\n",
      "Epoch 401/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 2.7200e-04 - val_loss: 2.5636e-04\n",
      "Epoch 402/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.5229e-04 - val_loss: 3.0287e-04\n",
      "Epoch 403/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.1397e-04 - val_loss: 2.6912e-04\n",
      "Epoch 404/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.1701e-04 - val_loss: 2.5497e-04\n",
      "Epoch 405/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.5708e-04 - val_loss: 3.0813e-04\n",
      "Epoch 406/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 3.0316e-04 - val_loss: 2.7154e-04\n",
      "Epoch 407/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 2.6032e-04 - val_loss: 2.5930e-04\n",
      "Epoch 408/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.8324e-04 - val_loss: 2.6775e-04\n",
      "Epoch 409/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.0400e-04 - val_loss: 2.7192e-04\n",
      "Epoch 410/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.1918e-04 - val_loss: 2.9795e-04\n",
      "Epoch 411/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7234e-04 - val_loss: 2.7937e-04\n",
      "Epoch 412/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7949e-04 - val_loss: 3.0375e-04\n",
      "Epoch 413/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.7751e-04 - val_loss: 2.6774e-04\n",
      "Epoch 414/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.1232e-04 - val_loss: 2.8724e-04\n",
      "Epoch 415/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.6915e-04 - val_loss: 2.6804e-04\n",
      "Epoch 416/1000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 2.8604e-04 - val_loss: 2.8209e-04\n",
      "Epoch 417/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.4067e-04 - val_loss: 2.5799e-04\n",
      "Epoch 418/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 2.8512e-04 - val_loss: 2.7562e-04\n",
      "Epoch 419/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.8127e-04 - val_loss: 2.7425e-04\n",
      "Epoch 420/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.3986e-04 - val_loss: 2.7343e-04\n",
      "Epoch 421/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 2.6455e-04 - val_loss: 2.7340e-04\n",
      "Epoch 422/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.7792e-04 - val_loss: 3.2407e-04\n",
      "Epoch 423/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.9440e-04 - val_loss: 2.6105e-04\n",
      "Epoch 424/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.8255e-04 - val_loss: 2.6247e-04\n",
      "Epoch 425/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 2.8885e-04 - val_loss: 2.8657e-04\n",
      "Epoch 426/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7628e-04 - val_loss: 2.8365e-04\n",
      "Epoch 427/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.2768e-04 - val_loss: 2.7446e-04\n",
      "Epoch 428/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.0562e-04 - val_loss: 2.6946e-04\n",
      "Epoch 429/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8827e-04 - val_loss: 2.7693e-04\n",
      "Epoch 430/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.8735e-04 - val_loss: 2.5818e-04\n",
      "Epoch 431/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5853e-04 - val_loss: 2.6988e-04\n",
      "Epoch 432/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.8401e-04 - val_loss: 3.0198e-04\n",
      "Epoch 433/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8753e-04 - val_loss: 2.5179e-04\n",
      "Epoch 434/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 3.9968e-04 - val_loss: 2.6414e-04\n",
      "Epoch 435/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 3.0185e-04 - val_loss: 2.8556e-04\n",
      "Epoch 436/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5681e-04 - val_loss: 2.5733e-04\n",
      "Epoch 437/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.0655e-04 - val_loss: 2.5539e-04\n",
      "Epoch 438/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.8528e-04 - val_loss: 2.7096e-04\n",
      "Epoch 439/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.3914e-04 - val_loss: 2.7398e-04\n",
      "Epoch 440/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.1816e-04 - val_loss: 2.4563e-04\n",
      "Epoch 441/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 4.0043e-04 - val_loss: 2.5463e-04\n",
      "Epoch 442/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.6418e-04 - val_loss: 2.5581e-04\n",
      "Epoch 443/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.5043e-04 - val_loss: 2.7386e-04\n",
      "Epoch 444/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.6338e-04 - val_loss: 2.5451e-04\n",
      "Epoch 445/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8327e-04 - val_loss: 2.5151e-04\n",
      "Epoch 446/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.9192e-04 - val_loss: 2.5542e-04\n",
      "Epoch 447/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.9175e-04 - val_loss: 2.6618e-04\n",
      "Epoch 448/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7898e-04 - val_loss: 2.5615e-04\n",
      "Epoch 449/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7123e-04 - val_loss: 2.5456e-04\n",
      "Epoch 450/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.6849e-04 - val_loss: 2.5004e-04\n",
      "Epoch 451/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.7304e-04 - val_loss: 2.5115e-04\n",
      "Epoch 452/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.9509e-04 - val_loss: 2.6721e-04\n",
      "Epoch 453/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6816e-04 - val_loss: 2.4975e-04\n",
      "Epoch 454/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 4.2868e-04 - val_loss: 2.4642e-04\n",
      "Epoch 455/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4876e-04 - val_loss: 2.4792e-04\n",
      "Epoch 456/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.4682e-04 - val_loss: 2.4823e-04\n",
      "Epoch 457/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.0519e-04 - val_loss: 2.6650e-04\n",
      "Epoch 458/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7987e-04 - val_loss: 2.6375e-04\n",
      "Epoch 459/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.9031e-04 - val_loss: 2.4847e-04\n",
      "Epoch 460/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.6575e-04 - val_loss: 2.6626e-04\n",
      "Epoch 461/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.9943e-04 - val_loss: 2.5149e-04\n",
      "Epoch 462/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 3.1259e-04 - val_loss: 2.4757e-04\n",
      "Epoch 463/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.8391e-04 - val_loss: 2.4935e-04\n",
      "Epoch 464/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.8857e-04 - val_loss: 2.5002e-04\n",
      "Epoch 465/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.8845e-04 - val_loss: 2.7496e-04\n",
      "Epoch 466/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.7196e-04 - val_loss: 2.4284e-04\n",
      "Epoch 467/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8616e-04 - val_loss: 2.7314e-04\n",
      "Epoch 468/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.1359e-04 - val_loss: 2.4759e-04\n",
      "Epoch 469/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9175e-04 - val_loss: 2.6600e-04\n",
      "Epoch 470/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.7862e-04 - val_loss: 2.4591e-04\n",
      "Epoch 471/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5939e-04 - val_loss: 2.4012e-04\n",
      "Epoch 472/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7470e-04 - val_loss: 2.4976e-04\n",
      "Epoch 473/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.0893e-04 - val_loss: 2.4911e-04\n",
      "Epoch 474/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.9392e-04 - val_loss: 2.5146e-04\n",
      "Epoch 475/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8567e-04 - val_loss: 2.3952e-04\n",
      "Epoch 476/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.5858e-04 - val_loss: 2.4971e-04\n",
      "Epoch 477/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.8126e-04 - val_loss: 2.4648e-04\n",
      "Epoch 478/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.4888e-04 - val_loss: 2.4375e-04\n",
      "Epoch 479/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.0196e-04 - val_loss: 2.4816e-04\n",
      "Epoch 480/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.0446e-04 - val_loss: 2.4460e-04\n",
      "Epoch 481/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.5997e-04 - val_loss: 2.3951e-04\n",
      "Epoch 482/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.5505e-04 - val_loss: 2.4863e-04\n",
      "Epoch 483/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3752e-04 - val_loss: 2.4589e-04\n",
      "Epoch 484/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.0201e-04 - val_loss: 2.4885e-04\n",
      "Epoch 485/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6744e-04 - val_loss: 2.5054e-04\n",
      "Epoch 486/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.6487e-04 - val_loss: 2.3896e-04\n",
      "Epoch 487/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.3242e-04 - val_loss: 2.3636e-04\n",
      "Epoch 488/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.7316e-04 - val_loss: 2.5181e-04\n",
      "Epoch 489/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.7371e-04 - val_loss: 2.4702e-04\n",
      "Epoch 490/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.0317e-04 - val_loss: 2.3873e-04\n",
      "Epoch 491/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.8355e-04 - val_loss: 2.4124e-04\n",
      "Epoch 492/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5910e-04 - val_loss: 2.4410e-04\n",
      "Epoch 493/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.8012e-04 - val_loss: 2.5644e-04\n",
      "Epoch 494/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4702e-04 - val_loss: 2.5385e-04\n",
      "Epoch 495/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3666e-04 - val_loss: 2.4886e-04\n",
      "Epoch 496/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.1788e-04 - val_loss: 2.4118e-04\n",
      "Epoch 497/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9890e-04 - val_loss: 2.4540e-04\n",
      "Epoch 498/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6430e-04 - val_loss: 2.5810e-04\n",
      "Epoch 499/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.1610e-04 - val_loss: 2.4322e-04\n",
      "Epoch 500/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.9436e-04 - val_loss: 2.3609e-04\n",
      "Epoch 501/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.3286e-04 - val_loss: 2.3955e-04\n",
      "Epoch 502/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4498e-04 - val_loss: 2.5282e-04\n",
      "Epoch 503/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 2.7073e-04 - val_loss: 2.4597e-04\n",
      "Epoch 504/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4202e-04 - val_loss: 2.3940e-04\n",
      "Epoch 505/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4852e-04 - val_loss: 2.3971e-04\n",
      "Epoch 506/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5775e-04 - val_loss: 2.5255e-04\n",
      "Epoch 507/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7388e-04 - val_loss: 2.5463e-04\n",
      "Epoch 508/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.7875e-04 - val_loss: 2.4911e-04\n",
      "Epoch 509/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.8143e-04 - val_loss: 2.4359e-04\n",
      "Epoch 510/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7480e-04 - val_loss: 2.3795e-04\n",
      "Epoch 511/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.8972e-04 - val_loss: 2.3720e-04\n",
      "Epoch 512/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5677e-04 - val_loss: 2.3587e-04\n",
      "Epoch 513/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.7203e-04 - val_loss: 2.4213e-04\n",
      "Epoch 514/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.9772e-04 - val_loss: 2.4199e-04\n",
      "Epoch 515/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.9572e-04 - val_loss: 2.4194e-04\n",
      "Epoch 516/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.9318e-04 - val_loss: 2.3426e-04\n",
      "Epoch 517/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.4516e-04 - val_loss: 2.3292e-04\n",
      "Epoch 518/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7125e-04 - val_loss: 2.3368e-04\n",
      "Epoch 519/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.5919e-04 - val_loss: 2.3467e-04\n",
      "Epoch 520/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 4.0021e-04 - val_loss: 2.3450e-04\n",
      "Epoch 521/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.0589e-04 - val_loss: 2.4447e-04\n",
      "Epoch 522/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4946e-04 - val_loss: 2.4601e-04\n",
      "Epoch 523/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7036e-04 - val_loss: 2.4348e-04\n",
      "Epoch 524/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5773e-04 - val_loss: 2.3553e-04\n",
      "Epoch 525/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6317e-04 - val_loss: 2.3940e-04\n",
      "Epoch 526/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.6056e-04 - val_loss: 2.4119e-04\n",
      "Epoch 527/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8399e-04 - val_loss: 2.4643e-04\n",
      "Epoch 528/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.4246e-04 - val_loss: 2.4631e-04\n",
      "Epoch 529/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.1069e-04 - val_loss: 2.3210e-04\n",
      "Epoch 530/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5740e-04 - val_loss: 2.3477e-04\n",
      "Epoch 531/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.6262e-04 - val_loss: 2.3484e-04\n",
      "Epoch 532/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7142e-04 - val_loss: 2.3647e-04\n",
      "Epoch 533/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5415e-04 - val_loss: 2.3824e-04\n",
      "Epoch 534/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5304e-04 - val_loss: 2.2807e-04\n",
      "Epoch 535/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6560e-04 - val_loss: 2.3078e-04\n",
      "Epoch 536/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5117e-04 - val_loss: 2.3526e-04\n",
      "Epoch 537/1000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 4.2623e-04 - val_loss: 2.4102e-04\n",
      "Epoch 538/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9151e-04 - val_loss: 2.3042e-04\n",
      "Epoch 539/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.4019e-04 - val_loss: 2.3449e-04\n",
      "Epoch 540/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 3.2251e-04 - val_loss: 2.3731e-04\n",
      "Epoch 541/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.5741e-04 - val_loss: 2.3380e-04\n",
      "Epoch 542/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.9838e-04 - val_loss: 2.3213e-04\n",
      "Epoch 543/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8094e-04 - val_loss: 2.3346e-04\n",
      "Epoch 544/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.5562e-04 - val_loss: 2.2945e-04\n",
      "Epoch 545/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 2.6943e-04 - val_loss: 2.4003e-04\n",
      "Epoch 546/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7931e-04 - val_loss: 2.3469e-04\n",
      "Epoch 547/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.6158e-04 - val_loss: 2.3615e-04\n",
      "Epoch 548/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.4743e-04 - val_loss: 2.2640e-04\n",
      "Epoch 549/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.3048e-04 - val_loss: 2.2660e-04\n",
      "Epoch 550/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8831e-04 - val_loss: 2.2803e-04\n",
      "Epoch 551/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.5141e-04 - val_loss: 2.2850e-04\n",
      "Epoch 552/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.8532e-04 - val_loss: 2.3032e-04\n",
      "Epoch 553/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8989e-04 - val_loss: 2.3115e-04\n",
      "Epoch 554/1000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 2.4814e-04 - val_loss: 2.3115e-04\n",
      "Epoch 555/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 3.0627e-04 - val_loss: 2.2965e-04\n",
      "Epoch 556/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.9482e-04 - val_loss: 2.2886e-04\n",
      "Epoch 557/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4313e-04 - val_loss: 2.3839e-04\n",
      "Epoch 558/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5531e-04 - val_loss: 2.2785e-04\n",
      "Epoch 559/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.7516e-04 - val_loss: 2.3231e-04\n",
      "Epoch 560/1000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 2.4455e-04 - val_loss: 2.2851e-04\n",
      "Epoch 561/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.4332e-04 - val_loss: 2.2637e-04\n",
      "Epoch 562/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.9204e-04 - val_loss: 2.2756e-04\n",
      "Epoch 563/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5782e-04 - val_loss: 2.3655e-04\n",
      "Epoch 564/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4963e-04 - val_loss: 2.3822e-04\n",
      "Epoch 565/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.5493e-04 - val_loss: 2.3676e-04\n",
      "Epoch 566/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.7491e-04 - val_loss: 2.2780e-04\n",
      "Epoch 567/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.3748e-04 - val_loss: 2.2573e-04\n",
      "Epoch 568/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.3622e-04 - val_loss: 2.4286e-04\n",
      "Epoch 569/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4941e-04 - val_loss: 2.2765e-04\n",
      "Epoch 570/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.4208e-04 - val_loss: 2.2865e-04\n",
      "Epoch 571/1000\n",
      "4/4 [==============================] - 2s 406ms/step - loss: 3.3034e-04 - val_loss: 2.2411e-04\n",
      "Epoch 572/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.7428e-04 - val_loss: 2.2701e-04\n",
      "Epoch 573/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5169e-04 - val_loss: 2.2959e-04\n",
      "Epoch 574/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.4611e-04 - val_loss: 2.2634e-04\n",
      "Epoch 575/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3884e-04 - val_loss: 2.2351e-04\n",
      "Epoch 576/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.1763e-04 - val_loss: 2.2675e-04\n",
      "Epoch 577/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.0150e-04 - val_loss: 2.2557e-04\n",
      "Epoch 578/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6499e-04 - val_loss: 2.2560e-04\n",
      "Epoch 579/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4964e-04 - val_loss: 2.2493e-04\n",
      "Epoch 580/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 2.8502e-04 - val_loss: 2.2386e-04\n",
      "Epoch 581/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5559e-04 - val_loss: 2.2703e-04\n",
      "Epoch 582/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2836e-04 - val_loss: 2.3081e-04\n",
      "Epoch 583/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6167e-04 - val_loss: 2.2687e-04\n",
      "Epoch 584/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5781e-04 - val_loss: 2.2507e-04\n",
      "Epoch 585/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.8518e-04 - val_loss: 2.3005e-04\n",
      "Epoch 586/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7652e-04 - val_loss: 2.2446e-04\n",
      "Epoch 587/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.8640e-04 - val_loss: 2.3971e-04\n",
      "Epoch 588/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7533e-04 - val_loss: 2.3397e-04\n",
      "Epoch 589/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4593e-04 - val_loss: 2.2703e-04\n",
      "Epoch 590/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.3026e-04 - val_loss: 2.3085e-04\n",
      "Epoch 591/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6668e-04 - val_loss: 2.2611e-04\n",
      "Epoch 592/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9055e-04 - val_loss: 2.2447e-04\n",
      "Epoch 593/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.5302e-04 - val_loss: 2.3392e-04\n",
      "Epoch 594/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 3.3681e-04 - val_loss: 2.3443e-04\n",
      "Epoch 595/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6864e-04 - val_loss: 2.3201e-04\n",
      "Epoch 596/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9953e-04 - val_loss: 2.3111e-04\n",
      "Epoch 597/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.8851e-04 - val_loss: 2.2726e-04\n",
      "Epoch 598/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6979e-04 - val_loss: 2.2629e-04\n",
      "Epoch 599/1000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 2.9868e-04 - val_loss: 2.2709e-04\n",
      "Epoch 600/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 2.4516e-04 - val_loss: 2.2662e-04\n",
      "Epoch 601/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.3329e-04 - val_loss: 2.2384e-04\n",
      "Epoch 602/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.5505e-04 - val_loss: 2.2169e-04\n",
      "Epoch 603/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.4604e-04 - val_loss: 2.2247e-04\n",
      "Epoch 604/1000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 2.3891e-04 - val_loss: 2.2700e-04\n",
      "Epoch 605/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.6629e-04 - val_loss: 2.2598e-04\n",
      "Epoch 606/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.1054e-04 - val_loss: 2.2457e-04\n",
      "Epoch 607/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.5819e-04 - val_loss: 2.2225e-04\n",
      "Epoch 608/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 3.3958e-04 - val_loss: 2.3090e-04\n",
      "Epoch 609/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8806e-04 - val_loss: 2.2849e-04\n",
      "Epoch 610/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.8195e-04 - val_loss: 2.3493e-04\n",
      "Epoch 611/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.6466e-04 - val_loss: 2.2721e-04\n",
      "Epoch 612/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.0159e-04 - val_loss: 2.2358e-04\n",
      "Epoch 613/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.3625e-04 - val_loss: 2.2120e-04\n",
      "Epoch 614/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2567e-04 - val_loss: 2.3240e-04\n",
      "Epoch 615/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8419e-04 - val_loss: 2.2621e-04\n",
      "Epoch 616/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.7698e-04 - val_loss: 2.2145e-04\n",
      "Epoch 617/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.1460e-04 - val_loss: 2.2943e-04\n",
      "Epoch 618/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.7021e-04 - val_loss: 2.3031e-04\n",
      "Epoch 619/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8293e-04 - val_loss: 2.2619e-04\n",
      "Epoch 620/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.7891e-04 - val_loss: 2.3033e-04\n",
      "Epoch 621/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.6252e-04 - val_loss: 2.2666e-04\n",
      "Epoch 622/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 2.6424e-04 - val_loss: 2.2553e-04\n",
      "Epoch 623/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.4220e-04 - val_loss: 2.2709e-04\n",
      "Epoch 624/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7649e-04 - val_loss: 2.2497e-04\n",
      "Epoch 625/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 2.4628e-04 - val_loss: 2.2398e-04\n",
      "Epoch 626/1000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 4.1140e-04 - val_loss: 2.2604e-04\n",
      "Epoch 627/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.5656e-04 - val_loss: 2.2249e-04\n",
      "Epoch 628/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7110e-04 - val_loss: 2.2039e-04\n",
      "Epoch 629/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4994e-04 - val_loss: 2.2567e-04\n",
      "Epoch 630/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.8937e-04 - val_loss: 2.2709e-04\n",
      "Epoch 631/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.2332e-04 - val_loss: 2.2290e-04\n",
      "Epoch 632/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 3.2637e-04 - val_loss: 2.2062e-04\n",
      "Epoch 633/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.5056e-04 - val_loss: 2.2114e-04\n",
      "Epoch 634/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.8337e-04 - val_loss: 2.3007e-04\n",
      "Epoch 635/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3282e-04 - val_loss: 2.2607e-04\n",
      "Epoch 636/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.3539e-04 - val_loss: 2.2061e-04\n",
      "Epoch 637/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.5652e-04 - val_loss: 2.2412e-04\n",
      "Epoch 638/1000\n",
      "4/4 [==============================] - 1s 390ms/step - loss: 2.8063e-04 - val_loss: 2.2342e-04\n",
      "Epoch 639/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.3067e-04 - val_loss: 2.2063e-04\n",
      "Epoch 640/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.2802e-04 - val_loss: 2.3322e-04\n",
      "Epoch 641/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.6403e-04 - val_loss: 2.2248e-04\n",
      "Epoch 642/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.4106e-04 - val_loss: 2.2195e-04\n",
      "Epoch 643/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5401e-04 - val_loss: 2.2398e-04\n",
      "Epoch 644/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.2855e-04 - val_loss: 2.2161e-04\n",
      "Epoch 645/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6518e-04 - val_loss: 2.1782e-04\n",
      "Epoch 646/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.7372e-04 - val_loss: 2.1847e-04\n",
      "Epoch 647/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.3302e-04 - val_loss: 2.2126e-04\n",
      "Epoch 648/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6300e-04 - val_loss: 2.2007e-04\n",
      "Epoch 649/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.0901e-04 - val_loss: 2.1836e-04\n",
      "Epoch 650/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.3197e-04 - val_loss: 2.2093e-04\n",
      "Epoch 651/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.0838e-04 - val_loss: 2.2006e-04\n",
      "Epoch 652/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.9639e-04 - val_loss: 2.2004e-04\n",
      "Epoch 653/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.9126e-04 - val_loss: 2.2118e-04\n",
      "Epoch 654/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.0113e-04 - val_loss: 2.1973e-04\n",
      "Epoch 655/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7590e-04 - val_loss: 2.1900e-04\n",
      "Epoch 656/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6045e-04 - val_loss: 2.2019e-04\n",
      "Epoch 657/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.9458e-04 - val_loss: 2.1887e-04\n",
      "Epoch 658/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6136e-04 - val_loss: 2.1771e-04\n",
      "Epoch 659/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6663e-04 - val_loss: 2.1783e-04\n",
      "Epoch 660/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2419e-04 - val_loss: 2.2250e-04\n",
      "Epoch 661/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.8984e-04 - val_loss: 2.2358e-04\n",
      "Epoch 662/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3760e-04 - val_loss: 2.1869e-04\n",
      "Epoch 663/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.3705e-04 - val_loss: 2.1808e-04\n",
      "Epoch 664/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.6109e-04 - val_loss: 2.1801e-04\n",
      "Epoch 665/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.4184e-04 - val_loss: 2.2019e-04\n",
      "Epoch 666/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.4458e-04 - val_loss: 2.1844e-04\n",
      "Epoch 667/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.0982e-04 - val_loss: 2.1919e-04\n",
      "Epoch 668/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.2626e-04 - val_loss: 2.1846e-04\n",
      "Epoch 669/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 3.2883e-04 - val_loss: 2.2424e-04\n",
      "Epoch 670/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3233e-04 - val_loss: 2.2017e-04\n",
      "Epoch 671/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6232e-04 - val_loss: 2.1938e-04\n",
      "Epoch 672/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4220e-04 - val_loss: 2.2282e-04\n",
      "Epoch 673/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.7430e-04 - val_loss: 2.2034e-04\n",
      "Epoch 674/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 2.6125e-04 - val_loss: 2.1761e-04\n",
      "Epoch 675/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6270e-04 - val_loss: 2.1881e-04\n",
      "Epoch 676/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.0070e-04 - val_loss: 2.1862e-04\n",
      "Epoch 677/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.7680e-04 - val_loss: 2.1830e-04\n",
      "Epoch 678/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.7182e-04 - val_loss: 2.1987e-04\n",
      "Epoch 679/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.7649e-04 - val_loss: 2.1924e-04\n",
      "Epoch 680/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.2375e-04 - val_loss: 2.1608e-04\n",
      "Epoch 681/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.4669e-04 - val_loss: 2.1729e-04\n",
      "Epoch 682/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2806e-04 - val_loss: 2.1749e-04\n",
      "Epoch 683/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8518e-04 - val_loss: 2.1865e-04\n",
      "Epoch 684/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6277e-04 - val_loss: 2.2108e-04\n",
      "Epoch 685/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.9725e-04 - val_loss: 2.1641e-04\n",
      "Epoch 686/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4828e-04 - val_loss: 2.1526e-04\n",
      "Epoch 687/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.9383e-04 - val_loss: 2.1535e-04\n",
      "Epoch 688/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6939e-04 - val_loss: 2.1638e-04\n",
      "Epoch 689/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.2091e-04 - val_loss: 2.1898e-04\n",
      "Epoch 690/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6182e-04 - val_loss: 2.1831e-04\n",
      "Epoch 691/1000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 2.3508e-04 - val_loss: 2.1573e-04\n",
      "Epoch 692/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.4295e-04 - val_loss: 2.1553e-04\n",
      "Epoch 693/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.2876e-04 - val_loss: 2.1643e-04\n",
      "Epoch 694/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 3.0103e-04 - val_loss: 2.1805e-04\n",
      "Epoch 695/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.1792e-04 - val_loss: 2.1778e-04\n",
      "Epoch 696/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4327e-04 - val_loss: 2.1521e-04\n",
      "Epoch 697/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8087e-04 - val_loss: 2.1610e-04\n",
      "Epoch 698/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.4924e-04 - val_loss: 2.1622e-04\n",
      "Epoch 699/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.7498e-04 - val_loss: 2.1730e-04\n",
      "Epoch 700/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 2.2421e-04 - val_loss: 2.1709e-04\n",
      "Epoch 701/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5975e-04 - val_loss: 2.1521e-04\n",
      "Epoch 702/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5510e-04 - val_loss: 2.1575e-04\n",
      "Epoch 703/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7425e-04 - val_loss: 2.1541e-04\n",
      "Epoch 704/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.7708e-04 - val_loss: 2.1551e-04\n",
      "Epoch 705/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3555e-04 - val_loss: 2.1918e-04\n",
      "Epoch 706/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.0934e-04 - val_loss: 2.1697e-04\n",
      "Epoch 707/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.6145e-04 - val_loss: 2.1472e-04\n",
      "Epoch 708/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.6545e-04 - val_loss: 2.1728e-04\n",
      "Epoch 709/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.5757e-04 - val_loss: 2.1596e-04\n",
      "Epoch 710/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8763e-04 - val_loss: 2.1752e-04\n",
      "Epoch 711/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.6713e-04 - val_loss: 2.1872e-04\n",
      "Epoch 712/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.8593e-04 - val_loss: 2.1833e-04\n",
      "Epoch 713/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.9513e-04 - val_loss: 2.1825e-04\n",
      "Epoch 714/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 3.3349e-04 - val_loss: 2.1587e-04\n",
      "Epoch 715/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4195e-04 - val_loss: 2.1394e-04\n",
      "Epoch 716/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.3504e-04 - val_loss: 2.1398e-04\n",
      "Epoch 717/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.3330e-04 - val_loss: 2.1394e-04\n",
      "Epoch 718/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.2642e-04 - val_loss: 2.1284e-04\n",
      "Epoch 719/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.5861e-04 - val_loss: 2.1348e-04\n",
      "Epoch 720/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.5574e-04 - val_loss: 2.1327e-04\n",
      "Epoch 721/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3755e-04 - val_loss: 2.1357e-04\n",
      "Epoch 722/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9410e-04 - val_loss: 2.1761e-04\n",
      "Epoch 723/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.6268e-04 - val_loss: 2.1480e-04\n",
      "Epoch 724/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4464e-04 - val_loss: 2.1511e-04\n",
      "Epoch 725/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4081e-04 - val_loss: 2.1469e-04\n",
      "Epoch 726/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5004e-04 - val_loss: 2.1675e-04\n",
      "Epoch 727/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.8367e-04 - val_loss: 2.1347e-04\n",
      "Epoch 728/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.7375e-04 - val_loss: 2.1331e-04\n",
      "Epoch 729/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5191e-04 - val_loss: 2.1538e-04\n",
      "Epoch 730/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.1399e-04 - val_loss: 2.1555e-04\n",
      "Epoch 731/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.5578e-04 - val_loss: 2.1507e-04\n",
      "Epoch 732/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.1691e-04 - val_loss: 2.1369e-04\n",
      "Epoch 733/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.4656e-04 - val_loss: 2.1332e-04\n",
      "Epoch 734/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4728e-04 - val_loss: 2.1277e-04\n",
      "Epoch 735/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.2581e-04 - val_loss: 2.1317e-04\n",
      "Epoch 736/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2526e-04 - val_loss: 2.1333e-04\n",
      "Epoch 737/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.5110e-04 - val_loss: 2.1441e-04\n",
      "Epoch 738/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.8883e-04 - val_loss: 2.1422e-04\n",
      "Epoch 739/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.9120e-04 - val_loss: 2.1408e-04\n",
      "Epoch 740/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.6839e-04 - val_loss: 2.1361e-04\n",
      "Epoch 741/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 3.8242e-04 - val_loss: 2.1302e-04\n",
      "Epoch 742/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3547e-04 - val_loss: 2.1257e-04\n",
      "Epoch 743/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3516e-04 - val_loss: 2.1197e-04\n",
      "Epoch 744/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.1652e-04 - val_loss: 2.1220e-04\n",
      "Epoch 745/1000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 2.9517e-04 - val_loss: 2.1221e-04\n",
      "Epoch 746/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.3207e-04 - val_loss: 2.1221e-04\n",
      "Epoch 747/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5045e-04 - val_loss: 2.1193e-04\n",
      "Epoch 748/1000\n",
      "4/4 [==============================] - 1s 390ms/step - loss: 2.4452e-04 - val_loss: 2.1226e-04\n",
      "Epoch 749/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.2292e-04 - val_loss: 2.1229e-04\n",
      "Epoch 750/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.5877e-04 - val_loss: 2.1221e-04\n",
      "Epoch 751/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6558e-04 - val_loss: 2.1251e-04\n",
      "Epoch 752/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.6758e-04 - val_loss: 2.1217e-04\n",
      "Epoch 753/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6298e-04 - val_loss: 2.1185e-04\n",
      "Epoch 754/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5093e-04 - val_loss: 2.1257e-04\n",
      "Epoch 755/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4434e-04 - val_loss: 2.1166e-04\n",
      "Epoch 756/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6955e-04 - val_loss: 2.1208e-04\n",
      "Epoch 757/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.7827e-04 - val_loss: 2.1154e-04\n",
      "Epoch 758/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8721e-04 - val_loss: 2.1161e-04\n",
      "Epoch 759/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7226e-04 - val_loss: 2.1217e-04\n",
      "Epoch 760/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7230e-04 - val_loss: 2.1318e-04\n",
      "Epoch 761/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2804e-04 - val_loss: 2.1236e-04\n",
      "Epoch 762/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.4610e-04 - val_loss: 2.1297e-04\n",
      "Epoch 763/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.9173e-04 - val_loss: 2.1137e-04\n",
      "Epoch 764/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.1614e-04 - val_loss: 2.1138e-04\n",
      "Epoch 765/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.3383e-04 - val_loss: 2.1094e-04\n",
      "Epoch 766/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7892e-04 - val_loss: 2.1078e-04\n",
      "Epoch 767/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.5053e-04 - val_loss: 2.1120e-04\n",
      "Epoch 768/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.3880e-04 - val_loss: 2.1250e-04\n",
      "Epoch 769/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.5705e-04 - val_loss: 2.1151e-04\n",
      "Epoch 770/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4013e-04 - val_loss: 2.1131e-04\n",
      "Epoch 771/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.5863e-04 - val_loss: 2.1211e-04\n",
      "Epoch 772/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.4905e-04 - val_loss: 2.1104e-04\n",
      "Epoch 773/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2712e-04 - val_loss: 2.1080e-04\n",
      "Epoch 774/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.2840e-04 - val_loss: 2.1091e-04\n",
      "Epoch 775/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.8862e-04 - val_loss: 2.1042e-04\n",
      "Epoch 776/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.4151e-04 - val_loss: 2.1317e-04\n",
      "Epoch 777/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2531e-04 - val_loss: 2.1289e-04\n",
      "Epoch 778/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.4748e-04 - val_loss: 2.1183e-04\n",
      "Epoch 779/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4667e-04 - val_loss: 2.1178e-04\n",
      "Epoch 780/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6952e-04 - val_loss: 2.1129e-04\n",
      "Epoch 781/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.3616e-04 - val_loss: 2.1106e-04\n",
      "Epoch 782/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8095e-04 - val_loss: 2.1036e-04\n",
      "Epoch 783/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.8954e-04 - val_loss: 2.1041e-04\n",
      "Epoch 784/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8067e-04 - val_loss: 2.1146e-04\n",
      "Epoch 785/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.2877e-04 - val_loss: 2.1203e-04\n",
      "Epoch 786/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7880e-04 - val_loss: 2.1092e-04\n",
      "Epoch 787/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6779e-04 - val_loss: 2.1100e-04\n",
      "Epoch 788/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3178e-04 - val_loss: 2.1072e-04\n",
      "Epoch 789/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.6294e-04 - val_loss: 2.1178e-04\n",
      "Epoch 790/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.2208e-04 - val_loss: 2.1140e-04\n",
      "Epoch 791/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.8344e-04 - val_loss: 2.1033e-04\n",
      "Epoch 792/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 2.3260e-04 - val_loss: 2.0998e-04\n",
      "Epoch 793/1000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 2.5459e-04 - val_loss: 2.0990e-04\n",
      "Epoch 794/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.9924e-04 - val_loss: 2.0962e-04\n",
      "Epoch 795/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.2098e-04 - val_loss: 2.1021e-04\n",
      "Epoch 796/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.1959e-04 - val_loss: 2.1100e-04\n",
      "Epoch 797/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7863e-04 - val_loss: 2.1074e-04\n",
      "Epoch 798/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.9034e-04 - val_loss: 2.1175e-04\n",
      "Epoch 799/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6204e-04 - val_loss: 2.1044e-04\n",
      "Epoch 800/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4937e-04 - val_loss: 2.1039e-04\n",
      "Epoch 801/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.5408e-04 - val_loss: 2.1045e-04\n",
      "Epoch 802/1000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 2.8593e-04 - val_loss: 2.1051e-04\n",
      "Epoch 803/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 3.4390e-04 - val_loss: 2.1082e-04\n",
      "Epoch 804/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 3.1679e-04 - val_loss: 2.1089e-04\n",
      "Epoch 805/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.8821e-04 - val_loss: 2.1083e-04\n",
      "Epoch 806/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.2290e-04 - val_loss: 2.1241e-04\n",
      "Epoch 807/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.7332e-04 - val_loss: 2.1078e-04\n",
      "Epoch 808/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 4.0749e-04 - val_loss: 2.1108e-04\n",
      "Epoch 809/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.2120e-04 - val_loss: 2.1117e-04\n",
      "Epoch 810/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6614e-04 - val_loss: 2.1056e-04\n",
      "Epoch 811/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4851e-04 - val_loss: 2.0984e-04\n",
      "Epoch 812/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.2090e-04 - val_loss: 2.1118e-04\n",
      "Epoch 813/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.2388e-04 - val_loss: 2.1042e-04\n",
      "Epoch 814/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7129e-04 - val_loss: 2.0943e-04\n",
      "Epoch 815/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.5687e-04 - val_loss: 2.1025e-04\n",
      "Epoch 816/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8896e-04 - val_loss: 2.0992e-04\n",
      "Epoch 817/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.1983e-04 - val_loss: 2.0955e-04\n",
      "Epoch 818/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7614e-04 - val_loss: 2.1141e-04\n",
      "Epoch 819/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6780e-04 - val_loss: 2.0949e-04\n",
      "Epoch 820/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.5162e-04 - val_loss: 2.0996e-04\n",
      "Epoch 821/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.4074e-04 - val_loss: 2.0954e-04\n",
      "Epoch 822/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4984e-04 - val_loss: 2.0918e-04\n",
      "Epoch 823/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2394e-04 - val_loss: 2.0919e-04\n",
      "Epoch 824/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.5060e-04 - val_loss: 2.0912e-04\n",
      "Epoch 825/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.9296e-04 - val_loss: 2.0936e-04\n",
      "Epoch 826/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.6313e-04 - val_loss: 2.0967e-04\n",
      "Epoch 827/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.3207e-04 - val_loss: 2.0971e-04\n",
      "Epoch 828/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 3.0245e-04 - val_loss: 2.1031e-04\n",
      "Epoch 829/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6200e-04 - val_loss: 2.0961e-04\n",
      "Epoch 830/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.4922e-04 - val_loss: 2.0963e-04\n",
      "Epoch 831/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4605e-04 - val_loss: 2.0916e-04\n",
      "Epoch 832/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7674e-04 - val_loss: 2.0914e-04\n",
      "Epoch 833/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6805e-04 - val_loss: 2.0884e-04\n",
      "Epoch 834/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 3.4943e-04 - val_loss: 2.0933e-04\n",
      "Epoch 835/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.8869e-04 - val_loss: 2.0927e-04\n",
      "Epoch 836/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.7683e-04 - val_loss: 2.0971e-04\n",
      "Epoch 837/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.0454e-04 - val_loss: 2.1027e-04\n",
      "Epoch 838/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.7679e-04 - val_loss: 2.1075e-04\n",
      "Epoch 839/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 3.2031e-04 - val_loss: 2.0987e-04\n",
      "Epoch 840/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.3244e-04 - val_loss: 2.0931e-04\n",
      "Epoch 841/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5879e-04 - val_loss: 2.0929e-04\n",
      "Epoch 842/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.0984e-04 - val_loss: 2.0879e-04\n",
      "Epoch 843/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8255e-04 - val_loss: 2.0872e-04\n",
      "Epoch 844/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.0073e-04 - val_loss: 2.0978e-04\n",
      "Epoch 845/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.5375e-04 - val_loss: 2.0926e-04\n",
      "Epoch 846/1000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 2.8658e-04 - val_loss: 2.0931e-04\n",
      "Epoch 847/1000\n",
      "4/4 [==============================] - 1s 384ms/step - loss: 2.2020e-04 - val_loss: 2.0881e-04\n",
      "Epoch 848/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8530e-04 - val_loss: 2.0838e-04\n",
      "Epoch 849/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.3529e-04 - val_loss: 2.0845e-04\n",
      "Epoch 850/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.4929e-04 - val_loss: 2.0862e-04\n",
      "Epoch 851/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.2409e-04 - val_loss: 2.0930e-04\n",
      "Epoch 852/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.1330e-04 - val_loss: 2.0876e-04\n",
      "Epoch 853/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3898e-04 - val_loss: 2.0860e-04\n",
      "Epoch 854/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.2360e-04 - val_loss: 2.0812e-04\n",
      "Epoch 855/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.8239e-04 - val_loss: 2.0802e-04\n",
      "Epoch 856/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4793e-04 - val_loss: 2.0806e-04\n",
      "Epoch 857/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.0788e-04 - val_loss: 2.0810e-04\n",
      "Epoch 858/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.2776e-04 - val_loss: 2.0814e-04\n",
      "Epoch 859/1000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 2.7876e-04 - val_loss: 2.0824e-04\n",
      "Epoch 860/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.4053e-04 - val_loss: 2.0906e-04\n",
      "Epoch 861/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8778e-04 - val_loss: 2.0875e-04\n",
      "Epoch 862/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.2939e-04 - val_loss: 2.0838e-04\n",
      "Epoch 863/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.7251e-04 - val_loss: 2.0816e-04\n",
      "Epoch 864/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7128e-04 - val_loss: 2.0809e-04\n",
      "Epoch 865/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.6294e-04 - val_loss: 2.0850e-04\n",
      "Epoch 866/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 3.5506e-04 - val_loss: 2.0893e-04\n",
      "Epoch 867/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 2.3557e-04 - val_loss: 2.0815e-04\n",
      "Epoch 868/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.7880e-04 - val_loss: 2.0866e-04\n",
      "Epoch 869/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2131e-04 - val_loss: 2.0775e-04\n",
      "Epoch 870/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.3565e-04 - val_loss: 2.0807e-04\n",
      "Epoch 871/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3345e-04 - val_loss: 2.0806e-04\n",
      "Epoch 872/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.6049e-04 - val_loss: 2.0823e-04\n",
      "Epoch 873/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.9463e-04 - val_loss: 2.0782e-04\n",
      "Epoch 874/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.2337e-04 - val_loss: 2.0828e-04\n",
      "Epoch 875/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.0122e-04 - val_loss: 2.0761e-04\n",
      "Epoch 876/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.3117e-04 - val_loss: 2.0762e-04\n",
      "Epoch 877/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 3.4381e-04 - val_loss: 2.0787e-04\n",
      "Epoch 878/1000\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 2.2941e-04 - val_loss: 2.0794e-04\n",
      "Epoch 879/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 3.3754e-04 - val_loss: 2.0771e-04\n",
      "Epoch 880/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6058e-04 - val_loss: 2.0793e-04\n",
      "Epoch 881/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 4.2356e-04 - val_loss: 2.0772e-04\n",
      "Epoch 882/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.2828e-04 - val_loss: 2.0778e-04\n",
      "Epoch 883/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4217e-04 - val_loss: 2.0782e-04\n",
      "Epoch 884/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3879e-04 - val_loss: 2.0767e-04\n",
      "Epoch 885/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.5586e-04 - val_loss: 2.0820e-04\n",
      "Epoch 886/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5354e-04 - val_loss: 2.0785e-04\n",
      "Epoch 887/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.1664e-04 - val_loss: 2.0768e-04\n",
      "Epoch 888/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.9881e-04 - val_loss: 2.0774e-04\n",
      "Epoch 889/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.6432e-04 - val_loss: 2.0759e-04\n",
      "Epoch 890/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.2846e-04 - val_loss: 2.0763e-04\n",
      "Epoch 891/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.4796e-04 - val_loss: 2.0728e-04\n",
      "Epoch 892/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3165e-04 - val_loss: 2.0763e-04\n",
      "Epoch 893/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6101e-04 - val_loss: 2.0768e-04\n",
      "Epoch 894/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 3.2847e-04 - val_loss: 2.0768e-04\n",
      "Epoch 895/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5414e-04 - val_loss: 2.0808e-04\n",
      "Epoch 896/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.3941e-04 - val_loss: 2.0754e-04\n",
      "Epoch 897/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.0517e-04 - val_loss: 2.0726e-04\n",
      "Epoch 898/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7301e-04 - val_loss: 2.0721e-04\n",
      "Epoch 899/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7442e-04 - val_loss: 2.0704e-04\n",
      "Epoch 900/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2869e-04 - val_loss: 2.0700e-04\n",
      "Epoch 901/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8640e-04 - val_loss: 2.0788e-04\n",
      "Epoch 902/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4385e-04 - val_loss: 2.0739e-04\n",
      "Epoch 903/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4657e-04 - val_loss: 2.0759e-04\n",
      "Epoch 904/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.4469e-04 - val_loss: 2.0738e-04\n",
      "Epoch 905/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.5314e-04 - val_loss: 2.0731e-04\n",
      "Epoch 906/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8199e-04 - val_loss: 2.0750e-04\n",
      "Epoch 907/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.1617e-04 - val_loss: 2.0734e-04\n",
      "Epoch 908/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.4489e-04 - val_loss: 2.0715e-04\n",
      "Epoch 909/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.2059e-04 - val_loss: 2.0701e-04\n",
      "Epoch 910/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5304e-04 - val_loss: 2.0714e-04\n",
      "Epoch 911/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8096e-04 - val_loss: 2.0719e-04\n",
      "Epoch 912/1000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 2.6608e-04 - val_loss: 2.0698e-04\n",
      "Epoch 913/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.7541e-04 - val_loss: 2.0691e-04\n",
      "Epoch 914/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.4700e-04 - val_loss: 2.0681e-04\n",
      "Epoch 915/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4370e-04 - val_loss: 2.0681e-04\n",
      "Epoch 916/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8148e-04 - val_loss: 2.0688e-04\n",
      "Epoch 917/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.8693e-04 - val_loss: 2.0680e-04\n",
      "Epoch 918/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2965e-04 - val_loss: 2.0687e-04\n",
      "Epoch 919/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.1059e-04 - val_loss: 2.0695e-04\n",
      "Epoch 920/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.8479e-04 - val_loss: 2.0693e-04\n",
      "Epoch 921/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.3952e-04 - val_loss: 2.0710e-04\n",
      "Epoch 922/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5092e-04 - val_loss: 2.0706e-04\n",
      "Epoch 923/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.0858e-04 - val_loss: 2.0692e-04\n",
      "Epoch 924/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.7131e-04 - val_loss: 2.0709e-04\n",
      "Epoch 925/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.2196e-04 - val_loss: 2.0665e-04\n",
      "Epoch 926/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.4900e-04 - val_loss: 2.0669e-04\n",
      "Epoch 927/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2534e-04 - val_loss: 2.0686e-04\n",
      "Epoch 928/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 3.0177e-04 - val_loss: 2.0660e-04\n",
      "Epoch 929/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4035e-04 - val_loss: 2.0702e-04\n",
      "Epoch 930/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.1894e-04 - val_loss: 2.0698e-04\n",
      "Epoch 931/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.5567e-04 - val_loss: 2.0660e-04\n",
      "Epoch 932/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.3969e-04 - val_loss: 2.0655e-04\n",
      "Epoch 933/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.4509e-04 - val_loss: 2.0703e-04\n",
      "Epoch 934/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.3017e-04 - val_loss: 2.0700e-04\n",
      "Epoch 935/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 2.5511e-04 - val_loss: 2.0672e-04\n",
      "Epoch 936/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.2765e-04 - val_loss: 2.0653e-04\n",
      "Epoch 937/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.4636e-04 - val_loss: 2.0660e-04\n",
      "Epoch 938/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8319e-04 - val_loss: 2.0647e-04\n",
      "Epoch 939/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.2874e-04 - val_loss: 2.0649e-04\n",
      "Epoch 940/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4905e-04 - val_loss: 2.0636e-04\n",
      "Epoch 941/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6283e-04 - val_loss: 2.0645e-04\n",
      "Epoch 942/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.2824e-04 - val_loss: 2.0661e-04\n",
      "Epoch 943/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.3675e-04 - val_loss: 2.0657e-04\n",
      "Epoch 944/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.4496e-04 - val_loss: 2.0642e-04\n",
      "Epoch 945/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.1258e-04 - val_loss: 2.0625e-04\n",
      "Epoch 946/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.4983e-04 - val_loss: 2.0620e-04\n",
      "Epoch 947/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.5847e-04 - val_loss: 2.0627e-04\n",
      "Epoch 948/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 2.8244e-04 - val_loss: 2.0633e-04\n",
      "Epoch 949/1000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 2.2689e-04 - val_loss: 2.0641e-04\n",
      "Epoch 950/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.5868e-04 - val_loss: 2.0631e-04\n",
      "Epoch 951/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.3748e-04 - val_loss: 2.0633e-04\n",
      "Epoch 952/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3934e-04 - val_loss: 2.0633e-04\n",
      "Epoch 953/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.6547e-04 - val_loss: 2.0632e-04\n",
      "Epoch 954/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.6299e-04 - val_loss: 2.0648e-04\n",
      "Epoch 955/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.2732e-04 - val_loss: 2.0626e-04\n",
      "Epoch 956/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.8233e-04 - val_loss: 2.0639e-04\n",
      "Epoch 957/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.8371e-04 - val_loss: 2.0643e-04\n",
      "Epoch 958/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.2455e-04 - val_loss: 2.0626e-04\n",
      "Epoch 959/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.7474e-04 - val_loss: 2.0617e-04\n",
      "Epoch 960/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.9375e-04 - val_loss: 2.0613e-04\n",
      "Epoch 961/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.5671e-04 - val_loss: 2.0632e-04\n",
      "Epoch 962/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.3210e-04 - val_loss: 2.0635e-04\n",
      "Epoch 963/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.9268e-04 - val_loss: 2.0599e-04\n",
      "Epoch 964/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8686e-04 - val_loss: 2.0593e-04\n",
      "Epoch 965/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.7461e-04 - val_loss: 2.0603e-04\n",
      "Epoch 966/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.1261e-04 - val_loss: 2.0592e-04\n",
      "Epoch 967/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2522e-04 - val_loss: 2.0581e-04\n",
      "Epoch 968/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.3762e-04 - val_loss: 2.0629e-04\n",
      "Epoch 969/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.6385e-04 - val_loss: 2.0613e-04\n",
      "Epoch 970/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.0587e-04 - val_loss: 2.0591e-04\n",
      "Epoch 971/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.9480e-04 - val_loss: 2.0572e-04\n",
      "Epoch 972/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 2.6072e-04 - val_loss: 2.0571e-04\n",
      "Epoch 973/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.8939e-04 - val_loss: 2.0569e-04\n",
      "Epoch 974/1000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 2.6346e-04 - val_loss: 2.0589e-04\n",
      "Epoch 975/1000\n",
      "4/4 [==============================] - 1s 387ms/step - loss: 3.5527e-04 - val_loss: 2.0624e-04\n",
      "Epoch 976/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8895e-04 - val_loss: 2.0599e-04\n",
      "Epoch 977/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.4726e-04 - val_loss: 2.0581e-04\n",
      "Epoch 978/1000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 2.3303e-04 - val_loss: 2.0584e-04\n",
      "Epoch 979/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 2.8600e-04 - val_loss: 2.0592e-04\n",
      "Epoch 980/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 2.6322e-04 - val_loss: 2.0607e-04\n",
      "Epoch 981/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.4824e-04 - val_loss: 2.0574e-04\n",
      "Epoch 982/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.2298e-04 - val_loss: 2.0585e-04\n",
      "Epoch 983/1000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 2.6347e-04 - val_loss: 2.0566e-04\n",
      "Epoch 984/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.2580e-04 - val_loss: 2.0566e-04\n",
      "Epoch 985/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.4506e-04 - val_loss: 2.0577e-04\n",
      "Epoch 986/1000\n",
      "4/4 [==============================] - 1s 383ms/step - loss: 2.2686e-04 - val_loss: 2.0593e-04\n",
      "Epoch 987/1000\n",
      "4/4 [==============================] - 1s 388ms/step - loss: 2.7012e-04 - val_loss: 2.0586e-04\n",
      "Epoch 988/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5235e-04 - val_loss: 2.0603e-04\n",
      "Epoch 989/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.3843e-04 - val_loss: 2.0581e-04\n",
      "Epoch 990/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 2.5938e-04 - val_loss: 2.0574e-04\n",
      "Epoch 991/1000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 3.6417e-04 - val_loss: 2.0554e-04\n",
      "Epoch 992/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5925e-04 - val_loss: 2.0562e-04\n",
      "Epoch 993/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 2.7417e-04 - val_loss: 2.0588e-04\n",
      "Epoch 994/1000\n",
      "4/4 [==============================] - 2s 387ms/step - loss: 2.5382e-04 - val_loss: 2.0632e-04\n",
      "Epoch 995/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 2.6640e-04 - val_loss: 2.0591e-04\n",
      "Epoch 996/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.3100e-04 - val_loss: 2.0563e-04\n",
      "Epoch 997/1000\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 3.1335e-04 - val_loss: 2.0622e-04\n",
      "Epoch 998/1000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 2.2441e-04 - val_loss: 2.0649e-04\n",
      "Epoch 999/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 2.3718e-04 - val_loss: 2.0583e-04\n",
      "Epoch 1000/1000\n",
      "4/4 [==============================] - 2s 388ms/step - loss: 2.4192e-04 - val_loss: 2.0548e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./trained_fully_conv\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./trained_fully_conv\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 1s 69ms/step - loss: 0.0635 - val_loss: 0.0785\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0555 - val_loss: 0.1355\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0498 - val_loss: 0.2053\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0462 - val_loss: 0.2451\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0437 - val_loss: 0.2553\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0417 - val_loss: 0.2452\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0402 - val_loss: 0.2273\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0386 - val_loss: 0.2067\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0371 - val_loss: 0.1802\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0360 - val_loss: 0.1575\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0348 - val_loss: 0.1402\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0338 - val_loss: 0.1190\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0329 - val_loss: 0.1018\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0319 - val_loss: 0.0876\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0308 - val_loss: 0.0747\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0299 - val_loss: 0.0694\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0289 - val_loss: 0.0594\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0280 - val_loss: 0.0533\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0274 - val_loss: 0.0459\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0265 - val_loss: 0.0449\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0258 - val_loss: 0.0379\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0252 - val_loss: 0.0348\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0247 - val_loss: 0.0322\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0241 - val_loss: 0.0296\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0236 - val_loss: 0.0270\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0232 - val_loss: 0.0268\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0228 - val_loss: 0.0257\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0220 - val_loss: 0.0243\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0216 - val_loss: 0.0244\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0213 - val_loss: 0.0234\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0210 - val_loss: 0.0226\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0206 - val_loss: 0.0244\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0203 - val_loss: 0.0244\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0201 - val_loss: 0.0223\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0197 - val_loss: 0.0245\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0195 - val_loss: 0.0252\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0192 - val_loss: 0.0228\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0189 - val_loss: 0.0251\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0187 - val_loss: 0.0268\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0184 - val_loss: 0.0232\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0183 - val_loss: 0.0221\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0179 - val_loss: 0.0235\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0177 - val_loss: 0.0272\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0174 - val_loss: 0.0270\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0172 - val_loss: 0.0272\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0170 - val_loss: 0.0274\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0168 - val_loss: 0.0235\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0167 - val_loss: 0.0217\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0164 - val_loss: 0.0273\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0162 - val_loss: 0.0353\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0161 - val_loss: 0.0275\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0159 - val_loss: 0.0217\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0246\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0155 - val_loss: 0.0249\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0154 - val_loss: 0.0272\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0153 - val_loss: 0.0247\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0150 - val_loss: 0.0289\n",
      "Epoch 59/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0148 - val_loss: 0.0272\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0147 - val_loss: 0.0249\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0146 - val_loss: 0.0222\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0145 - val_loss: 0.0218\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0143 - val_loss: 0.0199\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0142 - val_loss: 0.0233\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0141 - val_loss: 0.0220\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0139 - val_loss: 0.0202\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0138 - val_loss: 0.0214\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0137 - val_loss: 0.0243\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0136 - val_loss: 0.0231\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0135 - val_loss: 0.0212\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0135 - val_loss: 0.0207\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0133 - val_loss: 0.0213\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0133 - val_loss: 0.0182\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0131 - val_loss: 0.0181\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0203\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0130 - val_loss: 0.0257\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0129 - val_loss: 0.0272\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0129 - val_loss: 0.0226\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0128 - val_loss: 0.0198\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0127 - val_loss: 0.0173\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0126 - val_loss: 0.0157\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0125 - val_loss: 0.0169\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0125 - val_loss: 0.0221\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0124 - val_loss: 0.0249\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0124 - val_loss: 0.0200\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0122 - val_loss: 0.0171\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0121 - val_loss: 0.0165\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0120 - val_loss: 0.0184\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0119 - val_loss: 0.0199\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0118 - val_loss: 0.0162\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0119 - val_loss: 0.0191\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0118 - val_loss: 0.0176\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0117 - val_loss: 0.0177\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0116 - val_loss: 0.0166\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0116 - val_loss: 0.0166\n",
      "Epoch 97/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0115 - val_loss: 0.0166\n",
      "Epoch 98/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 99/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0115 - val_loss: 0.0184\n",
      "Epoch 100/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0114 - val_loss: 0.0170\n",
      "Epoch 101/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0113 - val_loss: 0.0170\n",
      "Epoch 102/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0114 - val_loss: 0.0169\n",
      "Epoch 103/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0113 - val_loss: 0.0181\n",
      "Epoch 104/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0112 - val_loss: 0.0152\n",
      "Epoch 105/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0111 - val_loss: 0.0150\n",
      "Epoch 106/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0111 - val_loss: 0.0144\n",
      "Epoch 107/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0110 - val_loss: 0.0141\n",
      "Epoch 108/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0111 - val_loss: 0.0164\n",
      "Epoch 109/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0110 - val_loss: 0.0165\n",
      "Epoch 110/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0109 - val_loss: 0.0141\n",
      "Epoch 111/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0109 - val_loss: 0.0139\n",
      "Epoch 112/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 113/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 114/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0108 - val_loss: 0.0153\n",
      "Epoch 115/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0107 - val_loss: 0.0150\n",
      "Epoch 116/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 117/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 118/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 119/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 120/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 121/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0105 - val_loss: 0.0145\n",
      "Epoch 122/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0105 - val_loss: 0.0152\n",
      "Epoch 123/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0105 - val_loss: 0.0172\n",
      "Epoch 124/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0105 - val_loss: 0.0175\n",
      "Epoch 125/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0105 - val_loss: 0.0143\n",
      "Epoch 126/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0104 - val_loss: 0.0145\n",
      "Epoch 127/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0107 - val_loss: 0.0132\n",
      "Epoch 128/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0104 - val_loss: 0.0131\n",
      "Epoch 129/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 130/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 131/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0103 - val_loss: 0.0135\n",
      "Epoch 132/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0102 - val_loss: 0.0134\n",
      "Epoch 133/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0103 - val_loss: 0.0134\n",
      "Epoch 134/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0103 - val_loss: 0.0132\n",
      "Epoch 135/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0102 - val_loss: 0.0137\n",
      "Epoch 136/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0101 - val_loss: 0.0137\n",
      "Epoch 137/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0102 - val_loss: 0.0133\n",
      "Epoch 138/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0101 - val_loss: 0.0132\n",
      "Epoch 139/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0102 - val_loss: 0.0130\n",
      "Epoch 140/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0101 - val_loss: 0.0130\n",
      "Epoch 141/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0100 - val_loss: 0.0132\n",
      "Epoch 142/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 143/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 144/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0100 - val_loss: 0.0128\n",
      "Epoch 145/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0101 - val_loss: 0.0133\n",
      "Epoch 146/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0100 - val_loss: 0.0128\n",
      "Epoch 147/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 148/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 149/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0130\n",
      "Epoch 150/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 151/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 152/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 153/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0099 - val_loss: 0.0124\n",
      "Epoch 154/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 155/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0099 - val_loss: 0.0124\n",
      "Epoch 156/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 157/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 158/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 159/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 160/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 161/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0124\n",
      "Epoch 162/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0100 - val_loss: 0.0129\n",
      "Epoch 163/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 164/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 165/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 166/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0131\n",
      "Epoch 167/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 168/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 169/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 170/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0098 - val_loss: 0.0124\n",
      "Epoch 171/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 172/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 173/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0129\n",
      "Epoch 174/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 175/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 176/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0124\n",
      "Epoch 177/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 178/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 179/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 180/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 181/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 182/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0099 - val_loss: 0.0130\n",
      "Epoch 183/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0129\n",
      "Epoch 184/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 185/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 186/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 187/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 188/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 189/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 190/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0131\n",
      "Epoch 191/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 192/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 193/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 194/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 195/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0130\n",
      "Epoch 196/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0131\n",
      "Epoch 197/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 198/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 199/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.0130\n",
      "Epoch 200/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 201/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0130\n",
      "Epoch 202/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 203/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 204/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 205/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0131\n",
      "Epoch 206/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 207/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 208/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0131\n",
      "Epoch 209/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0143\n",
      "Epoch 210/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0138\n",
      "Epoch 211/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 212/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0096 - val_loss: 0.0141\n",
      "Epoch 213/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0096 - val_loss: 0.0131\n",
      "Epoch 214/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 215/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0144\n",
      "Epoch 216/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 217/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 218/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0137\n",
      "Epoch 219/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 220/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 221/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0139\n",
      "Epoch 222/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 223/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 224/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0133\n",
      "Epoch 225/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0123\n",
      "Epoch 226/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 227/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0132\n",
      "Epoch 228/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0096 - val_loss: 0.0123\n",
      "Epoch 229/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 230/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 231/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 232/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 233/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 234/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 235/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 236/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 237/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0137\n",
      "Epoch 238/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 239/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 240/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 241/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 242/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 243/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0131\n",
      "Epoch 244/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 245/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 246/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 247/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 248/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 249/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 250/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 251/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 252/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0132\n",
      "Epoch 253/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 254/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 255/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 256/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 257/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 258/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 259/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 260/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 261/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 262/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 263/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 264/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 265/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0133\n",
      "Epoch 266/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0131\n",
      "Epoch 267/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 268/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0129\n",
      "Epoch 269/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0134\n",
      "Epoch 270/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 271/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 272/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0130\n",
      "Epoch 273/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 274/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 275/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0126\n",
      "Epoch 276/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 277/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 278/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 279/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 280/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 281/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 282/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 283/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 284/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 285/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 286/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 287/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 288/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 289/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 290/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0127\n",
      "Epoch 291/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 292/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 293/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0129\n",
      "Epoch 294/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 295/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 296/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 297/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 298/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 299/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 300/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 301/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 302/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 303/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 304/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 305/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 306/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 307/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 308/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 309/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 310/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 311/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 312/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 313/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 314/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 315/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 316/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0096 - val_loss: 0.0123\n",
      "Epoch 317/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 318/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 319/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 320/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 321/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 322/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 323/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 324/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 325/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 326/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 327/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 328/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 329/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 330/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 331/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 332/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 333/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 334/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 335/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 336/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 337/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 338/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 339/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 340/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0126\n",
      "Epoch 341/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 342/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 343/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 344/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 345/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 346/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 347/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 348/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 349/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 350/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 351/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 352/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0132\n",
      "Epoch 353/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 354/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 355/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 356/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 357/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 358/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 359/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 360/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 361/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 362/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 363/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 364/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 365/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 366/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 367/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 368/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 369/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 370/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 371/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 372/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 373/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 374/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0130\n",
      "Epoch 375/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 376/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 377/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 378/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 379/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 380/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 381/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 382/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 383/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 384/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 385/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 386/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 387/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 388/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 389/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 390/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 391/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 392/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 393/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 394/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 395/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 396/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 397/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 398/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 399/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 400/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 401/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0134\n",
      "Epoch 402/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0132\n",
      "Epoch 403/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 404/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0131\n",
      "Epoch 405/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 406/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 407/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 408/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 409/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 410/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 411/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 412/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 413/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 414/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 415/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 416/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 417/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 418/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 419/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 420/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 421/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 422/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 423/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 424/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 425/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 426/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 427/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 428/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 429/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 430/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 431/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 432/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 433/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 434/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 435/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 436/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 437/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0095 - val_loss: 0.0126\n",
      "Epoch 438/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 439/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 440/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 441/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 442/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 443/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 444/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 445/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 446/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 447/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 448/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 449/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 450/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 451/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0130\n",
      "Epoch 452/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 453/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0129\n",
      "Epoch 454/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 455/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 456/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 457/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0130\n",
      "Epoch 458/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 459/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 460/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 461/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 462/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 463/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 464/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 465/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 466/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 467/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 468/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 469/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 470/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 471/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 472/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0123\n",
      "Epoch 473/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 474/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 475/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 476/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 477/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 478/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 479/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 480/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 481/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 482/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0094 - val_loss: 0.0130\n",
      "Epoch 483/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 484/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 485/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 486/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 487/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 488/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 489/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 490/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 491/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 492/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 493/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 494/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 495/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 496/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 497/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 498/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 499/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 500/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0129\n",
      "Epoch 501/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 502/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 503/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0095 - val_loss: 0.0129\n",
      "Epoch 504/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 505/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 506/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 507/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 508/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 509/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 510/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 511/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 512/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 513/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 514/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 515/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 516/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 517/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 518/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 519/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 520/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 521/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 522/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 523/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 524/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 525/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 526/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 527/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 528/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 529/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 530/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 531/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 532/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 533/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 534/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 535/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 536/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 537/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "Epoch 538/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 539/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 540/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 541/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 542/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 543/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 544/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 545/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 546/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 547/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 548/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 549/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 550/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 551/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 552/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 553/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 554/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 555/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 556/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 557/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 558/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 559/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 560/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 561/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 562/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 563/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 564/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 565/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 566/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 567/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 568/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 569/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0124\n",
      "Epoch 570/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 571/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 572/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 573/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 574/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 575/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 576/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 577/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 578/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 579/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 580/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 581/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 582/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 583/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 584/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 585/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 586/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 587/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 588/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 589/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 590/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 591/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 592/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 593/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 594/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 595/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 596/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 597/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 598/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0131\n",
      "Epoch 599/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 600/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 601/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 602/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 603/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 604/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 605/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 606/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 607/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 608/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 609/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 610/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 611/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 612/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 613/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 614/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 615/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 616/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 617/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 618/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 619/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 620/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 621/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 622/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 623/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 624/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 625/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 626/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 627/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 628/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 629/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 630/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 631/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 632/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 633/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 634/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 635/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 636/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 637/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 638/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 639/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 640/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 641/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 642/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 643/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 644/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 645/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 646/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 647/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 648/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 649/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 650/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 651/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 652/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 653/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 654/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 655/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 656/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 657/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 658/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 659/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 660/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 661/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 662/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 663/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0125\n",
      "Epoch 664/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 665/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 666/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 667/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 668/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 669/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 670/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 671/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 672/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 673/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 674/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 675/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 676/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 677/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 678/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 679/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 680/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 681/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 682/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 683/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 684/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 685/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 686/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 687/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 688/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 689/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 690/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 691/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 692/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 693/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 694/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 695/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 696/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 697/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 698/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 699/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 700/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 701/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 702/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 703/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 704/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0126\n",
      "Epoch 705/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 706/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 707/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 708/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 709/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 710/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 711/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 712/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 713/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 714/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 715/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 716/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 717/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 718/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 719/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0126\n",
      "Epoch 720/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 721/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 722/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 723/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 724/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0127\n",
      "Epoch 725/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 726/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 727/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 728/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 729/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 730/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 731/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 732/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 733/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 734/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 735/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 736/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 737/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 738/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 739/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 740/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 741/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 742/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 743/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 744/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 745/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 746/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 747/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 748/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0126\n",
      "Epoch 749/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 750/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 751/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 752/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 753/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 754/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 755/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 756/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 757/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 758/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0126\n",
      "Epoch 759/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 760/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 761/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 762/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 763/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 764/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 765/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 766/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 767/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 768/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 769/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 770/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 771/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 772/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 773/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 774/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 775/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 776/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 777/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 778/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 779/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 780/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 781/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 782/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 783/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 784/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 785/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 786/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 787/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 788/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 789/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 790/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 791/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 792/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 793/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 794/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 795/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 796/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 797/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 798/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 799/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 800/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 801/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 802/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 803/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 804/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 805/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 806/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 807/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 808/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 809/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 810/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 811/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 812/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 813/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 814/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 815/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 816/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 817/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 818/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 819/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 820/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 821/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 822/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 823/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 824/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 825/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 826/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 827/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 828/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 829/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 830/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 831/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 832/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 833/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 834/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 835/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 836/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 837/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 838/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 839/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 840/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 841/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 842/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 843/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 844/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 845/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 846/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 847/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 848/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 849/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 850/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 851/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 852/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 853/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 854/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 855/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 856/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 857/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 858/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 859/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 860/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 861/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 862/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 863/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 864/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 865/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 866/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 867/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 868/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 869/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 870/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 871/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 872/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 873/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0091 - val_loss: 0.0127\n",
      "Epoch 874/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 875/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 876/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0127\n",
      "Epoch 877/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 878/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 879/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 880/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 881/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 882/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 883/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 884/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 885/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 886/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 887/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 888/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 889/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 890/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 891/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 892/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 893/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 894/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 895/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 896/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 897/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 898/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 899/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 900/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 901/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 902/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 903/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 904/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 905/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 906/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 907/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 908/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 909/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 910/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 911/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 912/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 913/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 914/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 915/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 916/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 917/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 918/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 919/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 920/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 921/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 922/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 923/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 924/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 925/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 926/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 927/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 928/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 929/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 930/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 931/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 932/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 933/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 934/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 935/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 936/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 937/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 938/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 939/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 940/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 941/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 942/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 943/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 944/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 945/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 946/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 947/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 948/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 949/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 950/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 951/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 952/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 953/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 954/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 955/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 956/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 957/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 958/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 959/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 960/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 961/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 962/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 963/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 964/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 965/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 966/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 967/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 968/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 969/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 970/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 971/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 972/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 973/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 974/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 975/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0128\n",
      "Epoch 976/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0091 - val_loss: 0.0129\n",
      "Epoch 977/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 978/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 979/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 980/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 981/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 982/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0129\n",
      "Epoch 983/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 984/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0091 - val_loss: 0.0129\n",
      "Epoch 985/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 986/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0129\n",
      "Epoch 987/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 988/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 989/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 990/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 991/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0128\n",
      "Epoch 992/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 993/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 994/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 995/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0129\n",
      "Epoch 996/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0129\n",
      "Epoch 997/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0128\n",
      "Epoch 998/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 999/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0129\n",
      "Epoch 1000/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./trained_fully_con\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./trained_fully_con\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/1000\n",
      "    autoencoder_loss_train: 5.459406852722168 - discriminator_loss_train: 1.37445867061615\n",
      "    autoencoder_loss_val: 5.385638236999512 - discriminator_loss_val: 1.3772220611572266\n",
      "Starting epoch 2/1000\n",
      "    autoencoder_loss_train: 5.4573493003845215 - discriminator_loss_train: 1.3716609477996826\n",
      "    autoencoder_loss_val: 5.3835530281066895 - discriminator_loss_val: 1.3748302459716797\n",
      "Starting epoch 3/1000\n",
      "    autoencoder_loss_train: 5.455262660980225 - discriminator_loss_train: 1.3688380718231201\n",
      "    autoencoder_loss_val: 5.38144063949585 - discriminator_loss_val: 1.3723931312561035\n",
      "Starting epoch 4/1000\n",
      "    autoencoder_loss_train: 5.454345703125 - discriminator_loss_train: 1.3660216331481934\n",
      "    autoencoder_loss_val: 5.380526542663574 - discriminator_loss_val: 1.3699169158935547\n",
      "Starting epoch 5/1000\n",
      "    autoencoder_loss_train: 5.4541826248168945 - discriminator_loss_train: 1.3632856607437134\n",
      "    autoencoder_loss_val: 5.380392074584961 - discriminator_loss_val: 1.367490530014038\n",
      "Starting epoch 6/1000\n",
      "    autoencoder_loss_train: 5.45548677444458 - discriminator_loss_train: 1.3602840900421143\n",
      "    autoencoder_loss_val: 5.381745338439941 - discriminator_loss_val: 1.3647699356079102\n",
      "Starting epoch 7/1000\n",
      "    autoencoder_loss_train: 5.457139492034912 - discriminator_loss_train: 1.3575149774551392\n",
      "    autoencoder_loss_val: 5.383474349975586 - discriminator_loss_val: 1.3622547388076782\n",
      "Starting epoch 8/1000\n",
      "    autoencoder_loss_train: 5.459602355957031 - discriminator_loss_train: 1.3547197580337524\n",
      "    autoencoder_loss_val: 5.386067867279053 - discriminator_loss_val: 1.3597087860107422\n",
      "Starting epoch 9/1000\n",
      "    autoencoder_loss_train: 5.461690425872803 - discriminator_loss_train: 1.3521490097045898\n",
      "    autoencoder_loss_val: 5.388365268707275 - discriminator_loss_val: 1.357385516166687\n",
      "Starting epoch 10/1000\n",
      "    autoencoder_loss_train: 5.46258544921875 - discriminator_loss_train: 1.3498352766036987\n",
      "    autoencoder_loss_val: 5.3895673751831055 - discriminator_loss_val: 1.3552849292755127\n",
      "Starting epoch 11/1000\n",
      "    autoencoder_loss_train: 5.4630537033081055 - discriminator_loss_train: 1.3482170104980469\n",
      "    autoencoder_loss_val: 5.390420913696289 - discriminator_loss_val: 1.3538635969161987\n",
      "Starting epoch 12/1000\n",
      "    autoencoder_loss_train: 5.463112831115723 - discriminator_loss_train: 1.3471043109893799\n",
      "    autoencoder_loss_val: 5.3909454345703125 - discriminator_loss_val: 1.352944254875183\n",
      "Starting epoch 13/1000\n",
      "    autoencoder_loss_train: 5.4608893394470215 - discriminator_loss_train: 1.3465862274169922\n",
      "    autoencoder_loss_val: 5.389238357543945 - discriminator_loss_val: 1.3526124954223633\n",
      "Starting epoch 14/1000\n",
      "    autoencoder_loss_train: 5.456984043121338 - discriminator_loss_train: 1.3465182781219482\n",
      "    autoencoder_loss_val: 5.385894775390625 - discriminator_loss_val: 1.3527915477752686\n",
      "Starting epoch 15/1000\n",
      "    autoencoder_loss_train: 5.453230857849121 - discriminator_loss_train: 1.34660005569458\n",
      "    autoencoder_loss_val: 5.3827738761901855 - discriminator_loss_val: 1.353231430053711\n",
      "Starting epoch 16/1000\n",
      "    autoencoder_loss_train: 5.450361728668213 - discriminator_loss_train: 1.3468596935272217\n",
      "    autoencoder_loss_val: 5.380593299865723 - discriminator_loss_val: 1.3539316654205322\n",
      "Starting epoch 17/1000\n",
      "    autoencoder_loss_train: 5.447474479675293 - discriminator_loss_train: 1.3472867012023926\n",
      "    autoencoder_loss_val: 5.3784589767456055 - discriminator_loss_val: 1.3547313213348389\n",
      "Starting epoch 18/1000\n",
      "    autoencoder_loss_train: 5.44441032409668 - discriminator_loss_train: 1.34775710105896\n",
      "    autoencoder_loss_val: 5.376211643218994 - discriminator_loss_val: 1.3555850982666016\n",
      "Starting epoch 19/1000\n",
      "    autoencoder_loss_train: 5.44215726852417 - discriminator_loss_train: 1.3480093479156494\n",
      "    autoencoder_loss_val: 5.374817371368408 - discriminator_loss_val: 1.3562662601470947\n",
      "Starting epoch 20/1000\n",
      "    autoencoder_loss_train: 5.440921783447266 - discriminator_loss_train: 1.3478718996047974\n",
      "    autoencoder_loss_val: 5.374478340148926 - discriminator_loss_val: 1.3565828800201416\n",
      "Starting epoch 21/1000\n",
      "    autoencoder_loss_train: 5.439078330993652 - discriminator_loss_train: 1.346865177154541\n",
      "    autoencoder_loss_val: 5.373592376708984 - discriminator_loss_val: 1.3559290170669556\n",
      "Starting epoch 22/1000\n",
      "    autoencoder_loss_train: 5.439608097076416 - discriminator_loss_train: 1.345338225364685\n",
      "    autoencoder_loss_val: 5.3751373291015625 - discriminator_loss_val: 1.3546513319015503\n",
      "Starting epoch 23/1000\n",
      "    autoencoder_loss_train: 5.443607330322266 - discriminator_loss_train: 1.3429116010665894\n",
      "    autoencoder_loss_val: 5.380194664001465 - discriminator_loss_val: 1.3523900508880615\n",
      "Starting epoch 24/1000\n",
      "    autoencoder_loss_train: 5.448229789733887 - discriminator_loss_train: 1.3399951457977295\n",
      "    autoencoder_loss_val: 5.38592529296875 - discriminator_loss_val: 1.3497341871261597\n",
      "Starting epoch 25/1000\n",
      "    autoencoder_loss_train: 5.453655242919922 - discriminator_loss_train: 1.3362658023834229\n",
      "    autoencoder_loss_val: 5.392487049102783 - discriminator_loss_val: 1.3464082479476929\n",
      "Starting epoch 26/1000\n",
      "    autoencoder_loss_train: 5.459553241729736 - discriminator_loss_train: 1.3325004577636719\n",
      "    autoencoder_loss_val: 5.399579048156738 - discriminator_loss_val: 1.342978596687317\n",
      "Starting epoch 27/1000\n",
      "    autoencoder_loss_train: 5.463782787322998 - discriminator_loss_train: 1.3289190530776978\n",
      "    autoencoder_loss_val: 5.405022144317627 - discriminator_loss_val: 1.3397119045257568\n",
      "Starting epoch 28/1000\n",
      "    autoencoder_loss_train: 5.462967395782471 - discriminator_loss_train: 1.3256916999816895\n",
      "    autoencoder_loss_val: 5.405433654785156 - discriminator_loss_val: 1.336795687675476\n",
      "Starting epoch 29/1000\n",
      "    autoencoder_loss_train: 5.461301326751709 - discriminator_loss_train: 1.3223098516464233\n",
      "    autoencoder_loss_val: 5.405023574829102 - discriminator_loss_val: 1.3337349891662598\n",
      "Starting epoch 30/1000\n",
      "    autoencoder_loss_train: 5.458309173583984 - discriminator_loss_train: 1.3183865547180176\n",
      "    autoencoder_loss_val: 5.40330696105957 - discriminator_loss_val: 1.3301571607589722\n",
      "Starting epoch 31/1000\n",
      "    autoencoder_loss_train: 5.4558539390563965 - discriminator_loss_train: 1.3147728443145752\n",
      "    autoencoder_loss_val: 5.402129173278809 - discriminator_loss_val: 1.326960563659668\n",
      "Starting epoch 32/1000\n",
      "    autoencoder_loss_train: 5.452913761138916 - discriminator_loss_train: 1.311521291732788\n",
      "    autoencoder_loss_val: 5.400490760803223 - discriminator_loss_val: 1.3241125345230103\n",
      "Starting epoch 33/1000\n",
      "    autoencoder_loss_train: 5.44934606552124 - discriminator_loss_train: 1.3083096742630005\n",
      "    autoencoder_loss_val: 5.398232460021973 - discriminator_loss_val: 1.3214011192321777\n",
      "Starting epoch 34/1000\n",
      "    autoencoder_loss_train: 5.444436073303223 - discriminator_loss_train: 1.3047175407409668\n",
      "    autoencoder_loss_val: 5.394587516784668 - discriminator_loss_val: 1.3183329105377197\n",
      "Starting epoch 35/1000\n",
      "    autoencoder_loss_train: 5.439769268035889 - discriminator_loss_train: 1.3008017539978027\n",
      "    autoencoder_loss_val: 5.391185283660889 - discriminator_loss_val: 1.3148512840270996\n",
      "Starting epoch 36/1000\n",
      "    autoencoder_loss_train: 5.433374881744385 - discriminator_loss_train: 1.2970867156982422\n",
      "    autoencoder_loss_val: 5.386076927185059 - discriminator_loss_val: 1.3115062713623047\n",
      "Starting epoch 37/1000\n",
      "    autoencoder_loss_train: 5.424668788909912 - discriminator_loss_train: 1.2940573692321777\n",
      "    autoencoder_loss_val: 5.378697395324707 - discriminator_loss_val: 1.3089046478271484\n",
      "Starting epoch 38/1000\n",
      "    autoencoder_loss_train: 5.409651279449463 - discriminator_loss_train: 1.2922618389129639\n",
      "    autoencoder_loss_val: 5.364946365356445 - discriminator_loss_val: 1.307349681854248\n",
      "Starting epoch 39/1000\n",
      "    autoencoder_loss_train: 5.395646095275879 - discriminator_loss_train: 1.291696548461914\n",
      "    autoencoder_loss_val: 5.352139472961426 - discriminator_loss_val: 1.3068876266479492\n",
      "Starting epoch 40/1000\n",
      "    autoencoder_loss_train: 5.3846282958984375 - discriminator_loss_train: 1.2901172637939453\n",
      "    autoencoder_loss_val: 5.3423309326171875 - discriminator_loss_val: 1.305692195892334\n",
      "Starting epoch 41/1000\n",
      "    autoencoder_loss_train: 5.3781890869140625 - discriminator_loss_train: 1.287351369857788\n",
      "    autoencoder_loss_val: 5.337096691131592 - discriminator_loss_val: 1.303666114807129\n",
      "Starting epoch 42/1000\n",
      "    autoencoder_loss_train: 5.3675336837768555 - discriminator_loss_train: 1.2833251953125\n",
      "    autoencoder_loss_val: 5.3277082443237305 - discriminator_loss_val: 1.3003422021865845\n",
      "Starting epoch 43/1000\n",
      "    autoencoder_loss_train: 5.3543901443481445 - discriminator_loss_train: 1.279078722000122\n",
      "    autoencoder_loss_val: 5.315788269042969 - discriminator_loss_val: 1.29655921459198\n",
      "Starting epoch 44/1000\n",
      "    autoencoder_loss_train: 5.341253280639648 - discriminator_loss_train: 1.275210976600647\n",
      "    autoencoder_loss_val: 5.303869724273682 - discriminator_loss_val: 1.2929019927978516\n",
      "Starting epoch 45/1000\n",
      "    autoencoder_loss_train: 5.318405628204346 - discriminator_loss_train: 1.271979808807373\n",
      "    autoencoder_loss_val: 5.282236099243164 - discriminator_loss_val: 1.2894922494888306\n",
      "Starting epoch 46/1000\n",
      "    autoencoder_loss_train: 5.292998313903809 - discriminator_loss_train: 1.2693455219268799\n",
      "    autoencoder_loss_val: 5.258020401000977 - discriminator_loss_val: 1.2866531610488892\n",
      "Starting epoch 47/1000\n",
      "    autoencoder_loss_train: 5.267872333526611 - discriminator_loss_train: 1.2677831649780273\n",
      "    autoencoder_loss_val: 5.234045505523682 - discriminator_loss_val: 1.2849080562591553\n",
      "Starting epoch 48/1000\n",
      "    autoencoder_loss_train: 5.234360218048096 - discriminator_loss_train: 1.2669928073883057\n",
      "    autoencoder_loss_val: 5.201614856719971 - discriminator_loss_val: 1.2837016582489014\n",
      "Starting epoch 49/1000\n",
      "    autoencoder_loss_train: 5.202068328857422 - discriminator_loss_train: 1.265830397605896\n",
      "    autoencoder_loss_val: 5.170540809631348 - discriminator_loss_val: 1.2821784019470215\n",
      "Starting epoch 50/1000\n",
      "    autoencoder_loss_train: 5.168055534362793 - discriminator_loss_train: 1.2651960849761963\n",
      "    autoencoder_loss_val: 5.137800216674805 - discriminator_loss_val: 1.2813303470611572\n",
      "Starting epoch 51/1000\n",
      "    autoencoder_loss_train: 5.132971286773682 - discriminator_loss_train: 1.2659971714019775\n",
      "    autoencoder_loss_val: 5.104007720947266 - discriminator_loss_val: 1.2817652225494385\n",
      "Starting epoch 52/1000\n",
      "    autoencoder_loss_train: 5.096430778503418 - discriminator_loss_train: 1.2670025825500488\n",
      "    autoencoder_loss_val: 5.068800449371338 - discriminator_loss_val: 1.2822235822677612\n",
      "Starting epoch 53/1000\n",
      "    autoencoder_loss_train: 5.0586771965026855 - discriminator_loss_train: 1.2663335800170898\n",
      "    autoencoder_loss_val: 5.0323638916015625 - discriminator_loss_val: 1.2811510562896729\n",
      "Starting epoch 54/1000\n",
      "    autoencoder_loss_train: 5.016493320465088 - discriminator_loss_train: 1.2658421993255615\n",
      "    autoencoder_loss_val: 4.99152135848999 - discriminator_loss_val: 1.2800447940826416\n",
      "Starting epoch 55/1000\n",
      "    autoencoder_loss_train: 4.969715595245361 - discriminator_loss_train: 1.2657651901245117\n",
      "    autoencoder_loss_val: 4.946058750152588 - discriminator_loss_val: 1.2794075012207031\n",
      "Starting epoch 56/1000\n",
      "    autoencoder_loss_train: 4.920035362243652 - discriminator_loss_train: 1.2661429643630981\n",
      "    autoencoder_loss_val: 4.897702217102051 - discriminator_loss_val: 1.2793821096420288\n",
      "Starting epoch 57/1000\n",
      "    autoencoder_loss_train: 4.875415802001953 - discriminator_loss_train: 1.2660772800445557\n",
      "    autoencoder_loss_val: 4.854447364807129 - discriminator_loss_val: 1.2790229320526123\n",
      "Starting epoch 58/1000\n",
      "    autoencoder_loss_train: 4.831336498260498 - discriminator_loss_train: 1.2654772996902466\n",
      "    autoencoder_loss_val: 4.811649799346924 - discriminator_loss_val: 1.2781720161437988\n",
      "Starting epoch 59/1000\n",
      "    autoencoder_loss_train: 4.789134502410889 - discriminator_loss_train: 1.2643890380859375\n",
      "    autoencoder_loss_val: 4.770733833312988 - discriminator_loss_val: 1.277058482170105\n",
      "Starting epoch 60/1000\n",
      "    autoencoder_loss_train: 4.747711181640625 - discriminator_loss_train: 1.2640511989593506\n",
      "    autoencoder_loss_val: 4.730656623840332 - discriminator_loss_val: 1.2769863605499268\n",
      "Starting epoch 61/1000\n",
      "    autoencoder_loss_train: 4.70389986038208 - discriminator_loss_train: 1.263502836227417\n",
      "    autoencoder_loss_val: 4.688235282897949 - discriminator_loss_val: 1.2769848108291626\n",
      "Starting epoch 62/1000\n",
      "    autoencoder_loss_train: 4.66843843460083 - discriminator_loss_train: 1.264131784439087\n",
      "    autoencoder_loss_val: 4.654067039489746 - discriminator_loss_val: 1.278376817703247\n",
      "Starting epoch 63/1000\n",
      "    autoencoder_loss_train: 4.6379218101501465 - discriminator_loss_train: 1.2650245428085327\n",
      "    autoencoder_loss_val: 4.624491214752197 - discriminator_loss_val: 1.279942274093628\n",
      "Starting epoch 64/1000\n",
      "    autoencoder_loss_train: 4.602284908294678 - discriminator_loss_train: 1.2640788555145264\n",
      "    autoencoder_loss_val: 4.589691638946533 - discriminator_loss_val: 1.279559850692749\n",
      "Starting epoch 65/1000\n",
      "    autoencoder_loss_train: 4.57204008102417 - discriminator_loss_train: 1.2647535800933838\n",
      "    autoencoder_loss_val: 4.560117244720459 - discriminator_loss_val: 1.280541181564331\n",
      "Starting epoch 66/1000\n",
      "    autoencoder_loss_train: 4.538122177124023 - discriminator_loss_train: 1.265199899673462\n",
      "    autoencoder_loss_val: 4.526856422424316 - discriminator_loss_val: 1.2815377712249756\n",
      "Starting epoch 67/1000\n",
      "    autoencoder_loss_train: 4.500070571899414 - discriminator_loss_train: 1.2657127380371094\n",
      "    autoencoder_loss_val: 4.489480972290039 - discriminator_loss_val: 1.2832870483398438\n",
      "Starting epoch 68/1000\n",
      "    autoencoder_loss_train: 4.455355644226074 - discriminator_loss_train: 1.265254259109497\n",
      "    autoencoder_loss_val: 4.445672035217285 - discriminator_loss_val: 1.2847800254821777\n",
      "Starting epoch 69/1000\n",
      "    autoencoder_loss_train: 4.399043560028076 - discriminator_loss_train: 1.262975811958313\n",
      "    autoencoder_loss_val: 4.390404224395752 - discriminator_loss_val: 1.2846838235855103\n",
      "Starting epoch 70/1000\n",
      "    autoencoder_loss_train: 4.341095447540283 - discriminator_loss_train: 1.2633893489837646\n",
      "    autoencoder_loss_val: 4.333566665649414 - discriminator_loss_val: 1.2871763706207275\n",
      "Starting epoch 71/1000\n",
      "    autoencoder_loss_train: 4.283470630645752 - discriminator_loss_train: 1.263931393623352\n",
      "    autoencoder_loss_val: 4.276945114135742 - discriminator_loss_val: 1.2894784212112427\n",
      "Starting epoch 72/1000\n",
      "    autoencoder_loss_train: 4.236651420593262 - discriminator_loss_train: 1.2658016681671143\n",
      "    autoencoder_loss_val: 4.230862140655518 - discriminator_loss_val: 1.2930383682250977\n",
      "Starting epoch 73/1000\n",
      "    autoencoder_loss_train: 4.193058967590332 - discriminator_loss_train: 1.2672781944274902\n",
      "    autoencoder_loss_val: 4.187666416168213 - discriminator_loss_val: 1.29656183719635\n",
      "Starting epoch 74/1000\n",
      "    autoencoder_loss_train: 4.144118785858154 - discriminator_loss_train: 1.2693758010864258\n",
      "    autoencoder_loss_val: 4.138753414154053 - discriminator_loss_val: 1.300579309463501\n",
      "Starting epoch 75/1000\n",
      "    autoencoder_loss_train: 4.0897440910339355 - discriminator_loss_train: 1.2701342105865479\n",
      "    autoencoder_loss_val: 4.084454536437988 - discriminator_loss_val: 1.3034387826919556\n",
      "Starting epoch 76/1000\n",
      "    autoencoder_loss_train: 4.032516002655029 - discriminator_loss_train: 1.2706141471862793\n",
      "    autoencoder_loss_val: 4.027400016784668 - discriminator_loss_val: 1.306191086769104\n",
      "Starting epoch 77/1000\n",
      "    autoencoder_loss_train: 3.9692811965942383 - discriminator_loss_train: 1.2707911729812622\n",
      "    autoencoder_loss_val: 3.9642200469970703 - discriminator_loss_val: 1.3086624145507812\n",
      "Starting epoch 78/1000\n",
      "    autoencoder_loss_train: 3.913383960723877 - discriminator_loss_train: 1.2684131860733032\n",
      "    autoencoder_loss_val: 3.9084219932556152 - discriminator_loss_val: 1.308492660522461\n",
      "Starting epoch 79/1000\n",
      "    autoencoder_loss_train: 3.8619544506073 - discriminator_loss_train: 1.2633460760116577\n",
      "    autoencoder_loss_val: 3.8569366931915283 - discriminator_loss_val: 1.305795669555664\n",
      "Starting epoch 80/1000\n",
      "    autoencoder_loss_train: 3.8137407302856445 - discriminator_loss_train: 1.2567760944366455\n",
      "    autoencoder_loss_val: 3.8084685802459717 - discriminator_loss_val: 1.3017171621322632\n",
      "Starting epoch 81/1000\n",
      "    autoencoder_loss_train: 3.7646234035491943 - discriminator_loss_train: 1.2489686012268066\n",
      "    autoencoder_loss_val: 3.7592058181762695 - discriminator_loss_val: 1.2965962886810303\n",
      "Starting epoch 82/1000\n",
      "    autoencoder_loss_train: 3.7161388397216797 - discriminator_loss_train: 1.241051435470581\n",
      "    autoencoder_loss_val: 3.7108664512634277 - discriminator_loss_val: 1.2910575866699219\n",
      "Starting epoch 83/1000\n",
      "    autoencoder_loss_train: 3.673337459564209 - discriminator_loss_train: 1.2321592569351196\n",
      "    autoencoder_loss_val: 3.6680264472961426 - discriminator_loss_val: 1.2843809127807617\n",
      "Starting epoch 84/1000\n",
      "    autoencoder_loss_train: 3.629129648208618 - discriminator_loss_train: 1.2227933406829834\n",
      "    autoencoder_loss_val: 3.623944044113159 - discriminator_loss_val: 1.2768635749816895\n",
      "Starting epoch 85/1000\n",
      "    autoencoder_loss_train: 3.5863358974456787 - discriminator_loss_train: 1.21108877658844\n",
      "    autoencoder_loss_val: 3.581491231918335 - discriminator_loss_val: 1.2665600776672363\n",
      "Starting epoch 86/1000\n",
      "    autoencoder_loss_train: 3.540687322616577 - discriminator_loss_train: 1.1988515853881836\n",
      "    autoencoder_loss_val: 3.536208152770996 - discriminator_loss_val: 1.2554128170013428\n",
      "Starting epoch 87/1000\n",
      "    autoencoder_loss_train: 3.4981348514556885 - discriminator_loss_train: 1.1862483024597168\n",
      "    autoencoder_loss_val: 3.4942164421081543 - discriminator_loss_val: 1.2441463470458984\n",
      "Starting epoch 88/1000\n",
      "    autoencoder_loss_train: 3.447474241256714 - discriminator_loss_train: 1.1735422611236572\n",
      "    autoencoder_loss_val: 3.4443817138671875 - discriminator_loss_val: 1.2325305938720703\n",
      "Starting epoch 89/1000\n",
      "    autoencoder_loss_train: 3.3877370357513428 - discriminator_loss_train: 1.1611708402633667\n",
      "    autoencoder_loss_val: 3.3857293128967285 - discriminator_loss_val: 1.2204859256744385\n",
      "Starting epoch 90/1000\n",
      "    autoencoder_loss_train: 3.333796977996826 - discriminator_loss_train: 1.15023672580719\n",
      "    autoencoder_loss_val: 3.332880973815918 - discriminator_loss_val: 1.2098714113235474\n",
      "Starting epoch 91/1000\n",
      "    autoencoder_loss_train: 3.2791218757629395 - discriminator_loss_train: 1.1433179378509521\n",
      "    autoencoder_loss_val: 3.279205560684204 - discriminator_loss_val: 1.2036147117614746\n",
      "Starting epoch 92/1000\n",
      "    autoencoder_loss_train: 3.2228283882141113 - discriminator_loss_train: 1.1395063400268555\n",
      "    autoencoder_loss_val: 3.224198818206787 - discriminator_loss_val: 1.2003047466278076\n",
      "Starting epoch 93/1000\n",
      "    autoencoder_loss_train: 3.1798343658447266 - discriminator_loss_train: 1.1389203071594238\n",
      "    autoencoder_loss_val: 3.1824774742126465 - discriminator_loss_val: 1.2011865377426147\n",
      "Starting epoch 94/1000\n",
      "    autoencoder_loss_train: 3.144455671310425 - discriminator_loss_train: 1.138382911682129\n",
      "    autoencoder_loss_val: 3.1487531661987305 - discriminator_loss_val: 1.2020331621170044\n",
      "Starting epoch 95/1000\n",
      "    autoencoder_loss_train: 3.1206300258636475 - discriminator_loss_train: 1.1392526626586914\n",
      "    autoencoder_loss_val: 3.1263089179992676 - discriminator_loss_val: 1.2050690650939941\n",
      "Starting epoch 96/1000\n",
      "    autoencoder_loss_train: 3.096597194671631 - discriminator_loss_train: 1.1428310871124268\n",
      "    autoencoder_loss_val: 3.103738307952881 - discriminator_loss_val: 1.210798740386963\n",
      "Starting epoch 97/1000\n",
      "    autoencoder_loss_train: 3.0511631965637207 - discriminator_loss_train: 1.1493561267852783\n",
      "    autoencoder_loss_val: 3.0600836277008057 - discriminator_loss_val: 1.2181422710418701\n",
      "Starting epoch 98/1000\n",
      "    autoencoder_loss_train: 3.0033395290374756 - discriminator_loss_train: 1.1560850143432617\n",
      "    autoencoder_loss_val: 3.0137081146240234 - discriminator_loss_val: 1.2255640029907227\n",
      "Starting epoch 99/1000\n",
      "    autoencoder_loss_train: 2.966001272201538 - discriminator_loss_train: 1.1626958847045898\n",
      "    autoencoder_loss_val: 2.9775454998016357 - discriminator_loss_val: 1.2328938245773315\n",
      "Starting epoch 100/1000\n",
      "    autoencoder_loss_train: 2.9502503871917725 - discriminator_loss_train: 1.1686465740203857\n",
      "    autoencoder_loss_val: 2.962596893310547 - discriminator_loss_val: 1.2406158447265625\n",
      "Starting epoch 101/1000\n",
      "    autoencoder_loss_train: 2.945740222930908 - discriminator_loss_train: 1.1757283210754395\n",
      "    autoencoder_loss_val: 2.9586894512176514 - discriminator_loss_val: 1.2498937845230103\n",
      "Starting epoch 102/1000\n",
      "    autoencoder_loss_train: 2.9525201320648193 - discriminator_loss_train: 1.1834204196929932\n",
      "    autoencoder_loss_val: 2.966224193572998 - discriminator_loss_val: 1.2603530883789062\n",
      "Starting epoch 103/1000\n",
      "    autoencoder_loss_train: 2.9726600646972656 - discriminator_loss_train: 1.189296007156372\n",
      "    autoencoder_loss_val: 2.9870924949645996 - discriminator_loss_val: 1.2687592506408691\n",
      "Starting epoch 104/1000\n",
      "    autoencoder_loss_train: 3.0075836181640625 - discriminator_loss_train: 1.1958484649658203\n",
      "    autoencoder_loss_val: 3.0225472450256348 - discriminator_loss_val: 1.2781906127929688\n",
      "Starting epoch 105/1000\n",
      "    autoencoder_loss_train: 3.028972625732422 - discriminator_loss_train: 1.2023231983184814\n",
      "    autoencoder_loss_val: 3.0447838306427 - discriminator_loss_val: 1.2867107391357422\n",
      "Starting epoch 106/1000\n",
      "    autoencoder_loss_train: 3.0420069694519043 - discriminator_loss_train: 1.208970069885254\n",
      "    autoencoder_loss_val: 3.058619737625122 - discriminator_loss_val: 1.2945239543914795\n",
      "Starting epoch 107/1000\n",
      "    autoencoder_loss_train: 3.059062957763672 - discriminator_loss_train: 1.217226266860962\n",
      "    autoencoder_loss_val: 3.075936794281006 - discriminator_loss_val: 1.3039603233337402\n",
      "Starting epoch 108/1000\n",
      "    autoencoder_loss_train: 3.0641090869903564 - discriminator_loss_train: 1.2216298580169678\n",
      "    autoencoder_loss_val: 3.0815811157226562 - discriminator_loss_val: 1.3084865808486938\n",
      "Starting epoch 109/1000\n",
      "    autoencoder_loss_train: 3.0569629669189453 - discriminator_loss_train: 1.2207454442977905\n",
      "    autoencoder_loss_val: 3.0753960609436035 - discriminator_loss_val: 1.3068513870239258\n",
      "Starting epoch 110/1000\n",
      "    autoencoder_loss_train: 3.0428659915924072 - discriminator_loss_train: 1.2165722846984863\n",
      "    autoencoder_loss_val: 3.061941146850586 - discriminator_loss_val: 1.3011780977249146\n",
      "Starting epoch 111/1000\n",
      "    autoencoder_loss_train: 3.0345802307128906 - discriminator_loss_train: 1.2155835628509521\n",
      "    autoencoder_loss_val: 3.0537142753601074 - discriminator_loss_val: 1.2989109754562378\n",
      "Starting epoch 112/1000\n",
      "    autoencoder_loss_train: 3.0403621196746826 - discriminator_loss_train: 1.2163817882537842\n",
      "    autoencoder_loss_val: 3.0592005252838135 - discriminator_loss_val: 1.2989246845245361\n",
      "Starting epoch 113/1000\n",
      "    autoencoder_loss_train: 3.055269241333008 - discriminator_loss_train: 1.2173134088516235\n",
      "    autoencoder_loss_val: 3.072572708129883 - discriminator_loss_val: 1.2991468906402588\n",
      "Starting epoch 114/1000\n",
      "    autoencoder_loss_train: 3.0489323139190674 - discriminator_loss_train: 1.2109524011611938\n",
      "    autoencoder_loss_val: 3.0645594596862793 - discriminator_loss_val: 1.2912676334381104\n",
      "Starting epoch 115/1000\n",
      "    autoencoder_loss_train: 3.0192065238952637 - discriminator_loss_train: 1.1936171054840088\n",
      "    autoencoder_loss_val: 3.034083366394043 - discriminator_loss_val: 1.2715134620666504\n",
      "Starting epoch 116/1000\n",
      "    autoencoder_loss_train: 2.9609692096710205 - discriminator_loss_train: 1.1677935123443604\n",
      "    autoencoder_loss_val: 2.975924491882324 - discriminator_loss_val: 1.2421653270721436\n",
      "Starting epoch 117/1000\n",
      "    autoencoder_loss_train: 2.892332077026367 - discriminator_loss_train: 1.140153169631958\n",
      "    autoencoder_loss_val: 2.9080100059509277 - discriminator_loss_val: 1.210898756980896\n",
      "Starting epoch 118/1000\n",
      "    autoencoder_loss_train: 2.8322744369506836 - discriminator_loss_train: 1.1177092790603638\n",
      "    autoencoder_loss_val: 2.848759889602661 - discriminator_loss_val: 1.1853199005126953\n",
      "Starting epoch 119/1000\n",
      "    autoencoder_loss_train: 2.784576654434204 - discriminator_loss_train: 1.099316954612732\n",
      "    autoencoder_loss_val: 2.8010447025299072 - discriminator_loss_val: 1.1647363901138306\n",
      "Starting epoch 120/1000\n",
      "    autoencoder_loss_train: 2.758927345275879 - discriminator_loss_train: 1.0891005992889404\n",
      "    autoencoder_loss_val: 2.774568557739258 - discriminator_loss_val: 1.1539866924285889\n",
      "Starting epoch 121/1000\n",
      "    autoencoder_loss_train: 2.7372288703918457 - discriminator_loss_train: 1.084516167640686\n",
      "    autoencoder_loss_val: 2.7510740756988525 - discriminator_loss_val: 1.1496176719665527\n",
      "Starting epoch 122/1000\n",
      "    autoencoder_loss_train: 2.7092227935791016 - discriminator_loss_train: 1.0818198919296265\n",
      "    autoencoder_loss_val: 2.7215890884399414 - discriminator_loss_val: 1.1471869945526123\n",
      "Starting epoch 123/1000\n",
      "    autoencoder_loss_train: 2.6291422843933105 - discriminator_loss_train: 1.078011393547058\n",
      "    autoencoder_loss_val: 2.6405134201049805 - discriminator_loss_val: 1.1421658992767334\n",
      "Starting epoch 124/1000\n",
      "    autoencoder_loss_train: 2.5266337394714355 - discriminator_loss_train: 1.077670931816101\n",
      "    autoencoder_loss_val: 2.5373919010162354 - discriminator_loss_val: 1.1399645805358887\n",
      "Starting epoch 125/1000\n",
      "    autoencoder_loss_train: 2.4447269439697266 - discriminator_loss_train: 1.08505380153656\n",
      "    autoencoder_loss_val: 2.4549293518066406 - discriminator_loss_val: 1.1467199325561523\n",
      "Starting epoch 126/1000\n",
      "    autoencoder_loss_train: 2.360701560974121 - discriminator_loss_train: 1.0992310047149658\n",
      "    autoencoder_loss_val: 2.3705482482910156 - discriminator_loss_val: 1.1601781845092773\n",
      "Starting epoch 127/1000\n",
      "    autoencoder_loss_train: 2.2886266708374023 - discriminator_loss_train: 1.1199815273284912\n",
      "    autoencoder_loss_val: 2.298736572265625 - discriminator_loss_val: 1.1809930801391602\n",
      "Starting epoch 128/1000\n",
      "    autoencoder_loss_train: 2.2473387718200684 - discriminator_loss_train: 1.14687979221344\n",
      "    autoencoder_loss_val: 2.2574851512908936 - discriminator_loss_val: 1.209693431854248\n",
      "Starting epoch 129/1000\n",
      "    autoencoder_loss_train: 2.197523832321167 - discriminator_loss_train: 1.1758167743682861\n",
      "    autoencoder_loss_val: 2.2082669734954834 - discriminator_loss_val: 1.2401409149169922\n",
      "Starting epoch 130/1000\n",
      "    autoencoder_loss_train: 2.1375815868377686 - discriminator_loss_train: 1.2078462839126587\n",
      "    autoencoder_loss_val: 2.14918851852417 - discriminator_loss_val: 1.2734873294830322\n",
      "Starting epoch 131/1000\n",
      "    autoencoder_loss_train: 2.08854341506958 - discriminator_loss_train: 1.243115782737732\n",
      "    autoencoder_loss_val: 2.1009695529937744 - discriminator_loss_val: 1.3105539083480835\n",
      "Starting epoch 132/1000\n",
      "    autoencoder_loss_train: 2.051295280456543 - discriminator_loss_train: 1.2780452966690063\n",
      "    autoencoder_loss_val: 2.0646326541900635 - discriminator_loss_val: 1.3470799922943115\n",
      "Starting epoch 133/1000\n",
      "    autoencoder_loss_train: 2.0213096141815186 - discriminator_loss_train: 1.3119064569473267\n",
      "    autoencoder_loss_val: 2.0354883670806885 - discriminator_loss_val: 1.3828340768814087\n",
      "Starting epoch 134/1000\n",
      "    autoencoder_loss_train: 1.996387243270874 - discriminator_loss_train: 1.3441133499145508\n",
      "    autoencoder_loss_val: 2.011823892593384 - discriminator_loss_val: 1.4167653322219849\n",
      "Starting epoch 135/1000\n",
      "    autoencoder_loss_train: 1.9736309051513672 - discriminator_loss_train: 1.372161865234375\n",
      "    autoencoder_loss_val: 1.989797592163086 - discriminator_loss_val: 1.4465079307556152\n",
      "Starting epoch 136/1000\n",
      "    autoencoder_loss_train: 1.947009563446045 - discriminator_loss_train: 1.3914515972137451\n",
      "    autoencoder_loss_val: 1.9631510972976685 - discriminator_loss_val: 1.4668619632720947\n",
      "Starting epoch 137/1000\n",
      "    autoencoder_loss_train: 1.92299222946167 - discriminator_loss_train: 1.401975154876709\n",
      "    autoencoder_loss_val: 1.9385368824005127 - discriminator_loss_val: 1.4782788753509521\n",
      "Starting epoch 138/1000\n",
      "    autoencoder_loss_train: 1.902876853942871 - discriminator_loss_train: 1.4036688804626465\n",
      "    autoencoder_loss_val: 1.918128490447998 - discriminator_loss_val: 1.480440616607666\n",
      "Starting epoch 139/1000\n",
      "    autoencoder_loss_train: 1.8891081809997559 - discriminator_loss_train: 1.3970494270324707\n",
      "    autoencoder_loss_val: 1.9055533409118652 - discriminator_loss_val: 1.4733963012695312\n",
      "Starting epoch 140/1000\n",
      "    autoencoder_loss_train: 1.8899765014648438 - discriminator_loss_train: 1.380173683166504\n",
      "    autoencoder_loss_val: 1.9079217910766602 - discriminator_loss_val: 1.4566335678100586\n",
      "Starting epoch 141/1000\n",
      "    autoencoder_loss_train: 1.914335012435913 - discriminator_loss_train: 1.3549141883850098\n",
      "    autoencoder_loss_val: 1.9341915845870972 - discriminator_loss_val: 1.4328420162200928\n",
      "Starting epoch 142/1000\n",
      "    autoencoder_loss_train: 1.9386849403381348 - discriminator_loss_train: 1.3246456384658813\n",
      "    autoencoder_loss_val: 1.960490345954895 - discriminator_loss_val: 1.4041907787322998\n",
      "Starting epoch 143/1000\n",
      "    autoencoder_loss_train: 1.9455373287200928 - discriminator_loss_train: 1.2854061126708984\n",
      "    autoencoder_loss_val: 1.9691872596740723 - discriminator_loss_val: 1.364625334739685\n",
      "Starting epoch 144/1000\n",
      "    autoencoder_loss_train: 1.941902995109558 - discriminator_loss_train: 1.2425270080566406\n",
      "    autoencoder_loss_val: 1.9678046703338623 - discriminator_loss_val: 1.320589303970337\n",
      "Starting epoch 145/1000\n",
      "    autoencoder_loss_train: 1.9367930889129639 - discriminator_loss_train: 1.2007288932800293\n",
      "    autoencoder_loss_val: 1.9642505645751953 - discriminator_loss_val: 1.2779492139816284\n",
      "Starting epoch 146/1000\n",
      "    autoencoder_loss_train: 1.9334282875061035 - discriminator_loss_train: 1.1612989902496338\n",
      "    autoencoder_loss_val: 1.9622209072113037 - discriminator_loss_val: 1.2383532524108887\n",
      "Starting epoch 147/1000\n",
      "    autoencoder_loss_train: 1.9086370468139648 - discriminator_loss_train: 1.127092719078064\n",
      "    autoencoder_loss_val: 1.9387067556381226 - discriminator_loss_val: 1.2026877403259277\n",
      "Starting epoch 148/1000\n",
      "    autoencoder_loss_train: 1.8784624338150024 - discriminator_loss_train: 1.1040244102478027\n",
      "    autoencoder_loss_val: 1.909398078918457 - discriminator_loss_val: 1.1784592866897583\n",
      "Starting epoch 149/1000\n",
      "    autoencoder_loss_train: 1.838403582572937 - discriminator_loss_train: 1.093870997428894\n",
      "    autoencoder_loss_val: 1.8699910640716553 - discriminator_loss_val: 1.167404055595398\n",
      "Starting epoch 150/1000\n",
      "    autoencoder_loss_train: 1.813178300857544 - discriminator_loss_train: 1.0958969593048096\n",
      "    autoencoder_loss_val: 1.8463256359100342 - discriminator_loss_val: 1.1702048778533936\n",
      "Starting epoch 151/1000\n",
      "    autoencoder_loss_train: 1.793116807937622 - discriminator_loss_train: 1.1075140237808228\n",
      "    autoencoder_loss_val: 1.8281983137130737 - discriminator_loss_val: 1.183349609375\n",
      "Starting epoch 152/1000\n",
      "    autoencoder_loss_train: 1.7919483184814453 - discriminator_loss_train: 1.1242835521697998\n",
      "    autoencoder_loss_val: 1.8280389308929443 - discriminator_loss_val: 1.203382968902588\n",
      "Starting epoch 153/1000\n",
      "    autoencoder_loss_train: 1.798050045967102 - discriminator_loss_train: 1.1468098163604736\n",
      "    autoencoder_loss_val: 1.835038661956787 - discriminator_loss_val: 1.2303252220153809\n",
      "Starting epoch 154/1000\n",
      "    autoencoder_loss_train: 1.7999869585037231 - discriminator_loss_train: 1.1748228073120117\n",
      "    autoencoder_loss_val: 1.839404582977295 - discriminator_loss_val: 1.2622342109680176\n",
      "Starting epoch 155/1000\n",
      "    autoencoder_loss_train: 1.7918717861175537 - discriminator_loss_train: 1.2038757801055908\n",
      "    autoencoder_loss_val: 1.832897424697876 - discriminator_loss_val: 1.2942297458648682\n",
      "Starting epoch 156/1000\n",
      "    autoencoder_loss_train: 1.7824673652648926 - discriminator_loss_train: 1.2312703132629395\n",
      "    autoencoder_loss_val: 1.8253545761108398 - discriminator_loss_val: 1.3235074281692505\n",
      "Starting epoch 157/1000\n",
      "    autoencoder_loss_train: 1.778037667274475 - discriminator_loss_train: 1.257131814956665\n",
      "    autoencoder_loss_val: 1.822995901107788 - discriminator_loss_val: 1.350501298904419\n",
      "Starting epoch 158/1000\n",
      "    autoencoder_loss_train: 1.7930490970611572 - discriminator_loss_train: 1.2777166366577148\n",
      "    autoencoder_loss_val: 1.8394315242767334 - discriminator_loss_val: 1.373342752456665\n",
      "Starting epoch 159/1000\n",
      "    autoencoder_loss_train: 1.810654640197754 - discriminator_loss_train: 1.2910213470458984\n",
      "    autoencoder_loss_val: 1.8574340343475342 - discriminator_loss_val: 1.3887808322906494\n",
      "Starting epoch 160/1000\n",
      "    autoencoder_loss_train: 1.80855131149292 - discriminator_loss_train: 1.2984294891357422\n",
      "    autoencoder_loss_val: 1.8554131984710693 - discriminator_loss_val: 1.396266222000122\n",
      "Starting epoch 161/1000\n",
      "    autoencoder_loss_train: 1.7846031188964844 - discriminator_loss_train: 1.3002798557281494\n",
      "    autoencoder_loss_val: 1.830866813659668 - discriminator_loss_val: 1.3958028554916382\n",
      "Starting epoch 162/1000\n",
      "    autoencoder_loss_train: 1.7683273553848267 - discriminator_loss_train: 1.3024404048919678\n",
      "    autoencoder_loss_val: 1.8140788078308105 - discriminator_loss_val: 1.3966931104660034\n",
      "Starting epoch 163/1000\n",
      "    autoencoder_loss_train: 1.7362706661224365 - discriminator_loss_train: 1.3083257675170898\n",
      "    autoencoder_loss_val: 1.782217264175415 - discriminator_loss_val: 1.3995555639266968\n",
      "Starting epoch 164/1000\n",
      "    autoencoder_loss_train: 1.7034695148468018 - discriminator_loss_train: 1.3134727478027344\n",
      "    autoencoder_loss_val: 1.7494382858276367 - discriminator_loss_val: 1.401334524154663\n",
      "Starting epoch 165/1000\n",
      "    autoencoder_loss_train: 1.6788437366485596 - discriminator_loss_train: 1.3175098896026611\n",
      "    autoencoder_loss_val: 1.724670171737671 - discriminator_loss_val: 1.402669906616211\n",
      "Starting epoch 166/1000\n",
      "    autoencoder_loss_train: 1.660405158996582 - discriminator_loss_train: 1.3214893341064453\n",
      "    autoencoder_loss_val: 1.7062327861785889 - discriminator_loss_val: 1.4041533470153809\n",
      "Starting epoch 167/1000\n",
      "    autoencoder_loss_train: 1.6308270692825317 - discriminator_loss_train: 1.3244884014129639\n",
      "    autoencoder_loss_val: 1.6763869524002075 - discriminator_loss_val: 1.403576374053955\n",
      "Starting epoch 168/1000\n",
      "    autoencoder_loss_train: 1.6098649501800537 - discriminator_loss_train: 1.324617862701416\n",
      "    autoencoder_loss_val: 1.6554360389709473 - discriminator_loss_val: 1.4007327556610107\n",
      "Starting epoch 169/1000\n",
      "    autoencoder_loss_train: 1.6035168170928955 - discriminator_loss_train: 1.3187284469604492\n",
      "    autoencoder_loss_val: 1.6491211652755737 - discriminator_loss_val: 1.394040584564209\n",
      "Starting epoch 170/1000\n",
      "    autoencoder_loss_train: 1.5928351879119873 - discriminator_loss_train: 1.318871021270752\n",
      "    autoencoder_loss_val: 1.6394927501678467 - discriminator_loss_val: 1.391874074935913\n",
      "Starting epoch 171/1000\n",
      "    autoencoder_loss_train: 1.5951910018920898 - discriminator_loss_train: 1.3135493993759155\n",
      "    autoencoder_loss_val: 1.6426732540130615 - discriminator_loss_val: 1.386138677597046\n",
      "Starting epoch 172/1000\n",
      "    autoencoder_loss_train: 1.5933506488800049 - discriminator_loss_train: 1.3097341060638428\n",
      "    autoencoder_loss_val: 1.6418142318725586 - discriminator_loss_val: 1.3813179731369019\n",
      "Starting epoch 173/1000\n",
      "    autoencoder_loss_train: 1.5738720893859863 - discriminator_loss_train: 1.3116832971572876\n",
      "    autoencoder_loss_val: 1.6233328580856323 - discriminator_loss_val: 1.379007339477539\n",
      "Starting epoch 174/1000\n",
      "    autoencoder_loss_train: 1.5525749921798706 - discriminator_loss_train: 1.318395972251892\n",
      "    autoencoder_loss_val: 1.6028931140899658 - discriminator_loss_val: 1.3804407119750977\n",
      "Starting epoch 175/1000\n",
      "    autoencoder_loss_train: 1.5274765491485596 - discriminator_loss_train: 1.3267185688018799\n",
      "    autoencoder_loss_val: 1.5776786804199219 - discriminator_loss_val: 1.3837072849273682\n",
      "Starting epoch 176/1000\n",
      "    autoencoder_loss_train: 1.5081830024719238 - discriminator_loss_train: 1.3328018188476562\n",
      "    autoencoder_loss_val: 1.5580629110336304 - discriminator_loss_val: 1.3856914043426514\n",
      "Starting epoch 177/1000\n",
      "    autoencoder_loss_train: 1.501988172531128 - discriminator_loss_train: 1.3382301330566406\n",
      "    autoencoder_loss_val: 1.5525343418121338 - discriminator_loss_val: 1.3892658948898315\n",
      "Starting epoch 178/1000\n",
      "    autoencoder_loss_train: 1.5060219764709473 - discriminator_loss_train: 1.3368277549743652\n",
      "    autoencoder_loss_val: 1.557469367980957 - discriminator_loss_val: 1.3888463973999023\n",
      "Starting epoch 179/1000\n",
      "    autoencoder_loss_train: 1.5183842182159424 - discriminator_loss_train: 1.3335275650024414\n",
      "    autoencoder_loss_val: 1.5703797340393066 - discriminator_loss_val: 1.3882304430007935\n",
      "Starting epoch 180/1000\n",
      "    autoencoder_loss_train: 1.5229926109313965 - discriminator_loss_train: 1.3381620645523071\n",
      "    autoencoder_loss_val: 1.575083613395691 - discriminator_loss_val: 1.3942785263061523\n",
      "Starting epoch 181/1000\n",
      "    autoencoder_loss_train: 1.507502794265747 - discriminator_loss_train: 1.3554890155792236\n",
      "    autoencoder_loss_val: 1.559142827987671 - discriminator_loss_val: 1.409721851348877\n",
      "Starting epoch 182/1000\n",
      "    autoencoder_loss_train: 1.4897377490997314 - discriminator_loss_train: 1.3761780261993408\n",
      "    autoencoder_loss_val: 1.5409159660339355 - discriminator_loss_val: 1.4276325702667236\n",
      "Starting epoch 183/1000\n",
      "    autoencoder_loss_train: 1.473845362663269 - discriminator_loss_train: 1.395228385925293\n",
      "    autoencoder_loss_val: 1.5241379737854004 - discriminator_loss_val: 1.4443310499191284\n",
      "Starting epoch 184/1000\n",
      "    autoencoder_loss_train: 1.4608628749847412 - discriminator_loss_train: 1.4134782552719116\n",
      "    autoencoder_loss_val: 1.5101076364517212 - discriminator_loss_val: 1.460626482963562\n",
      "Starting epoch 185/1000\n",
      "    autoencoder_loss_train: 1.45924711227417 - discriminator_loss_train: 1.4274978637695312\n",
      "    autoencoder_loss_val: 1.5078407526016235 - discriminator_loss_val: 1.4745819568634033\n",
      "Starting epoch 186/1000\n",
      "    autoencoder_loss_train: 1.4718083143234253 - discriminator_loss_train: 1.427198052406311\n",
      "    autoencoder_loss_val: 1.5196940898895264 - discriminator_loss_val: 1.476334810256958\n",
      "Starting epoch 187/1000\n",
      "    autoencoder_loss_train: 1.4905993938446045 - discriminator_loss_train: 1.4230585098266602\n",
      "    autoencoder_loss_val: 1.5378823280334473 - discriminator_loss_val: 1.474675178527832\n",
      "Starting epoch 188/1000\n",
      "    autoencoder_loss_train: 1.5050427913665771 - discriminator_loss_train: 1.4179409742355347\n",
      "    autoencoder_loss_val: 1.5514400005340576 - discriminator_loss_val: 1.4714604616165161\n",
      "Starting epoch 189/1000\n",
      "    autoencoder_loss_train: 1.5200378894805908 - discriminator_loss_train: 1.4075052738189697\n",
      "    autoencoder_loss_val: 1.564814567565918 - discriminator_loss_val: 1.4629275798797607\n",
      "Starting epoch 190/1000\n",
      "    autoencoder_loss_train: 1.5316636562347412 - discriminator_loss_train: 1.4019129276275635\n",
      "    autoencoder_loss_val: 1.5747075080871582 - discriminator_loss_val: 1.4585096836090088\n",
      "Starting epoch 191/1000\n",
      "    autoencoder_loss_train: 1.5316544771194458 - discriminator_loss_train: 1.4013006687164307\n",
      "    autoencoder_loss_val: 1.5727627277374268 - discriminator_loss_val: 1.4581375122070312\n",
      "Starting epoch 192/1000\n",
      "    autoencoder_loss_train: 1.520693063735962 - discriminator_loss_train: 1.407667875289917\n",
      "    autoencoder_loss_val: 1.560031533241272 - discriminator_loss_val: 1.4636343717575073\n",
      "Starting epoch 193/1000\n",
      "    autoencoder_loss_train: 1.5177701711654663 - discriminator_loss_train: 1.4136836528778076\n",
      "    autoencoder_loss_val: 1.5548902750015259 - discriminator_loss_val: 1.4697827100753784\n",
      "Starting epoch 194/1000\n",
      "    autoencoder_loss_train: 1.5275208950042725 - discriminator_loss_train: 1.412651777267456\n",
      "    autoencoder_loss_val: 1.562532901763916 - discriminator_loss_val: 1.4701108932495117\n",
      "Starting epoch 195/1000\n",
      "    autoencoder_loss_train: 1.536651372909546 - discriminator_loss_train: 1.4134867191314697\n",
      "    autoencoder_loss_val: 1.5701967477798462 - discriminator_loss_val: 1.472784161567688\n",
      "Starting epoch 196/1000\n",
      "    autoencoder_loss_train: 1.541380763053894 - discriminator_loss_train: 1.4206739664077759\n",
      "    autoencoder_loss_val: 1.57386314868927 - discriminator_loss_val: 1.4822676181793213\n",
      "Starting epoch 197/1000\n",
      "    autoencoder_loss_train: 1.551584005355835 - discriminator_loss_train: 1.428274393081665\n",
      "    autoencoder_loss_val: 1.5832529067993164 - discriminator_loss_val: 1.4929872751235962\n",
      "Starting epoch 198/1000\n",
      "    autoencoder_loss_train: 1.5574767589569092 - discriminator_loss_train: 1.4374363422393799\n",
      "    autoencoder_loss_val: 1.5881092548370361 - discriminator_loss_val: 1.504984974861145\n",
      "Starting epoch 199/1000\n",
      "    autoencoder_loss_train: 1.5504239797592163 - discriminator_loss_train: 1.4486067295074463\n",
      "    autoencoder_loss_val: 1.5797808170318604 - discriminator_loss_val: 1.518423080444336\n",
      "Starting epoch 200/1000\n",
      "    autoencoder_loss_train: 1.542272686958313 - discriminator_loss_train: 1.4593567848205566\n",
      "    autoencoder_loss_val: 1.5702500343322754 - discriminator_loss_val: 1.5311144590377808\n",
      "Starting epoch 201/1000\n",
      "    autoencoder_loss_train: 1.5243110656738281 - discriminator_loss_train: 1.4742510318756104\n",
      "    autoencoder_loss_val: 1.551306128501892 - discriminator_loss_val: 1.5468542575836182\n",
      "Starting epoch 202/1000\n",
      "    autoencoder_loss_train: 1.5081756114959717 - discriminator_loss_train: 1.489235520362854\n",
      "    autoencoder_loss_val: 1.5340139865875244 - discriminator_loss_val: 1.562809944152832\n",
      "Starting epoch 203/1000\n",
      "    autoencoder_loss_train: 1.4828054904937744 - discriminator_loss_train: 1.5066571235656738\n",
      "    autoencoder_loss_val: 1.5076978206634521 - discriminator_loss_val: 1.579867959022522\n",
      "Starting epoch 204/1000\n",
      "    autoencoder_loss_train: 1.454312801361084 - discriminator_loss_train: 1.5249648094177246\n",
      "    autoencoder_loss_val: 1.478363037109375 - discriminator_loss_val: 1.5968017578125\n",
      "Starting epoch 205/1000\n",
      "    autoencoder_loss_train: 1.4391918182373047 - discriminator_loss_train: 1.5345088243484497\n",
      "    autoencoder_loss_val: 1.4619131088256836 - discriminator_loss_val: 1.6056504249572754\n",
      "Starting epoch 206/1000\n",
      "    autoencoder_loss_train: 1.4414637088775635 - discriminator_loss_train: 1.5323126316070557\n",
      "    autoencoder_loss_val: 1.462508201599121 - discriminator_loss_val: 1.6038236618041992\n",
      "Starting epoch 207/1000\n",
      "    autoencoder_loss_train: 1.4448819160461426 - discriminator_loss_train: 1.526219367980957\n",
      "    autoencoder_loss_val: 1.4648387432098389 - discriminator_loss_val: 1.5977466106414795\n",
      "Starting epoch 208/1000\n",
      "    autoencoder_loss_train: 1.4551525115966797 - discriminator_loss_train: 1.5150420665740967\n",
      "    autoencoder_loss_val: 1.4742542505264282 - discriminator_loss_val: 1.5871237516403198\n",
      "Starting epoch 209/1000\n",
      "    autoencoder_loss_train: 1.4759135246276855 - discriminator_loss_train: 1.4988090991973877\n",
      "    autoencoder_loss_val: 1.4950140714645386 - discriminator_loss_val: 1.5723949670791626\n",
      "Starting epoch 210/1000\n",
      "    autoencoder_loss_train: 1.4917210340499878 - discriminator_loss_train: 1.4777336120605469\n",
      "    autoencoder_loss_val: 1.510746955871582 - discriminator_loss_val: 1.5520155429840088\n",
      "Starting epoch 211/1000\n",
      "    autoencoder_loss_train: 1.498409390449524 - discriminator_loss_train: 1.4535319805145264\n",
      "    autoencoder_loss_val: 1.5172390937805176 - discriminator_loss_val: 1.5277973413467407\n",
      "Starting epoch 212/1000\n",
      "    autoencoder_loss_train: 1.5075125694274902 - discriminator_loss_train: 1.4231677055358887\n",
      "    autoencoder_loss_val: 1.525756597518921 - discriminator_loss_val: 1.4971749782562256\n",
      "Starting epoch 213/1000\n",
      "    autoencoder_loss_train: 1.531450629234314 - discriminator_loss_train: 1.3892905712127686\n",
      "    autoencoder_loss_val: 1.549033284187317 - discriminator_loss_val: 1.463714361190796\n",
      "Starting epoch 214/1000\n",
      "    autoencoder_loss_train: 1.5503969192504883 - discriminator_loss_train: 1.3540539741516113\n",
      "    autoencoder_loss_val: 1.567535400390625 - discriminator_loss_val: 1.4283883571624756\n",
      "Starting epoch 215/1000\n",
      "    autoencoder_loss_train: 1.55983567237854 - discriminator_loss_train: 1.3179175853729248\n",
      "    autoencoder_loss_val: 1.577410101890564 - discriminator_loss_val: 1.3921282291412354\n",
      "Starting epoch 216/1000\n",
      "    autoencoder_loss_train: 1.5544226169586182 - discriminator_loss_train: 1.282494306564331\n",
      "    autoencoder_loss_val: 1.573401927947998 - discriminator_loss_val: 1.3557612895965576\n",
      "Starting epoch 217/1000\n",
      "    autoencoder_loss_train: 1.554213523864746 - discriminator_loss_train: 1.250377893447876\n",
      "    autoencoder_loss_val: 1.5749526023864746 - discriminator_loss_val: 1.3230772018432617\n",
      "Starting epoch 218/1000\n",
      "    autoencoder_loss_train: 1.5715909004211426 - discriminator_loss_train: 1.2219123840332031\n",
      "    autoencoder_loss_val: 1.5933539867401123 - discriminator_loss_val: 1.2952325344085693\n",
      "Starting epoch 219/1000\n",
      "    autoencoder_loss_train: 1.595076322555542 - discriminator_loss_train: 1.1972172260284424\n",
      "    autoencoder_loss_val: 1.6178648471832275 - discriminator_loss_val: 1.271750569343567\n",
      "Starting epoch 220/1000\n",
      "    autoencoder_loss_train: 1.6103909015655518 - discriminator_loss_train: 1.178635597229004\n",
      "    autoencoder_loss_val: 1.6350460052490234 - discriminator_loss_val: 1.254150152206421\n",
      "Starting epoch 221/1000\n",
      "    autoencoder_loss_train: 1.6110479831695557 - discriminator_loss_train: 1.1648948192596436\n",
      "    autoencoder_loss_val: 1.6380208730697632 - discriminator_loss_val: 1.2407063245773315\n",
      "Starting epoch 222/1000\n",
      "    autoencoder_loss_train: 1.6081405878067017 - discriminator_loss_train: 1.1573420763015747\n",
      "    autoencoder_loss_val: 1.637825846672058 - discriminator_loss_val: 1.2335865497589111\n",
      "Starting epoch 223/1000\n",
      "    autoencoder_loss_train: 1.5934627056121826 - discriminator_loss_train: 1.156299352645874\n",
      "    autoencoder_loss_val: 1.6260700225830078 - discriminator_loss_val: 1.23284912109375\n",
      "Starting epoch 224/1000\n",
      "    autoencoder_loss_train: 1.5759328603744507 - discriminator_loss_train: 1.161635398864746\n",
      "    autoencoder_loss_val: 1.6113944053649902 - discriminator_loss_val: 1.2386735677719116\n",
      "Starting epoch 225/1000\n",
      "    autoencoder_loss_train: 1.5596990585327148 - discriminator_loss_train: 1.171112060546875\n",
      "    autoencoder_loss_val: 1.597702980041504 - discriminator_loss_val: 1.2487521171569824\n",
      "Starting epoch 226/1000\n",
      "    autoencoder_loss_train: 1.5683002471923828 - discriminator_loss_train: 1.1840074062347412\n",
      "    autoencoder_loss_val: 1.6085950136184692 - discriminator_loss_val: 1.2641379833221436\n",
      "Starting epoch 227/1000\n",
      "    autoencoder_loss_train: 1.5745999813079834 - discriminator_loss_train: 1.2016687393188477\n",
      "    autoencoder_loss_val: 1.6163673400878906 - discriminator_loss_val: 1.2841039896011353\n",
      "Starting epoch 228/1000\n",
      "    autoencoder_loss_train: 1.5588173866271973 - discriminator_loss_train: 1.2215726375579834\n",
      "    autoencoder_loss_val: 1.6020829677581787 - discriminator_loss_val: 1.305267095565796\n",
      "Starting epoch 229/1000\n",
      "    autoencoder_loss_train: 1.5396578311920166 - discriminator_loss_train: 1.2434463500976562\n",
      "    autoencoder_loss_val: 1.5846306085586548 - discriminator_loss_val: 1.3281075954437256\n",
      "Starting epoch 230/1000\n",
      "    autoencoder_loss_train: 1.5265047550201416 - discriminator_loss_train: 1.2710388898849487\n",
      "    autoencoder_loss_val: 1.5744168758392334 - discriminator_loss_val: 1.3572359085083008\n",
      "Starting epoch 231/1000\n",
      "    autoencoder_loss_train: 1.5280680656433105 - discriminator_loss_train: 1.2988979816436768\n",
      "    autoencoder_loss_val: 1.5783330202102661 - discriminator_loss_val: 1.387566089630127\n",
      "Starting epoch 232/1000\n",
      "    autoencoder_loss_train: 1.540174961090088 - discriminator_loss_train: 1.3268024921417236\n",
      "    autoencoder_loss_val: 1.5923683643341064 - discriminator_loss_val: 1.4183056354522705\n",
      "Starting epoch 233/1000\n",
      "    autoencoder_loss_train: 1.5556411743164062 - discriminator_loss_train: 1.3522758483886719\n",
      "    autoencoder_loss_val: 1.6102941036224365 - discriminator_loss_val: 1.4464032649993896\n",
      "Starting epoch 234/1000\n",
      "    autoencoder_loss_train: 1.573400616645813 - discriminator_loss_train: 1.3723886013031006\n",
      "    autoencoder_loss_val: 1.630159854888916 - discriminator_loss_val: 1.468871831893921\n",
      "Starting epoch 235/1000\n",
      "    autoencoder_loss_train: 1.576324701309204 - discriminator_loss_train: 1.3859508037567139\n",
      "    autoencoder_loss_val: 1.6344852447509766 - discriminator_loss_val: 1.483285665512085\n",
      "Starting epoch 236/1000\n",
      "    autoencoder_loss_train: 1.5689799785614014 - discriminator_loss_train: 1.3933837413787842\n",
      "    autoencoder_loss_val: 1.6285072565078735 - discriminator_loss_val: 1.490421175956726\n",
      "Starting epoch 237/1000\n",
      "    autoencoder_loss_train: 1.5549612045288086 - discriminator_loss_train: 1.3960826396942139\n",
      "    autoencoder_loss_val: 1.615578532218933 - discriminator_loss_val: 1.4918534755706787\n",
      "Starting epoch 238/1000\n",
      "    autoencoder_loss_train: 1.5356523990631104 - discriminator_loss_train: 1.394058108329773\n",
      "    autoencoder_loss_val: 1.59695303440094 - discriminator_loss_val: 1.4882221221923828\n",
      "Starting epoch 239/1000\n",
      "    autoencoder_loss_train: 1.5129492282867432 - discriminator_loss_train: 1.3886797428131104\n",
      "    autoencoder_loss_val: 1.5750463008880615 - discriminator_loss_val: 1.480459213256836\n",
      "Starting epoch 240/1000\n",
      "    autoencoder_loss_train: 1.5065243244171143 - discriminator_loss_train: 1.3844575881958008\n",
      "    autoencoder_loss_val: 1.570044994354248 - discriminator_loss_val: 1.4759278297424316\n",
      "Starting epoch 241/1000\n",
      "    autoencoder_loss_train: 1.5050381422042847 - discriminator_loss_train: 1.379157304763794\n",
      "    autoencoder_loss_val: 1.570124864578247 - discriminator_loss_val: 1.4708831310272217\n",
      "Starting epoch 242/1000\n",
      "    autoencoder_loss_train: 1.5226765871047974 - discriminator_loss_train: 1.3768978118896484\n",
      "    autoencoder_loss_val: 1.5905474424362183 - discriminator_loss_val: 1.4712902307510376\n",
      "Starting epoch 243/1000\n",
      "    autoencoder_loss_train: 1.5469200611114502 - discriminator_loss_train: 1.3763189315795898\n",
      "    autoencoder_loss_val: 1.6174805164337158 - discriminator_loss_val: 1.4739632606506348\n",
      "Starting epoch 244/1000\n",
      "    autoencoder_loss_train: 1.5635638236999512 - discriminator_loss_train: 1.3736376762390137\n",
      "    autoencoder_loss_val: 1.6358137130737305 - discriminator_loss_val: 1.4735064506530762\n",
      "Starting epoch 245/1000\n",
      "    autoencoder_loss_train: 1.5802595615386963 - discriminator_loss_train: 1.3710331916809082\n",
      "    autoencoder_loss_val: 1.6535896062850952 - discriminator_loss_val: 1.473066806793213\n",
      "Starting epoch 246/1000\n",
      "    autoencoder_loss_train: 1.591956615447998 - discriminator_loss_train: 1.370471477508545\n",
      "    autoencoder_loss_val: 1.665541648864746 - discriminator_loss_val: 1.4744234085083008\n",
      "Starting epoch 247/1000\n",
      "    autoencoder_loss_train: 1.5826733112335205 - discriminator_loss_train: 1.369131088256836\n",
      "    autoencoder_loss_val: 1.655236005783081 - discriminator_loss_val: 1.4725902080535889\n",
      "Starting epoch 248/1000\n",
      "    autoencoder_loss_train: 1.5554587841033936 - discriminator_loss_train: 1.3682454824447632\n",
      "    autoencoder_loss_val: 1.6261796951293945 - discriminator_loss_val: 1.4688143730163574\n",
      "Starting epoch 249/1000\n",
      "    autoencoder_loss_train: 1.5244150161743164 - discriminator_loss_train: 1.3696670532226562\n",
      "    autoencoder_loss_val: 1.5930354595184326 - discriminator_loss_val: 1.4659948348999023\n",
      "Starting epoch 250/1000\n",
      "    autoencoder_loss_train: 1.5069024562835693 - discriminator_loss_train: 1.3709416389465332\n",
      "    autoencoder_loss_val: 1.57362699508667 - discriminator_loss_val: 1.4630470275878906\n",
      "Starting epoch 251/1000\n",
      "    autoencoder_loss_train: 1.5184283256530762 - discriminator_loss_train: 1.371238350868225\n",
      "    autoencoder_loss_val: 1.5835773944854736 - discriminator_loss_val: 1.4624674320220947\n",
      "Starting epoch 252/1000\n",
      "    autoencoder_loss_train: 1.5482534170150757 - discriminator_loss_train: 1.371910572052002\n",
      "    autoencoder_loss_val: 1.6122304201126099 - discriminator_loss_val: 1.4646668434143066\n",
      "Starting epoch 253/1000\n",
      "    autoencoder_loss_train: 1.5778623819351196 - discriminator_loss_train: 1.3725411891937256\n",
      "    autoencoder_loss_val: 1.6402912139892578 - discriminator_loss_val: 1.4668519496917725\n",
      "Starting epoch 254/1000\n",
      "    autoencoder_loss_train: 1.6026252508163452 - discriminator_loss_train: 1.3737690448760986\n",
      "    autoencoder_loss_val: 1.6626536846160889 - discriminator_loss_val: 1.4695565700531006\n",
      "Starting epoch 255/1000\n",
      "    autoencoder_loss_train: 1.6144070625305176 - discriminator_loss_train: 1.3777040243148804\n",
      "    autoencoder_loss_val: 1.6715391874313354 - discriminator_loss_val: 1.4737955331802368\n",
      "Starting epoch 256/1000\n",
      "    autoencoder_loss_train: 1.6227519512176514 - discriminator_loss_train: 1.3809304237365723\n",
      "    autoencoder_loss_val: 1.677311658859253 - discriminator_loss_val: 1.4768174886703491\n",
      "Starting epoch 257/1000\n",
      "    autoencoder_loss_train: 1.627012014389038 - discriminator_loss_train: 1.3815048933029175\n",
      "    autoencoder_loss_val: 1.6795475482940674 - discriminator_loss_val: 1.476264476776123\n",
      "Starting epoch 258/1000\n",
      "    autoencoder_loss_train: 1.6218814849853516 - discriminator_loss_train: 1.3784306049346924\n",
      "    autoencoder_loss_val: 1.6725406646728516 - discriminator_loss_val: 1.4705626964569092\n",
      "Starting epoch 259/1000\n",
      "    autoencoder_loss_train: 1.6124160289764404 - discriminator_loss_train: 1.373544692993164\n",
      "    autoencoder_loss_val: 1.6605908870697021 - discriminator_loss_val: 1.4634532928466797\n",
      "Starting epoch 260/1000\n",
      "    autoencoder_loss_train: 1.6115617752075195 - discriminator_loss_train: 1.3660705089569092\n",
      "    autoencoder_loss_val: 1.657240867614746 - discriminator_loss_val: 1.4551618099212646\n",
      "Starting epoch 261/1000\n",
      "    autoencoder_loss_train: 1.6158998012542725 - discriminator_loss_train: 1.3565568923950195\n",
      "    autoencoder_loss_val: 1.6594462394714355 - discriminator_loss_val: 1.4450759887695312\n",
      "Starting epoch 262/1000\n",
      "    autoencoder_loss_train: 1.6152806282043457 - discriminator_loss_train: 1.3454749584197998\n",
      "    autoencoder_loss_val: 1.6569092273712158 - discriminator_loss_val: 1.4331295490264893\n",
      "Starting epoch 263/1000\n",
      "    autoencoder_loss_train: 1.610168218612671 - discriminator_loss_train: 1.3332089185714722\n",
      "    autoencoder_loss_val: 1.6504051685333252 - discriminator_loss_val: 1.419621467590332\n",
      "Starting epoch 264/1000\n",
      "    autoencoder_loss_train: 1.6063083410263062 - discriminator_loss_train: 1.3185513019561768\n",
      "    autoencoder_loss_val: 1.6451845169067383 - discriminator_loss_val: 1.4040439128875732\n",
      "Starting epoch 265/1000\n",
      "    autoencoder_loss_train: 1.614854335784912 - discriminator_loss_train: 1.3032211065292358\n",
      "    autoencoder_loss_val: 1.6529183387756348 - discriminator_loss_val: 1.3893139362335205\n",
      "Starting epoch 266/1000\n",
      "    autoencoder_loss_train: 1.626744270324707 - discriminator_loss_train: 1.2897140979766846\n",
      "    autoencoder_loss_val: 1.6640384197235107 - discriminator_loss_val: 1.3773069381713867\n",
      "Starting epoch 267/1000\n",
      "    autoencoder_loss_train: 1.636966586112976 - discriminator_loss_train: 1.2762794494628906\n",
      "    autoencoder_loss_val: 1.6742322444915771 - discriminator_loss_val: 1.365020513534546\n",
      "Starting epoch 268/1000\n",
      "    autoencoder_loss_train: 1.6339880228042603 - discriminator_loss_train: 1.2601202726364136\n",
      "    autoencoder_loss_val: 1.6717770099639893 - discriminator_loss_val: 1.3489105701446533\n",
      "Starting epoch 269/1000\n",
      "    autoencoder_loss_train: 1.6217541694641113 - discriminator_loss_train: 1.242943286895752\n",
      "    autoencoder_loss_val: 1.6610164642333984 - discriminator_loss_val: 1.330723762512207\n",
      "Starting epoch 270/1000\n",
      "    autoencoder_loss_train: 1.5991673469543457 - discriminator_loss_train: 1.2303818464279175\n",
      "    autoencoder_loss_val: 1.6404953002929688 - discriminator_loss_val: 1.316105842590332\n",
      "Starting epoch 271/1000\n",
      "    autoencoder_loss_train: 1.5713931322097778 - discriminator_loss_train: 1.2240033149719238\n",
      "    autoencoder_loss_val: 1.6151587963104248 - discriminator_loss_val: 1.3074398040771484\n",
      "Starting epoch 272/1000\n",
      "    autoencoder_loss_train: 1.5356392860412598 - discriminator_loss_train: 1.2235380411148071\n",
      "    autoencoder_loss_val: 1.5816826820373535 - discriminator_loss_val: 1.3042497634887695\n",
      "Starting epoch 273/1000\n",
      "    autoencoder_loss_train: 1.5081658363342285 - discriminator_loss_train: 1.2273199558258057\n",
      "    autoencoder_loss_val: 1.5568621158599854 - discriminator_loss_val: 1.3061566352844238\n",
      "Starting epoch 274/1000\n",
      "    autoencoder_loss_train: 1.491014003753662 - discriminator_loss_train: 1.234447717666626\n",
      "    autoencoder_loss_val: 1.5422580242156982 - discriminator_loss_val: 1.3123958110809326\n",
      "Starting epoch 275/1000\n",
      "    autoencoder_loss_train: 1.4903913736343384 - discriminator_loss_train: 1.2432760000228882\n",
      "    autoencoder_loss_val: 1.5434529781341553 - discriminator_loss_val: 1.3223292827606201\n",
      "Starting epoch 276/1000\n",
      "    autoencoder_loss_train: 1.5160181522369385 - discriminator_loss_train: 1.2563257217407227\n",
      "    autoencoder_loss_val: 1.5704143047332764 - discriminator_loss_val: 1.3390319347381592\n",
      "Starting epoch 277/1000\n",
      "    autoencoder_loss_train: 1.5334298610687256 - discriminator_loss_train: 1.276041030883789\n",
      "    autoencoder_loss_val: 1.5899088382720947 - discriminator_loss_val: 1.3617583513259888\n",
      "Starting epoch 278/1000\n",
      "    autoencoder_loss_train: 1.5262233018875122 - discriminator_loss_train: 1.2992618083953857\n",
      "    autoencoder_loss_val: 1.5851318836212158 - discriminator_loss_val: 1.3861427307128906\n",
      "Starting epoch 279/1000\n",
      "    autoencoder_loss_train: 1.5060129165649414 - discriminator_loss_train: 1.32625150680542\n",
      "    autoencoder_loss_val: 1.5666301250457764 - discriminator_loss_val: 1.4132072925567627\n",
      "Starting epoch 280/1000\n",
      "    autoencoder_loss_train: 1.4739148616790771 - discriminator_loss_train: 1.3512582778930664\n",
      "    autoencoder_loss_val: 1.5351734161376953 - discriminator_loss_val: 1.436518907546997\n",
      "Starting epoch 281/1000\n",
      "    autoencoder_loss_train: 1.438913106918335 - discriminator_loss_train: 1.3764184713363647\n",
      "    autoencoder_loss_val: 1.5001944303512573 - discriminator_loss_val: 1.4589252471923828\n",
      "Starting epoch 282/1000\n",
      "    autoencoder_loss_train: 1.4271056652069092 - discriminator_loss_train: 1.3838975429534912\n",
      "    autoencoder_loss_val: 1.4883129596710205 - discriminator_loss_val: 1.4660093784332275\n",
      "Starting epoch 283/1000\n",
      "    autoencoder_loss_train: 1.4451491832733154 - discriminator_loss_train: 1.3764139413833618\n",
      "    autoencoder_loss_val: 1.5070656538009644 - discriminator_loss_val: 1.4610202312469482\n",
      "Starting epoch 284/1000\n",
      "    autoencoder_loss_train: 1.4710551500320435 - discriminator_loss_train: 1.3659586906433105\n",
      "    autoencoder_loss_val: 1.5334804058074951 - discriminator_loss_val: 1.4530956745147705\n",
      "Starting epoch 285/1000\n",
      "    autoencoder_loss_train: 1.4934531450271606 - discriminator_loss_train: 1.3588811159133911\n",
      "    autoencoder_loss_val: 1.5560967922210693 - discriminator_loss_val: 1.4482529163360596\n",
      "Starting epoch 286/1000\n",
      "    autoencoder_loss_train: 1.5178861618041992 - discriminator_loss_train: 1.3545438051223755\n",
      "    autoencoder_loss_val: 1.5811371803283691 - discriminator_loss_val: 1.4471874237060547\n",
      "Starting epoch 287/1000\n",
      "    autoencoder_loss_train: 1.5393729209899902 - discriminator_loss_train: 1.34571373462677\n",
      "    autoencoder_loss_val: 1.6030900478363037 - discriminator_loss_val: 1.4410734176635742\n",
      "Starting epoch 288/1000\n",
      "    autoencoder_loss_train: 1.5411059856414795 - discriminator_loss_train: 1.3370659351348877\n",
      "    autoencoder_loss_val: 1.6054149866104126 - discriminator_loss_val: 1.4328904151916504\n",
      "Starting epoch 289/1000\n",
      "    autoencoder_loss_train: 1.5344915390014648 - discriminator_loss_train: 1.3189131021499634\n",
      "    autoencoder_loss_val: 1.5991889238357544 - discriminator_loss_val: 1.4138243198394775\n",
      "Starting epoch 290/1000\n",
      "    autoencoder_loss_train: 1.5397908687591553 - discriminator_loss_train: 1.292075514793396\n",
      "    autoencoder_loss_val: 1.6049914360046387 - discriminator_loss_val: 1.3873690366744995\n",
      "Starting epoch 291/1000\n",
      "    autoencoder_loss_train: 1.544001579284668 - discriminator_loss_train: 1.2614247798919678\n",
      "    autoencoder_loss_val: 1.6097185611724854 - discriminator_loss_val: 1.3568947315216064\n",
      "Starting epoch 292/1000\n",
      "    autoencoder_loss_train: 1.5373973846435547 - discriminator_loss_train: 1.2349295616149902\n",
      "    autoencoder_loss_val: 1.6036877632141113 - discriminator_loss_val: 1.329615831375122\n",
      "Starting epoch 293/1000\n",
      "    autoencoder_loss_train: 1.5224119424819946 - discriminator_loss_train: 1.2149640321731567\n",
      "    autoencoder_loss_val: 1.5897998809814453 - discriminator_loss_val: 1.308326005935669\n",
      "Starting epoch 294/1000\n",
      "    autoencoder_loss_train: 1.4984800815582275 - discriminator_loss_train: 1.1978816986083984\n",
      "    autoencoder_loss_val: 1.566986083984375 - discriminator_loss_val: 1.2889950275421143\n",
      "Starting epoch 295/1000\n",
      "    autoencoder_loss_train: 1.4738526344299316 - discriminator_loss_train: 1.1836130619049072\n",
      "    autoencoder_loss_val: 1.543866515159607 - discriminator_loss_val: 1.2723685503005981\n",
      "Starting epoch 296/1000\n",
      "    autoencoder_loss_train: 1.4726855754852295 - discriminator_loss_train: 1.1714619398117065\n",
      "    autoencoder_loss_val: 1.5442806482315063 - discriminator_loss_val: 1.2612605094909668\n",
      "Starting epoch 297/1000\n",
      "    autoencoder_loss_train: 1.4750460386276245 - discriminator_loss_train: 1.166390061378479\n",
      "    autoencoder_loss_val: 1.5477094650268555 - discriminator_loss_val: 1.2585179805755615\n",
      "Starting epoch 298/1000\n",
      "    autoencoder_loss_train: 1.4717772006988525 - discriminator_loss_train: 1.1674916744232178\n",
      "    autoencoder_loss_val: 1.54538094997406 - discriminator_loss_val: 1.2613942623138428\n",
      "Starting epoch 299/1000\n",
      "    autoencoder_loss_train: 1.4685635566711426 - discriminator_loss_train: 1.174414873123169\n",
      "    autoencoder_loss_val: 1.5426106452941895 - discriminator_loss_val: 1.2705744504928589\n",
      "Starting epoch 300/1000\n",
      "    autoencoder_loss_train: 1.4492466449737549 - discriminator_loss_train: 1.1875982284545898\n",
      "    autoencoder_loss_val: 1.523073434829712 - discriminator_loss_val: 1.2842481136322021\n",
      "Starting epoch 301/1000\n",
      "    autoencoder_loss_train: 1.4223036766052246 - discriminator_loss_train: 1.206018328666687\n",
      "    autoencoder_loss_val: 1.4952490329742432 - discriminator_loss_val: 1.302025318145752\n",
      "Starting epoch 302/1000\n",
      "    autoencoder_loss_train: 1.4101448059082031 - discriminator_loss_train: 1.2265840768814087\n",
      "    autoencoder_loss_val: 1.4816590547561646 - discriminator_loss_val: 1.3234387636184692\n",
      "Starting epoch 303/1000\n",
      "    autoencoder_loss_train: 1.3994901180267334 - discriminator_loss_train: 1.2504112720489502\n",
      "    autoencoder_loss_val: 1.4695308208465576 - discriminator_loss_val: 1.3475611209869385\n",
      "Starting epoch 304/1000\n",
      "    autoencoder_loss_train: 1.3938343524932861 - discriminator_loss_train: 1.275794267654419\n",
      "    autoencoder_loss_val: 1.4617743492126465 - discriminator_loss_val: 1.373164176940918\n",
      "Starting epoch 305/1000\n",
      "    autoencoder_loss_train: 1.3912975788116455 - discriminator_loss_train: 1.30120050907135\n",
      "    autoencoder_loss_val: 1.4569185972213745 - discriminator_loss_val: 1.3978869915008545\n",
      "Starting epoch 306/1000\n",
      "    autoencoder_loss_train: 1.3881230354309082 - discriminator_loss_train: 1.3249473571777344\n",
      "    autoencoder_loss_val: 1.4507912397384644 - discriminator_loss_val: 1.4198564291000366\n",
      "Starting epoch 307/1000\n",
      "    autoencoder_loss_train: 1.383772611618042 - discriminator_loss_train: 1.3456710577011108\n",
      "    autoencoder_loss_val: 1.442095160484314 - discriminator_loss_val: 1.4383978843688965\n",
      "Starting epoch 308/1000\n",
      "    autoencoder_loss_train: 1.396302342414856 - discriminator_loss_train: 1.3595819473266602\n",
      "    autoencoder_loss_val: 1.4503593444824219 - discriminator_loss_val: 1.4518425464630127\n",
      "Starting epoch 309/1000\n",
      "    autoencoder_loss_train: 1.4180676937103271 - discriminator_loss_train: 1.3708195686340332\n",
      "    autoencoder_loss_val: 1.4674198627471924 - discriminator_loss_val: 1.464421033859253\n",
      "Starting epoch 310/1000\n",
      "    autoencoder_loss_train: 1.4364254474639893 - discriminator_loss_train: 1.378697395324707\n",
      "    autoencoder_loss_val: 1.4812774658203125 - discriminator_loss_val: 1.473366141319275\n",
      "Starting epoch 311/1000\n",
      "    autoencoder_loss_train: 1.4453932046890259 - discriminator_loss_train: 1.3826799392700195\n",
      "    autoencoder_loss_val: 1.4857721328735352 - discriminator_loss_val: 1.477238416671753\n",
      "Starting epoch 312/1000\n",
      "    autoencoder_loss_train: 1.4421279430389404 - discriminator_loss_train: 1.3822247982025146\n",
      "    autoencoder_loss_val: 1.478715181350708 - discriminator_loss_val: 1.4748307466506958\n",
      "Starting epoch 313/1000\n",
      "    autoencoder_loss_train: 1.4339485168457031 - discriminator_loss_train: 1.3804497718811035\n",
      "    autoencoder_loss_val: 1.4675434827804565 - discriminator_loss_val: 1.4711952209472656\n",
      "Starting epoch 314/1000\n",
      "    autoencoder_loss_train: 1.4103436470031738 - discriminator_loss_train: 1.3798376321792603\n",
      "    autoencoder_loss_val: 1.4414235353469849 - discriminator_loss_val: 1.4669685363769531\n",
      "Starting epoch 315/1000\n",
      "    autoencoder_loss_train: 1.3886699676513672 - discriminator_loss_train: 1.3773994445800781\n",
      "    autoencoder_loss_val: 1.4174683094024658 - discriminator_loss_val: 1.4621250629425049\n",
      "Starting epoch 316/1000\n",
      "    autoencoder_loss_train: 1.3851619958877563 - discriminator_loss_train: 1.3673635721206665\n",
      "    autoencoder_loss_val: 1.4121432304382324 - discriminator_loss_val: 1.4524526596069336\n",
      "Starting epoch 317/1000\n",
      "    autoencoder_loss_train: 1.4050853252410889 - discriminator_loss_train: 1.352941870689392\n",
      "    autoencoder_loss_val: 1.4299907684326172 - discriminator_loss_val: 1.4418964385986328\n",
      "Starting epoch 318/1000\n",
      "    autoencoder_loss_train: 1.4192519187927246 - discriminator_loss_train: 1.3431110382080078\n",
      "    autoencoder_loss_val: 1.4427590370178223 - discriminator_loss_val: 1.4348807334899902\n",
      "Starting epoch 319/1000\n",
      "    autoencoder_loss_train: 1.4106333255767822 - discriminator_loss_train: 1.3346874713897705\n",
      "    autoencoder_loss_val: 1.4337248802185059 - discriminator_loss_val: 1.4269537925720215\n",
      "Starting epoch 320/1000\n",
      "    autoencoder_loss_train: 1.3819408416748047 - discriminator_loss_train: 1.327096700668335\n",
      "    autoencoder_loss_val: 1.4055061340332031 - discriminator_loss_val: 1.4176630973815918\n",
      "Starting epoch 321/1000\n",
      "    autoencoder_loss_train: 1.343430995941162 - discriminator_loss_train: 1.3260459899902344\n",
      "    autoencoder_loss_val: 1.367415428161621 - discriminator_loss_val: 1.41439688205719\n",
      "Starting epoch 322/1000\n",
      "    autoencoder_loss_train: 1.2999024391174316 - discriminator_loss_train: 1.331204891204834\n",
      "    autoencoder_loss_val: 1.3243844509124756 - discriminator_loss_val: 1.4168624877929688\n",
      "Starting epoch 323/1000\n",
      "    autoencoder_loss_train: 1.2752501964569092 - discriminator_loss_train: 1.334831953048706\n",
      "    autoencoder_loss_val: 1.3006775379180908 - discriminator_loss_val: 1.4199784994125366\n",
      "Starting epoch 324/1000\n",
      "    autoencoder_loss_train: 1.2720032930374146 - discriminator_loss_train: 1.3324739933013916\n",
      "    autoencoder_loss_val: 1.298754096031189 - discriminator_loss_val: 1.4193322658538818\n",
      "Starting epoch 325/1000\n",
      "    autoencoder_loss_train: 1.2816600799560547 - discriminator_loss_train: 1.328231930732727\n",
      "    autoencoder_loss_val: 1.3097097873687744 - discriminator_loss_val: 1.4180707931518555\n",
      "Starting epoch 326/1000\n",
      "    autoencoder_loss_train: 1.30222487449646 - discriminator_loss_train: 1.326719045639038\n",
      "    autoencoder_loss_val: 1.331366777420044 - discriminator_loss_val: 1.4200785160064697\n",
      "Starting epoch 327/1000\n",
      "    autoencoder_loss_train: 1.3207693099975586 - discriminator_loss_train: 1.3279409408569336\n",
      "    autoencoder_loss_val: 1.351203203201294 - discriminator_loss_val: 1.4240500926971436\n",
      "Starting epoch 328/1000\n",
      "    autoencoder_loss_train: 1.31306791305542 - discriminator_loss_train: 1.330470323562622\n",
      "    autoencoder_loss_val: 1.3451156616210938 - discriminator_loss_val: 1.4265999794006348\n",
      "Starting epoch 329/1000\n",
      "    autoencoder_loss_train: 1.292943000793457 - discriminator_loss_train: 1.3341240882873535\n",
      "    autoencoder_loss_val: 1.3270254135131836 - discriminator_loss_val: 1.428726315498352\n",
      "Starting epoch 330/1000\n",
      "    autoencoder_loss_train: 1.2704569101333618 - discriminator_loss_train: 1.339160680770874\n",
      "    autoencoder_loss_val: 1.305762529373169 - discriminator_loss_val: 1.4315440654754639\n",
      "Starting epoch 331/1000\n",
      "    autoencoder_loss_train: 1.250131368637085 - discriminator_loss_train: 1.3438796997070312\n",
      "    autoencoder_loss_val: 1.2860896587371826 - discriminator_loss_val: 1.433549165725708\n",
      "Starting epoch 332/1000\n",
      "    autoencoder_loss_train: 1.240386724472046 - discriminator_loss_train: 1.3455331325531006\n",
      "    autoencoder_loss_val: 1.2769882678985596 - discriminator_loss_val: 1.4328904151916504\n",
      "Starting epoch 333/1000\n",
      "    autoencoder_loss_train: 1.2530035972595215 - discriminator_loss_train: 1.3400986194610596\n",
      "    autoencoder_loss_val: 1.2895872592926025 - discriminator_loss_val: 1.427304744720459\n",
      "Starting epoch 334/1000\n",
      "    autoencoder_loss_train: 1.266822099685669 - discriminator_loss_train: 1.3337868452072144\n",
      "    autoencoder_loss_val: 1.3037755489349365 - discriminator_loss_val: 1.4211177825927734\n",
      "Starting epoch 335/1000\n",
      "    autoencoder_loss_train: 1.2692378759384155 - discriminator_loss_train: 1.3282129764556885\n",
      "    autoencoder_loss_val: 1.306666374206543 - discriminator_loss_val: 1.4147601127624512\n",
      "Starting epoch 336/1000\n",
      "    autoencoder_loss_train: 1.2747673988342285 - discriminator_loss_train: 1.3213661909103394\n",
      "    autoencoder_loss_val: 1.3129013776779175 - discriminator_loss_val: 1.407238483428955\n",
      "Starting epoch 337/1000\n",
      "    autoencoder_loss_train: 1.2954041957855225 - discriminator_loss_train: 1.3069653511047363\n",
      "    autoencoder_loss_val: 1.3344635963439941 - discriminator_loss_val: 1.3937175273895264\n",
      "Starting epoch 338/1000\n",
      "    autoencoder_loss_train: 1.3256194591522217 - discriminator_loss_train: 1.2868069410324097\n",
      "    autoencoder_loss_val: 1.36552894115448 - discriminator_loss_val: 1.3757634162902832\n",
      "Starting epoch 339/1000\n",
      "    autoencoder_loss_train: 1.357865810394287 - discriminator_loss_train: 1.264817237854004\n",
      "    autoencoder_loss_val: 1.3983192443847656 - discriminator_loss_val: 1.3559048175811768\n",
      "Starting epoch 340/1000\n",
      "    autoencoder_loss_train: 1.3686578273773193 - discriminator_loss_train: 1.2453858852386475\n",
      "    autoencoder_loss_val: 1.4099366664886475 - discriminator_loss_val: 1.3367584943771362\n",
      "Starting epoch 341/1000\n",
      "    autoencoder_loss_train: 1.3621315956115723 - discriminator_loss_train: 1.2307558059692383\n",
      "    autoencoder_loss_val: 1.4049078226089478 - discriminator_loss_val: 1.3212716579437256\n",
      "Starting epoch 342/1000\n",
      "    autoencoder_loss_train: 1.3471810817718506 - discriminator_loss_train: 1.2233915328979492\n",
      "    autoencoder_loss_val: 1.3922264575958252 - discriminator_loss_val: 1.3126111030578613\n",
      "Starting epoch 343/1000\n",
      "    autoencoder_loss_train: 1.3266083002090454 - discriminator_loss_train: 1.2242637872695923\n",
      "    autoencoder_loss_val: 1.3742306232452393 - discriminator_loss_val: 1.312319040298462\n",
      "Starting epoch 344/1000\n",
      "    autoencoder_loss_train: 1.316598653793335 - discriminator_loss_train: 1.2295982837677002\n",
      "    autoencoder_loss_val: 1.3672266006469727 - discriminator_loss_val: 1.3184525966644287\n",
      "Starting epoch 345/1000\n",
      "    autoencoder_loss_train: 1.3142459392547607 - discriminator_loss_train: 1.2395038604736328\n",
      "    autoencoder_loss_val: 1.3680675029754639 - discriminator_loss_val: 1.3307876586914062\n",
      "Starting epoch 346/1000\n",
      "    autoencoder_loss_train: 1.3048497438430786 - discriminator_loss_train: 1.2586345672607422\n",
      "    autoencoder_loss_val: 1.3619619607925415 - discriminator_loss_val: 1.3520512580871582\n",
      "Starting epoch 347/1000\n",
      "    autoencoder_loss_train: 1.2953566312789917 - discriminator_loss_train: 1.2823972702026367\n",
      "    autoencoder_loss_val: 1.3553533554077148 - discriminator_loss_val: 1.3783029317855835\n",
      "Starting epoch 348/1000\n",
      "    autoencoder_loss_train: 1.2990316152572632 - discriminator_loss_train: 1.3087836503982544\n",
      "    autoencoder_loss_val: 1.361605167388916 - discriminator_loss_val: 1.4085073471069336\n",
      "Starting epoch 349/1000\n",
      "    autoencoder_loss_train: 1.302929401397705 - discriminator_loss_train: 1.33852219581604\n",
      "    autoencoder_loss_val: 1.3677778244018555 - discriminator_loss_val: 1.4412298202514648\n",
      "Starting epoch 350/1000\n",
      "    autoencoder_loss_train: 1.305551528930664 - discriminator_loss_train: 1.3689637184143066\n",
      "    autoencoder_loss_val: 1.3722970485687256 - discriminator_loss_val: 1.4736560583114624\n",
      "Starting epoch 351/1000\n",
      "    autoencoder_loss_train: 1.302533745765686 - discriminator_loss_train: 1.3977690935134888\n",
      "    autoencoder_loss_val: 1.3704619407653809 - discriminator_loss_val: 1.502436876296997\n",
      "Starting epoch 352/1000\n",
      "    autoencoder_loss_train: 1.3080250024795532 - discriminator_loss_train: 1.4219094514846802\n",
      "    autoencoder_loss_val: 1.376452922821045 - discriminator_loss_val: 1.5256139039993286\n",
      "Starting epoch 353/1000\n",
      "    autoencoder_loss_train: 1.309938907623291 - discriminator_loss_train: 1.439727544784546\n",
      "    autoencoder_loss_val: 1.3780460357666016 - discriminator_loss_val: 1.5407235622406006\n",
      "Starting epoch 354/1000\n",
      "    autoencoder_loss_train: 1.3209362030029297 - discriminator_loss_train: 1.4480783939361572\n",
      "    autoencoder_loss_val: 1.3880012035369873 - discriminator_loss_val: 1.5469752550125122\n",
      "Starting epoch 355/1000\n",
      "    autoencoder_loss_train: 1.3332265615463257 - discriminator_loss_train: 1.4492876529693604\n",
      "    autoencoder_loss_val: 1.3986191749572754 - discriminator_loss_val: 1.5458440780639648\n",
      "Starting epoch 356/1000\n",
      "    autoencoder_loss_train: 1.3459885120391846 - discriminator_loss_train: 1.4442967176437378\n",
      "    autoencoder_loss_val: 1.4089021682739258 - discriminator_loss_val: 1.5382301807403564\n",
      "Starting epoch 357/1000\n",
      "    autoencoder_loss_train: 1.3608613014221191 - discriminator_loss_train: 1.4336509704589844\n",
      "    autoencoder_loss_val: 1.4211816787719727 - discriminator_loss_val: 1.52457594871521\n",
      "Starting epoch 358/1000\n",
      "    autoencoder_loss_train: 1.3662660121917725 - discriminator_loss_train: 1.4179553985595703\n",
      "    autoencoder_loss_val: 1.424079418182373 - discriminator_loss_val: 1.504591941833496\n",
      "Starting epoch 359/1000\n",
      "    autoencoder_loss_train: 1.3631629943847656 - discriminator_loss_train: 1.4007117748260498\n",
      "    autoencoder_loss_val: 1.418487787246704 - discriminator_loss_val: 1.4821689128875732\n",
      "Starting epoch 360/1000\n",
      "    autoencoder_loss_train: 1.3561431169509888 - discriminator_loss_train: 1.3806287050247192\n",
      "    autoencoder_loss_val: 1.4087283611297607 - discriminator_loss_val: 1.457542896270752\n",
      "Starting epoch 361/1000\n",
      "    autoencoder_loss_train: 1.3445208072662354 - discriminator_loss_train: 1.3606841564178467\n",
      "    autoencoder_loss_val: 1.3943171501159668 - discriminator_loss_val: 1.433462381362915\n",
      "Starting epoch 362/1000\n",
      "    autoencoder_loss_train: 1.3503334522247314 - discriminator_loss_train: 1.33748459815979\n",
      "    autoencoder_loss_val: 1.3976671695709229 - discriminator_loss_val: 1.4089686870574951\n",
      "Starting epoch 363/1000\n",
      "    autoencoder_loss_train: 1.357992172241211 - discriminator_loss_train: 1.3132212162017822\n",
      "    autoencoder_loss_val: 1.4028754234313965 - discriminator_loss_val: 1.3845584392547607\n",
      "Starting epoch 364/1000\n",
      "    autoencoder_loss_train: 1.3620431423187256 - discriminator_loss_train: 1.2935703992843628\n",
      "    autoencoder_loss_val: 1.4045801162719727 - discriminator_loss_val: 1.3649096488952637\n",
      "Starting epoch 365/1000\n",
      "    autoencoder_loss_train: 1.355515718460083 - discriminator_loss_train: 1.2794430255889893\n",
      "    autoencoder_loss_val: 1.3957117795944214 - discriminator_loss_val: 1.3503358364105225\n",
      "Starting epoch 366/1000\n",
      "    autoencoder_loss_train: 1.3477331399917603 - discriminator_loss_train: 1.2702162265777588\n",
      "    autoencoder_loss_val: 1.3852603435516357 - discriminator_loss_val: 1.3413304090499878\n",
      "Starting epoch 367/1000\n",
      "    autoencoder_loss_train: 1.3275737762451172 - discriminator_loss_train: 1.2686855792999268\n",
      "    autoencoder_loss_val: 1.3624117374420166 - discriminator_loss_val: 1.339951515197754\n",
      "Starting epoch 368/1000\n",
      "    autoencoder_loss_train: 1.301391839981079 - discriminator_loss_train: 1.2717149257659912\n",
      "    autoencoder_loss_val: 1.3330720663070679 - discriminator_loss_val: 1.3437139987945557\n",
      "Starting epoch 369/1000\n",
      "    autoencoder_loss_train: 1.2732958793640137 - discriminator_loss_train: 1.2804590463638306\n",
      "    autoencoder_loss_val: 1.3019769191741943 - discriminator_loss_val: 1.3540316820144653\n",
      "Starting epoch 370/1000\n",
      "    autoencoder_loss_train: 1.2523646354675293 - discriminator_loss_train: 1.2946662902832031\n",
      "    autoencoder_loss_val: 1.2785784006118774 - discriminator_loss_val: 1.370938777923584\n",
      "Starting epoch 371/1000\n",
      "    autoencoder_loss_train: 1.2446470260620117 - discriminator_loss_train: 1.3108198642730713\n",
      "    autoencoder_loss_val: 1.2686691284179688 - discriminator_loss_val: 1.3915550708770752\n",
      "Starting epoch 372/1000\n",
      "    autoencoder_loss_train: 1.2485758066177368 - discriminator_loss_train: 1.3276739120483398\n",
      "    autoencoder_loss_val: 1.2704991102218628 - discriminator_loss_val: 1.4140565395355225\n",
      "Starting epoch 373/1000\n",
      "    autoencoder_loss_train: 1.264986276626587 - discriminator_loss_train: 1.3439933061599731\n",
      "    autoencoder_loss_val: 1.2849631309509277 - discriminator_loss_val: 1.4368089437484741\n",
      "Starting epoch 374/1000\n",
      "    autoencoder_loss_train: 1.2734178304672241 - discriminator_loss_train: 1.3601113557815552\n",
      "    autoencoder_loss_val: 1.2920796871185303 - discriminator_loss_val: 1.4584966897964478\n",
      "Starting epoch 375/1000\n",
      "    autoencoder_loss_train: 1.2814562320709229 - discriminator_loss_train: 1.377428412437439\n",
      "    autoencoder_loss_val: 1.2994329929351807 - discriminator_loss_val: 1.4811588525772095\n",
      "Starting epoch 376/1000\n",
      "    autoencoder_loss_train: 1.288475751876831 - discriminator_loss_train: 1.39481520652771\n",
      "    autoencoder_loss_val: 1.306097388267517 - discriminator_loss_val: 1.5030903816223145\n",
      "Starting epoch 377/1000\n",
      "    autoencoder_loss_train: 1.2939012050628662 - discriminator_loss_train: 1.4093568325042725\n",
      "    autoencoder_loss_val: 1.3116059303283691 - discriminator_loss_val: 1.5215110778808594\n",
      "Starting epoch 378/1000\n",
      "    autoencoder_loss_train: 1.302600622177124 - discriminator_loss_train: 1.4177594184875488\n",
      "    autoencoder_loss_val: 1.320626974105835 - discriminator_loss_val: 1.5327551364898682\n",
      "Starting epoch 379/1000\n",
      "    autoencoder_loss_train: 1.3095052242279053 - discriminator_loss_train: 1.4219026565551758\n",
      "    autoencoder_loss_val: 1.3284366130828857 - discriminator_loss_val: 1.538989543914795\n",
      "Starting epoch 380/1000\n",
      "    autoencoder_loss_train: 1.3132343292236328 - discriminator_loss_train: 1.4221982955932617\n",
      "    autoencoder_loss_val: 1.3338372707366943 - discriminator_loss_val: 1.5406687259674072\n",
      "Starting epoch 381/1000\n",
      "    autoencoder_loss_train: 1.3149724006652832 - discriminator_loss_train: 1.4145541191101074\n",
      "    autoencoder_loss_val: 1.3371527194976807 - discriminator_loss_val: 1.5327775478363037\n",
      "Starting epoch 382/1000\n",
      "    autoencoder_loss_train: 1.3121776580810547 - discriminator_loss_train: 1.4007792472839355\n",
      "    autoencoder_loss_val: 1.3360754251480103 - discriminator_loss_val: 1.5171774625778198\n",
      "Starting epoch 383/1000\n",
      "    autoencoder_loss_train: 1.322998046875 - discriminator_loss_train: 1.382485270500183\n",
      "    autoencoder_loss_val: 1.3486058712005615 - discriminator_loss_val: 1.4975519180297852\n",
      "Starting epoch 384/1000\n",
      "    autoencoder_loss_train: 1.3406360149383545 - discriminator_loss_train: 1.3615819215774536\n",
      "    autoencoder_loss_val: 1.3679094314575195 - discriminator_loss_val: 1.4753401279449463\n",
      "Starting epoch 385/1000\n",
      "    autoencoder_loss_train: 1.3540199995040894 - discriminator_loss_train: 1.3411505222320557\n",
      "    autoencoder_loss_val: 1.3840190172195435 - discriminator_loss_val: 1.4542059898376465\n",
      "Starting epoch 386/1000\n",
      "    autoencoder_loss_train: 1.373042345046997 - discriminator_loss_train: 1.321370005607605\n",
      "    autoencoder_loss_val: 1.406309723854065 - discriminator_loss_val: 1.4345920085906982\n",
      "Starting epoch 387/1000\n",
      "    autoencoder_loss_train: 1.390977382659912 - discriminator_loss_train: 1.3028327226638794\n",
      "    autoencoder_loss_val: 1.427774429321289 - discriminator_loss_val: 1.4164001941680908\n",
      "Starting epoch 388/1000\n",
      "    autoencoder_loss_train: 1.4054081439971924 - discriminator_loss_train: 1.284534215927124\n",
      "    autoencoder_loss_val: 1.4450085163116455 - discriminator_loss_val: 1.398226022720337\n",
      "Starting epoch 389/1000\n",
      "    autoencoder_loss_train: 1.417317271232605 - discriminator_loss_train: 1.268770456314087\n",
      "    autoencoder_loss_val: 1.4595675468444824 - discriminator_loss_val: 1.3825987577438354\n",
      "Starting epoch 390/1000\n",
      "    autoencoder_loss_train: 1.420971393585205 - discriminator_loss_train: 1.2551326751708984\n",
      "    autoencoder_loss_val: 1.4655187129974365 - discriminator_loss_val: 1.3690414428710938\n",
      "Starting epoch 391/1000\n",
      "    autoencoder_loss_train: 1.4103460311889648 - discriminator_loss_train: 1.2437537908554077\n",
      "    autoencoder_loss_val: 1.4573324918746948 - discriminator_loss_val: 1.3568847179412842\n",
      "Starting epoch 392/1000\n",
      "    autoencoder_loss_train: 1.3921897411346436 - discriminator_loss_train: 1.2386270761489868\n",
      "    autoencoder_loss_val: 1.441341519355774 - discriminator_loss_val: 1.351071834564209\n",
      "Starting epoch 393/1000\n",
      "    autoencoder_loss_train: 1.3701961040496826 - discriminator_loss_train: 1.2390586137771606\n",
      "    autoencoder_loss_val: 1.4216705560684204 - discriminator_loss_val: 1.3508546352386475\n",
      "Starting epoch 394/1000\n",
      "    autoencoder_loss_train: 1.3396635055541992 - discriminator_loss_train: 1.2472054958343506\n",
      "    autoencoder_loss_val: 1.3937956094741821 - discriminator_loss_val: 1.358262062072754\n",
      "Starting epoch 395/1000\n",
      "    autoencoder_loss_train: 1.319983959197998 - discriminator_loss_train: 1.263716697692871\n",
      "    autoencoder_loss_val: 1.376802682876587 - discriminator_loss_val: 1.3756502866744995\n",
      "Starting epoch 396/1000\n",
      "    autoencoder_loss_train: 1.300964593887329 - discriminator_loss_train: 1.2862842082977295\n",
      "    autoencoder_loss_val: 1.3602725267410278 - discriminator_loss_val: 1.3993988037109375\n",
      "Starting epoch 397/1000\n",
      "    autoencoder_loss_train: 1.288836121559143 - discriminator_loss_train: 1.3137023448944092\n",
      "    autoencoder_loss_val: 1.3499057292938232 - discriminator_loss_val: 1.4292018413543701\n",
      "Starting epoch 398/1000\n",
      "    autoencoder_loss_train: 1.273964285850525 - discriminator_loss_train: 1.342757225036621\n",
      "    autoencoder_loss_val: 1.3357738256454468 - discriminator_loss_val: 1.4601290225982666\n",
      "Starting epoch 399/1000\n",
      "    autoencoder_loss_train: 1.2562949657440186 - discriminator_loss_train: 1.3696529865264893\n",
      "    autoencoder_loss_val: 1.3192447423934937 - discriminator_loss_val: 1.4871301651000977\n",
      "Starting epoch 400/1000\n",
      "    autoencoder_loss_train: 1.2446099519729614 - discriminator_loss_train: 1.392701268196106\n",
      "    autoencoder_loss_val: 1.308009386062622 - discriminator_loss_val: 1.509645938873291\n",
      "Starting epoch 401/1000\n",
      "    autoencoder_loss_train: 1.2497202157974243 - discriminator_loss_train: 1.4070372581481934\n",
      "    autoencoder_loss_val: 1.3134939670562744 - discriminator_loss_val: 1.5242639780044556\n",
      "Starting epoch 402/1000\n",
      "    autoencoder_loss_train: 1.2748372554779053 - discriminator_loss_train: 1.4132931232452393\n",
      "    autoencoder_loss_val: 1.339646816253662 - discriminator_loss_val: 1.532001256942749\n",
      "Starting epoch 403/1000\n",
      "    autoencoder_loss_train: 1.3057154417037964 - discriminator_loss_train: 1.417701005935669\n",
      "    autoencoder_loss_val: 1.370247483253479 - discriminator_loss_val: 1.5380470752716064\n",
      "Starting epoch 404/1000\n",
      "    autoencoder_loss_train: 1.3251157999038696 - discriminator_loss_train: 1.419177770614624\n",
      "    autoencoder_loss_val: 1.3884830474853516 - discriminator_loss_val: 1.5391616821289062\n",
      "Starting epoch 405/1000\n",
      "    autoencoder_loss_train: 1.3446376323699951 - discriminator_loss_train: 1.4162771701812744\n",
      "    autoencoder_loss_val: 1.406346082687378 - discriminator_loss_val: 1.5351905822753906\n",
      "Starting epoch 406/1000\n",
      "    autoencoder_loss_train: 1.360331416130066 - discriminator_loss_train: 1.4106723070144653\n",
      "    autoencoder_loss_val: 1.4194309711456299 - discriminator_loss_val: 1.5277557373046875\n",
      "Starting epoch 407/1000\n",
      "    autoencoder_loss_train: 1.3652418851852417 - discriminator_loss_train: 1.4029438495635986\n",
      "    autoencoder_loss_val: 1.4214024543762207 - discriminator_loss_val: 1.5157585144042969\n",
      "Starting epoch 408/1000\n",
      "    autoencoder_loss_train: 1.3608037233352661 - discriminator_loss_train: 1.3926982879638672\n",
      "    autoencoder_loss_val: 1.4141347408294678 - discriminator_loss_val: 1.4994235038757324\n",
      "Starting epoch 409/1000\n",
      "    autoencoder_loss_train: 1.362709879875183 - discriminator_loss_train: 1.3795666694641113\n",
      "    autoencoder_loss_val: 1.4130148887634277 - discriminator_loss_val: 1.481307029724121\n",
      "Starting epoch 410/1000\n",
      "    autoencoder_loss_train: 1.3807661533355713 - discriminator_loss_train: 1.3624646663665771\n",
      "    autoencoder_loss_val: 1.4282047748565674 - discriminator_loss_val: 1.4607441425323486\n",
      "Starting epoch 411/1000\n",
      "    autoencoder_loss_train: 1.3829513788223267 - discriminator_loss_train: 1.3471901416778564\n",
      "    autoencoder_loss_val: 1.4272191524505615 - discriminator_loss_val: 1.4408738613128662\n",
      "Starting epoch 412/1000\n",
      "    autoencoder_loss_train: 1.3776652812957764 - discriminator_loss_train: 1.3338794708251953\n",
      "    autoencoder_loss_val: 1.4186477661132812 - discriminator_loss_val: 1.4233503341674805\n",
      "Starting epoch 413/1000\n",
      "    autoencoder_loss_train: 1.375166654586792 - discriminator_loss_train: 1.3220391273498535\n",
      "    autoencoder_loss_val: 1.4134795665740967 - discriminator_loss_val: 1.408277153968811\n",
      "Starting epoch 414/1000\n",
      "    autoencoder_loss_train: 1.3746488094329834 - discriminator_loss_train: 1.3116631507873535\n",
      "    autoencoder_loss_val: 1.4106465578079224 - discriminator_loss_val: 1.3950796127319336\n",
      "Starting epoch 415/1000\n",
      "    autoencoder_loss_train: 1.3693525791168213 - discriminator_loss_train: 1.3039698600769043\n",
      "    autoencoder_loss_val: 1.403376817703247 - discriminator_loss_val: 1.3843226432800293\n",
      "Starting epoch 416/1000\n",
      "    autoencoder_loss_train: 1.3616212606430054 - discriminator_loss_train: 1.2964930534362793\n",
      "    autoencoder_loss_val: 1.3936777114868164 - discriminator_loss_val: 1.3745418787002563\n",
      "Starting epoch 417/1000\n",
      "    autoencoder_loss_train: 1.3473691940307617 - discriminator_loss_train: 1.2917537689208984\n",
      "    autoencoder_loss_val: 1.3776788711547852 - discriminator_loss_val: 1.3676148653030396\n",
      "Starting epoch 418/1000\n",
      "    autoencoder_loss_train: 1.3441139459609985 - discriminator_loss_train: 1.2876906394958496\n",
      "    autoencoder_loss_val: 1.372999668121338 - discriminator_loss_val: 1.3632221221923828\n",
      "Starting epoch 419/1000\n",
      "    autoencoder_loss_train: 1.3363333940505981 - discriminator_loss_train: 1.2889173030853271\n",
      "    autoencoder_loss_val: 1.3642617464065552 - discriminator_loss_val: 1.3643176555633545\n",
      "Starting epoch 420/1000\n",
      "    autoencoder_loss_train: 1.3247578144073486 - discriminator_loss_train: 1.2924894094467163\n",
      "    autoencoder_loss_val: 1.3518623113632202 - discriminator_loss_val: 1.3679612874984741\n",
      "Starting epoch 421/1000\n",
      "    autoencoder_loss_train: 1.3085999488830566 - discriminator_loss_train: 1.2991068363189697\n",
      "    autoencoder_loss_val: 1.335483193397522 - discriminator_loss_val: 1.3747186660766602\n",
      "Starting epoch 422/1000\n",
      "    autoencoder_loss_train: 1.2900158166885376 - discriminator_loss_train: 1.3081917762756348\n",
      "    autoencoder_loss_val: 1.3166413307189941 - discriminator_loss_val: 1.383854866027832\n",
      "Starting epoch 423/1000\n",
      "    autoencoder_loss_train: 1.27518630027771 - discriminator_loss_train: 1.3136467933654785\n",
      "    autoencoder_loss_val: 1.3010004758834839 - discriminator_loss_val: 1.3898370265960693\n",
      "Starting epoch 424/1000\n",
      "    autoencoder_loss_train: 1.2532707452774048 - discriminator_loss_train: 1.319258213043213\n",
      "    autoencoder_loss_val: 1.2780742645263672 - discriminator_loss_val: 1.3956763744354248\n",
      "Starting epoch 425/1000\n",
      "    autoencoder_loss_train: 1.2305903434753418 - discriminator_loss_train: 1.3277976512908936\n",
      "    autoencoder_loss_val: 1.2543519735336304 - discriminator_loss_val: 1.4041073322296143\n",
      "Starting epoch 426/1000\n",
      "    autoencoder_loss_train: 1.2173123359680176 - discriminator_loss_train: 1.334334373474121\n",
      "    autoencoder_loss_val: 1.2403807640075684 - discriminator_loss_val: 1.4108524322509766\n",
      "Starting epoch 427/1000\n",
      "    autoencoder_loss_train: 1.2165799140930176 - discriminator_loss_train: 1.3367787599563599\n",
      "    autoencoder_loss_val: 1.239501714706421 - discriminator_loss_val: 1.4146207571029663\n",
      "Starting epoch 428/1000\n",
      "    autoencoder_loss_train: 1.2190581560134888 - discriminator_loss_train: 1.3364845514297485\n",
      "    autoencoder_loss_val: 1.2413883209228516 - discriminator_loss_val: 1.4155235290527344\n",
      "Starting epoch 429/1000\n",
      "    autoencoder_loss_train: 1.2130640745162964 - discriminator_loss_train: 1.3394114971160889\n",
      "    autoencoder_loss_val: 1.2355111837387085 - discriminator_loss_val: 1.4187982082366943\n",
      "Starting epoch 430/1000\n",
      "    autoencoder_loss_train: 1.2092010974884033 - discriminator_loss_train: 1.34303879737854\n",
      "    autoencoder_loss_val: 1.2324823141098022 - discriminator_loss_val: 1.4229451417922974\n",
      "Starting epoch 431/1000\n",
      "    autoencoder_loss_train: 1.2137879133224487 - discriminator_loss_train: 1.3442111015319824\n",
      "    autoencoder_loss_val: 1.2377054691314697 - discriminator_loss_val: 1.4252574443817139\n",
      "Starting epoch 432/1000\n",
      "    autoencoder_loss_train: 1.2214592695236206 - discriminator_loss_train: 1.3428759574890137\n",
      "    autoencoder_loss_val: 1.245577096939087 - discriminator_loss_val: 1.4252053499221802\n",
      "Starting epoch 433/1000\n",
      "    autoencoder_loss_train: 1.2286421060562134 - discriminator_loss_train: 1.3407338857650757\n",
      "    autoencoder_loss_val: 1.2528996467590332 - discriminator_loss_val: 1.4241135120391846\n",
      "Starting epoch 434/1000\n",
      "    autoencoder_loss_train: 1.230339527130127 - discriminator_loss_train: 1.3393959999084473\n",
      "    autoencoder_loss_val: 1.2548186779022217 - discriminator_loss_val: 1.4234492778778076\n",
      "Starting epoch 435/1000\n",
      "    autoencoder_loss_train: 1.2335801124572754 - discriminator_loss_train: 1.337960958480835\n",
      "    autoencoder_loss_val: 1.258073091506958 - discriminator_loss_val: 1.4227702617645264\n",
      "Starting epoch 436/1000\n",
      "    autoencoder_loss_train: 1.2360516786575317 - discriminator_loss_train: 1.3376238346099854\n",
      "    autoencoder_loss_val: 1.2606303691864014 - discriminator_loss_val: 1.4229273796081543\n",
      "Starting epoch 437/1000\n",
      "    autoencoder_loss_train: 1.2316677570343018 - discriminator_loss_train: 1.3389480113983154\n",
      "    autoencoder_loss_val: 1.2562928199768066 - discriminator_loss_val: 1.424060583114624\n",
      "Starting epoch 438/1000\n",
      "    autoencoder_loss_train: 1.2270456552505493 - discriminator_loss_train: 1.3414616584777832\n",
      "    autoencoder_loss_val: 1.2520654201507568 - discriminator_loss_val: 1.4266139268875122\n",
      "Starting epoch 439/1000\n",
      "    autoencoder_loss_train: 1.2155660390853882 - discriminator_loss_train: 1.3459718227386475\n",
      "    autoencoder_loss_val: 1.2416715621948242 - discriminator_loss_val: 1.4308760166168213\n",
      "Starting epoch 440/1000\n",
      "    autoencoder_loss_train: 1.1979448795318604 - discriminator_loss_train: 1.3532545566558838\n",
      "    autoencoder_loss_val: 1.225881814956665 - discriminator_loss_val: 1.4378082752227783\n",
      "Starting epoch 441/1000\n",
      "    autoencoder_loss_train: 1.1953730583190918 - discriminator_loss_train: 1.3592092990875244\n",
      "    autoencoder_loss_val: 1.224391222000122 - discriminator_loss_val: 1.4443951845169067\n",
      "Starting epoch 442/1000\n",
      "    autoencoder_loss_train: 1.1971626281738281 - discriminator_loss_train: 1.3647897243499756\n",
      "    autoencoder_loss_val: 1.227160096168518 - discriminator_loss_val: 1.4509727954864502\n",
      "Starting epoch 443/1000\n",
      "    autoencoder_loss_train: 1.1959139108657837 - discriminator_loss_train: 1.3715695142745972\n",
      "    autoencoder_loss_val: 1.2269748449325562 - discriminator_loss_val: 1.4586498737335205\n",
      "Starting epoch 444/1000\n",
      "    autoencoder_loss_train: 1.1888502836227417 - discriminator_loss_train: 1.3786790370941162\n",
      "    autoencoder_loss_val: 1.2199724912643433 - discriminator_loss_val: 1.4659022092819214\n",
      "Starting epoch 445/1000\n",
      "    autoencoder_loss_train: 1.181020975112915 - discriminator_loss_train: 1.3853553533554077\n",
      "    autoencoder_loss_val: 1.2118514776229858 - discriminator_loss_val: 1.4725544452667236\n",
      "Starting epoch 446/1000\n",
      "    autoencoder_loss_train: 1.1667197942733765 - discriminator_loss_train: 1.391746163368225\n",
      "    autoencoder_loss_val: 1.1979552507400513 - discriminator_loss_val: 1.4786213636398315\n",
      "Starting epoch 447/1000\n",
      "    autoencoder_loss_train: 1.1580836772918701 - discriminator_loss_train: 1.395233154296875\n",
      "    autoencoder_loss_val: 1.1900094747543335 - discriminator_loss_val: 1.482483148574829\n",
      "Starting epoch 448/1000\n",
      "    autoencoder_loss_train: 1.169740080833435 - discriminator_loss_train: 1.394407033920288\n",
      "    autoencoder_loss_val: 1.2023465633392334 - discriminator_loss_val: 1.4835296869277954\n",
      "Starting epoch 449/1000\n",
      "    autoencoder_loss_train: 1.1935076713562012 - discriminator_loss_train: 1.391430377960205\n",
      "    autoencoder_loss_val: 1.2265870571136475 - discriminator_loss_val: 1.4832720756530762\n",
      "Starting epoch 450/1000\n",
      "    autoencoder_loss_train: 1.2161533832550049 - discriminator_loss_train: 1.3861525058746338\n",
      "    autoencoder_loss_val: 1.2498865127563477 - discriminator_loss_val: 1.4805757999420166\n",
      "Starting epoch 451/1000\n",
      "    autoencoder_loss_train: 1.221869945526123 - discriminator_loss_train: 1.3795700073242188\n",
      "    autoencoder_loss_val: 1.2567402124404907 - discriminator_loss_val: 1.4750208854675293\n",
      "Starting epoch 452/1000\n",
      "    autoencoder_loss_train: 1.2070783376693726 - discriminator_loss_train: 1.3727326393127441\n",
      "    autoencoder_loss_val: 1.2433545589447021 - discriminator_loss_val: 1.4670751094818115\n",
      "Starting epoch 453/1000\n",
      "    autoencoder_loss_train: 1.1930263042449951 - discriminator_loss_train: 1.366286039352417\n",
      "    autoencoder_loss_val: 1.2300403118133545 - discriminator_loss_val: 1.4593009948730469\n",
      "Starting epoch 454/1000\n",
      "    autoencoder_loss_train: 1.1930019855499268 - discriminator_loss_train: 1.358565330505371\n",
      "    autoencoder_loss_val: 1.2297170162200928 - discriminator_loss_val: 1.4515236616134644\n",
      "Starting epoch 455/1000\n",
      "    autoencoder_loss_train: 1.1998510360717773 - discriminator_loss_train: 1.3514461517333984\n",
      "    autoencoder_loss_val: 1.2356914281845093 - discriminator_loss_val: 1.4449021816253662\n",
      "Starting epoch 456/1000\n",
      "    autoencoder_loss_train: 1.2094483375549316 - discriminator_loss_train: 1.3456945419311523\n",
      "    autoencoder_loss_val: 1.244823694229126 - discriminator_loss_val: 1.439995288848877\n",
      "Starting epoch 457/1000\n",
      "    autoencoder_loss_train: 1.2243895530700684 - discriminator_loss_train: 1.3385140895843506\n",
      "    autoencoder_loss_val: 1.2605187892913818 - discriminator_loss_val: 1.434490442276001\n",
      "Starting epoch 458/1000\n",
      "    autoencoder_loss_train: 1.2271323204040527 - discriminator_loss_train: 1.3293569087982178\n",
      "    autoencoder_loss_val: 1.2654107809066772 - discriminator_loss_val: 1.426788568496704\n",
      "Starting epoch 459/1000\n",
      "    autoencoder_loss_train: 1.2262299060821533 - discriminator_loss_train: 1.3234610557556152\n",
      "    autoencoder_loss_val: 1.2659952640533447 - discriminator_loss_val: 1.4218686819076538\n",
      "Starting epoch 460/1000\n",
      "    autoencoder_loss_train: 1.223915457725525 - discriminator_loss_train: 1.3200883865356445\n",
      "    autoencoder_loss_val: 1.264085292816162 - discriminator_loss_val: 1.4190893173217773\n",
      "Starting epoch 461/1000\n",
      "    autoencoder_loss_train: 1.220117211341858 - discriminator_loss_train: 1.3174638748168945\n",
      "    autoencoder_loss_val: 1.2601264715194702 - discriminator_loss_val: 1.4167704582214355\n",
      "Starting epoch 462/1000\n",
      "    autoencoder_loss_train: 1.228446125984192 - discriminator_loss_train: 1.3128447532653809\n",
      "    autoencoder_loss_val: 1.267662763595581 - discriminator_loss_val: 1.4134690761566162\n",
      "Starting epoch 463/1000\n",
      "    autoencoder_loss_train: 1.2359334230422974 - discriminator_loss_train: 1.3083702325820923\n",
      "    autoencoder_loss_val: 1.2738399505615234 - discriminator_loss_val: 1.4099748134613037\n",
      "Starting epoch 464/1000\n",
      "    autoencoder_loss_train: 1.245042324066162 - discriminator_loss_train: 1.3031394481658936\n",
      "    autoencoder_loss_val: 1.2812364101409912 - discriminator_loss_val: 1.4057002067565918\n",
      "Starting epoch 465/1000\n",
      "    autoencoder_loss_train: 1.2544938325881958 - discriminator_loss_train: 1.2979103326797485\n",
      "    autoencoder_loss_val: 1.289413332939148 - discriminator_loss_val: 1.4010810852050781\n",
      "Starting epoch 466/1000\n",
      "    autoencoder_loss_train: 1.261664867401123 - discriminator_loss_train: 1.2917780876159668\n",
      "    autoencoder_loss_val: 1.2954823970794678 - discriminator_loss_val: 1.3950610160827637\n",
      "Starting epoch 467/1000\n",
      "    autoencoder_loss_train: 1.2684046030044556 - discriminator_loss_train: 1.285028338432312\n",
      "    autoencoder_loss_val: 1.3012895584106445 - discriminator_loss_val: 1.388221025466919\n",
      "Starting epoch 468/1000\n",
      "    autoencoder_loss_train: 1.2624962329864502 - discriminator_loss_train: 1.2794520854949951\n",
      "    autoencoder_loss_val: 1.2948466539382935 - discriminator_loss_val: 1.381344199180603\n",
      "Starting epoch 469/1000\n",
      "    autoencoder_loss_train: 1.2620964050292969 - discriminator_loss_train: 1.2741187810897827\n",
      "    autoencoder_loss_val: 1.293977975845337 - discriminator_loss_val: 1.3749797344207764\n",
      "Starting epoch 470/1000\n",
      "    autoencoder_loss_train: 1.2588472366333008 - discriminator_loss_train: 1.270214557647705\n",
      "    autoencoder_loss_val: 1.2907918691635132 - discriminator_loss_val: 1.3698701858520508\n",
      "Starting epoch 471/1000\n",
      "    autoencoder_loss_train: 1.2459373474121094 - discriminator_loss_train: 1.2689570188522339\n",
      "    autoencoder_loss_val: 1.2781596183776855 - discriminator_loss_val: 1.3668864965438843\n",
      "Starting epoch 472/1000\n",
      "    autoencoder_loss_train: 1.2300684452056885 - discriminator_loss_train: 1.2705570459365845\n",
      "    autoencoder_loss_val: 1.26224684715271 - discriminator_loss_val: 1.3667793273925781\n",
      "Starting epoch 473/1000\n",
      "    autoencoder_loss_train: 1.2216277122497559 - discriminator_loss_train: 1.2730419635772705\n",
      "    autoencoder_loss_val: 1.253409504890442 - discriminator_loss_val: 1.3681789636611938\n",
      "Starting epoch 474/1000\n",
      "    autoencoder_loss_train: 1.2152901887893677 - discriminator_loss_train: 1.2781623601913452\n",
      "    autoencoder_loss_val: 1.2462831735610962 - discriminator_loss_val: 1.3728543519973755\n",
      "Starting epoch 475/1000\n",
      "    autoencoder_loss_train: 1.2020968198776245 - discriminator_loss_train: 1.2878077030181885\n",
      "    autoencoder_loss_val: 1.2326499223709106 - discriminator_loss_val: 1.3816359043121338\n",
      "Starting epoch 476/1000\n",
      "    autoencoder_loss_train: 1.1803932189941406 - discriminator_loss_train: 1.3024258613586426\n",
      "    autoencoder_loss_val: 1.2106322050094604 - discriminator_loss_val: 1.3947887420654297\n",
      "Starting epoch 477/1000\n",
      "    autoencoder_loss_train: 1.1740875244140625 - discriminator_loss_train: 1.3184908628463745\n",
      "    autoencoder_loss_val: 1.2038135528564453 - discriminator_loss_val: 1.4113266468048096\n",
      "Starting epoch 478/1000\n",
      "    autoencoder_loss_train: 1.1593345403671265 - discriminator_loss_train: 1.338688611984253\n",
      "    autoencoder_loss_val: 1.189069151878357 - discriminator_loss_val: 1.4312622547149658\n",
      "Starting epoch 479/1000\n",
      "    autoencoder_loss_train: 1.1442139148712158 - discriminator_loss_train: 1.359933853149414\n",
      "    autoencoder_loss_val: 1.1737719774246216 - discriminator_loss_val: 1.4519884586334229\n",
      "Starting epoch 480/1000\n",
      "    autoencoder_loss_train: 1.1336032152175903 - discriminator_loss_train: 1.3804166316986084\n",
      "    autoencoder_loss_val: 1.1635229587554932 - discriminator_loss_val: 1.4717912673950195\n",
      "Starting epoch 481/1000\n",
      "    autoencoder_loss_train: 1.1216861009597778 - discriminator_loss_train: 1.399895191192627\n",
      "    autoencoder_loss_val: 1.1521999835968018 - discriminator_loss_val: 1.4896385669708252\n",
      "Starting epoch 482/1000\n",
      "    autoencoder_loss_train: 1.1216052770614624 - discriminator_loss_train: 1.414125919342041\n",
      "    autoencoder_loss_val: 1.1531596183776855 - discriminator_loss_val: 1.5026624202728271\n",
      "Starting epoch 483/1000\n",
      "    autoencoder_loss_train: 1.128798484802246 - discriminator_loss_train: 1.421323299407959\n",
      "    autoencoder_loss_val: 1.161758542060852 - discriminator_loss_val: 1.5083966255187988\n",
      "Starting epoch 484/1000\n",
      "    autoencoder_loss_train: 1.1392078399658203 - discriminator_loss_train: 1.4204895496368408\n",
      "    autoencoder_loss_val: 1.1742680072784424 - discriminator_loss_val: 1.5048573017120361\n",
      "Starting epoch 485/1000\n",
      "    autoencoder_loss_train: 1.162811040878296 - discriminator_loss_train: 1.4119327068328857\n",
      "    autoencoder_loss_val: 1.1994225978851318 - discriminator_loss_val: 1.4945521354675293\n",
      "Starting epoch 486/1000\n",
      "    autoencoder_loss_train: 1.1906225681304932 - discriminator_loss_train: 1.3970091342926025\n",
      "    autoencoder_loss_val: 1.2285656929016113 - discriminator_loss_val: 1.4780876636505127\n",
      "Starting epoch 487/1000\n",
      "    autoencoder_loss_train: 1.2233190536499023 - discriminator_loss_train: 1.3759732246398926\n",
      "    autoencoder_loss_val: 1.262434482574463 - discriminator_loss_val: 1.4558570384979248\n",
      "Starting epoch 488/1000\n",
      "    autoencoder_loss_train: 1.2412850856781006 - discriminator_loss_train: 1.350825309753418\n",
      "    autoencoder_loss_val: 1.2815766334533691 - discriminator_loss_val: 1.4278546571731567\n",
      "Starting epoch 489/1000\n",
      "    autoencoder_loss_train: 1.2551978826522827 - discriminator_loss_train: 1.3240869045257568\n",
      "    autoencoder_loss_val: 1.2964786291122437 - discriminator_loss_val: 1.398226261138916\n",
      "Starting epoch 490/1000\n",
      "    autoencoder_loss_train: 1.2701301574707031 - discriminator_loss_train: 1.2974886894226074\n",
      "    autoencoder_loss_val: 1.3124427795410156 - discriminator_loss_val: 1.3689351081848145\n",
      "Starting epoch 491/1000\n",
      "    autoencoder_loss_train: 1.2859737873077393 - discriminator_loss_train: 1.273650884628296\n",
      "    autoencoder_loss_val: 1.3291043043136597 - discriminator_loss_val: 1.3428268432617188\n",
      "Starting epoch 492/1000\n",
      "    autoencoder_loss_train: 1.2923568487167358 - discriminator_loss_train: 1.2536261081695557\n",
      "    autoencoder_loss_val: 1.336568832397461 - discriminator_loss_val: 1.320159673690796\n",
      "Starting epoch 493/1000\n",
      "    autoencoder_loss_train: 1.2985327243804932 - discriminator_loss_train: 1.238417148590088\n",
      "    autoencoder_loss_val: 1.3437838554382324 - discriminator_loss_val: 1.3028953075408936\n",
      "Starting epoch 494/1000\n",
      "    autoencoder_loss_train: 1.3002663850784302 - discriminator_loss_train: 1.22861909866333\n",
      "    autoencoder_loss_val: 1.3465359210968018 - discriminator_loss_val: 1.2913191318511963\n",
      "Starting epoch 495/1000\n",
      "    autoencoder_loss_train: 1.305619239807129 - discriminator_loss_train: 1.2240073680877686\n",
      "    autoencoder_loss_val: 1.3529164791107178 - discriminator_loss_val: 1.285792589187622\n",
      "Starting epoch 496/1000\n",
      "    autoencoder_loss_train: 1.3074167966842651 - discriminator_loss_train: 1.225175380706787\n",
      "    autoencoder_loss_val: 1.355737328529358 - discriminator_loss_val: 1.2863335609436035\n",
      "Starting epoch 497/1000\n",
      "    autoencoder_loss_train: 1.3008451461791992 - discriminator_loss_train: 1.230855941772461\n",
      "    autoencoder_loss_val: 1.3492379188537598 - discriminator_loss_val: 1.291433334350586\n",
      "Starting epoch 498/1000\n",
      "    autoencoder_loss_train: 1.2945717573165894 - discriminator_loss_train: 1.2385079860687256\n",
      "    autoencoder_loss_val: 1.3425416946411133 - discriminator_loss_val: 1.2990553379058838\n",
      "Starting epoch 499/1000\n",
      "    autoencoder_loss_train: 1.293079137802124 - discriminator_loss_train: 1.246748685836792\n",
      "    autoencoder_loss_val: 1.339966058731079 - discriminator_loss_val: 1.3077454566955566\n",
      "Starting epoch 500/1000\n",
      "    autoencoder_loss_train: 1.2971316576004028 - discriminator_loss_train: 1.2552073001861572\n",
      "    autoencoder_loss_val: 1.342902421951294 - discriminator_loss_val: 1.317500352859497\n",
      "Starting epoch 501/1000\n",
      "    autoencoder_loss_train: 1.2887426614761353 - discriminator_loss_train: 1.2637804746627808\n",
      "    autoencoder_loss_val: 1.3332481384277344 - discriminator_loss_val: 1.3267565965652466\n",
      "Starting epoch 502/1000\n",
      "    autoencoder_loss_train: 1.2809324264526367 - discriminator_loss_train: 1.2724859714508057\n",
      "    autoencoder_loss_val: 1.324703574180603 - discriminator_loss_val: 1.3362709283828735\n",
      "Starting epoch 503/1000\n",
      "    autoencoder_loss_train: 1.2805790901184082 - discriminator_loss_train: 1.2801222801208496\n",
      "    autoencoder_loss_val: 1.3235976696014404 - discriminator_loss_val: 1.3450915813446045\n",
      "Starting epoch 504/1000\n",
      "    autoencoder_loss_train: 1.2744438648223877 - discriminator_loss_train: 1.2892260551452637\n",
      "    autoencoder_loss_val: 1.3167437314987183 - discriminator_loss_val: 1.3552935123443604\n",
      "Starting epoch 505/1000\n",
      "    autoencoder_loss_train: 1.2656385898590088 - discriminator_loss_train: 1.3013139963150024\n",
      "    autoencoder_loss_val: 1.307145595550537 - discriminator_loss_val: 1.3685705661773682\n",
      "Starting epoch 506/1000\n",
      "    autoencoder_loss_train: 1.2448878288269043 - discriminator_loss_train: 1.31752347946167\n",
      "    autoencoder_loss_val: 1.2851680517196655 - discriminator_loss_val: 1.3854985237121582\n",
      "Starting epoch 507/1000\n",
      "    autoencoder_loss_train: 1.2254860401153564 - discriminator_loss_train: 1.3375016450881958\n",
      "    autoencoder_loss_val: 1.2649019956588745 - discriminator_loss_val: 1.4066072702407837\n",
      "Starting epoch 508/1000\n",
      "    autoencoder_loss_train: 1.2135323286056519 - discriminator_loss_train: 1.3589613437652588\n",
      "    autoencoder_loss_val: 1.2524242401123047 - discriminator_loss_val: 1.4295885562896729\n",
      "Starting epoch 509/1000\n",
      "    autoencoder_loss_train: 1.1994853019714355 - discriminator_loss_train: 1.3817764520645142\n",
      "    autoencoder_loss_val: 1.2382782697677612 - discriminator_loss_val: 1.4532159566879272\n",
      "Starting epoch 510/1000\n",
      "    autoencoder_loss_train: 1.1808414459228516 - discriminator_loss_train: 1.4038128852844238\n",
      "    autoencoder_loss_val: 1.2202385663986206 - discriminator_loss_val: 1.4754130840301514\n",
      "Starting epoch 511/1000\n",
      "    autoencoder_loss_train: 1.1660337448120117 - discriminator_loss_train: 1.4270009994506836\n",
      "    autoencoder_loss_val: 1.2065019607543945 - discriminator_loss_val: 1.498236060142517\n",
      "Starting epoch 512/1000\n",
      "    autoencoder_loss_train: 1.1597466468811035 - discriminator_loss_train: 1.4502451419830322\n",
      "    autoencoder_loss_val: 1.2009371519088745 - discriminator_loss_val: 1.5214763879776\n",
      "Starting epoch 513/1000\n",
      "    autoencoder_loss_train: 1.1613621711730957 - discriminator_loss_train: 1.471121907234192\n",
      "    autoencoder_loss_val: 1.202266812324524 - discriminator_loss_val: 1.5425691604614258\n",
      "Starting epoch 514/1000\n",
      "    autoencoder_loss_train: 1.170196533203125 - discriminator_loss_train: 1.4903998374938965\n",
      "    autoencoder_loss_val: 1.210038185119629 - discriminator_loss_val: 1.5630145072937012\n",
      "Starting epoch 515/1000\n",
      "    autoencoder_loss_train: 1.185958981513977 - discriminator_loss_train: 1.5045051574707031\n",
      "    autoencoder_loss_val: 1.2258343696594238 - discriminator_loss_val: 1.5777944326400757\n",
      "Starting epoch 516/1000\n",
      "    autoencoder_loss_train: 1.2008001804351807 - discriminator_loss_train: 1.5121698379516602\n",
      "    autoencoder_loss_val: 1.2413036823272705 - discriminator_loss_val: 1.5852882862091064\n",
      "Starting epoch 517/1000\n",
      "    autoencoder_loss_train: 1.218583583831787 - discriminator_loss_train: 1.509183406829834\n",
      "    autoencoder_loss_val: 1.26107656955719 - discriminator_loss_val: 1.5822196006774902\n",
      "Starting epoch 518/1000\n",
      "    autoencoder_loss_train: 1.2303786277770996 - discriminator_loss_train: 1.5030741691589355\n",
      "    autoencoder_loss_val: 1.2742996215820312 - discriminator_loss_val: 1.5754882097244263\n",
      "Starting epoch 519/1000\n",
      "    autoencoder_loss_train: 1.2442865371704102 - discriminator_loss_train: 1.4950519800186157\n",
      "    autoencoder_loss_val: 1.2887458801269531 - discriminator_loss_val: 1.566635012626648\n",
      "Starting epoch 520/1000\n",
      "    autoencoder_loss_train: 1.2638585567474365 - discriminator_loss_train: 1.4806363582611084\n",
      "    autoencoder_loss_val: 1.3090319633483887 - discriminator_loss_val: 1.5518271923065186\n",
      "Starting epoch 521/1000\n",
      "    autoencoder_loss_train: 1.2850449085235596 - discriminator_loss_train: 1.4621758460998535\n",
      "    autoencoder_loss_val: 1.3306375741958618 - discriminator_loss_val: 1.5333552360534668\n",
      "Starting epoch 522/1000\n",
      "    autoencoder_loss_train: 1.3141450881958008 - discriminator_loss_train: 1.4406564235687256\n",
      "    autoencoder_loss_val: 1.3601596355438232 - discriminator_loss_val: 1.5133600234985352\n",
      "Starting epoch 523/1000\n",
      "    autoencoder_loss_train: 1.3475775718688965 - discriminator_loss_train: 1.4166594743728638\n",
      "    autoencoder_loss_val: 1.3939485549926758 - discriminator_loss_val: 1.4910956621170044\n",
      "Starting epoch 524/1000\n",
      "    autoencoder_loss_train: 1.3681797981262207 - discriminator_loss_train: 1.3927700519561768\n",
      "    autoencoder_loss_val: 1.4144694805145264 - discriminator_loss_val: 1.4679003953933716\n",
      "Starting epoch 525/1000\n",
      "    autoencoder_loss_train: 1.3815909624099731 - discriminator_loss_train: 1.3689085245132446\n",
      "    autoencoder_loss_val: 1.4278188943862915 - discriminator_loss_val: 1.4440174102783203\n",
      "Starting epoch 526/1000\n",
      "    autoencoder_loss_train: 1.3923583030700684 - discriminator_loss_train: 1.3458882570266724\n",
      "    autoencoder_loss_val: 1.4381375312805176 - discriminator_loss_val: 1.4209282398223877\n",
      "Starting epoch 527/1000\n",
      "    autoencoder_loss_train: 1.395270586013794 - discriminator_loss_train: 1.3223589658737183\n",
      "    autoencoder_loss_val: 1.4404382705688477 - discriminator_loss_val: 1.3975093364715576\n",
      "Starting epoch 528/1000\n",
      "    autoencoder_loss_train: 1.3921480178833008 - discriminator_loss_train: 1.3008532524108887\n",
      "    autoencoder_loss_val: 1.4357542991638184 - discriminator_loss_val: 1.3758926391601562\n",
      "Starting epoch 529/1000\n",
      "    autoencoder_loss_train: 1.391413688659668 - discriminator_loss_train: 1.284109115600586\n",
      "    autoencoder_loss_val: 1.4332302808761597 - discriminator_loss_val: 1.3591556549072266\n",
      "Starting epoch 530/1000\n",
      "    autoencoder_loss_train: 1.3946731090545654 - discriminator_loss_train: 1.2696095705032349\n",
      "    autoencoder_loss_val: 1.4350535869598389 - discriminator_loss_val: 1.3446667194366455\n",
      "Starting epoch 531/1000\n",
      "    autoencoder_loss_train: 1.397904872894287 - discriminator_loss_train: 1.2586805820465088\n",
      "    autoencoder_loss_val: 1.437110424041748 - discriminator_loss_val: 1.3341004848480225\n",
      "Starting epoch 532/1000\n",
      "    autoencoder_loss_train: 1.394344687461853 - discriminator_loss_train: 1.249971866607666\n",
      "    autoencoder_loss_val: 1.4321712255477905 - discriminator_loss_val: 1.3253884315490723\n",
      "Starting epoch 533/1000\n",
      "    autoencoder_loss_train: 1.3865413665771484 - discriminator_loss_train: 1.2453420162200928\n",
      "    autoencoder_loss_val: 1.4228756427764893 - discriminator_loss_val: 1.3211009502410889\n",
      "Starting epoch 534/1000\n",
      "    autoencoder_loss_train: 1.3817253112792969 - discriminator_loss_train: 1.2452366352081299\n",
      "    autoencoder_loss_val: 1.4175195693969727 - discriminator_loss_val: 1.3222460746765137\n",
      "Starting epoch 535/1000\n",
      "    autoencoder_loss_train: 1.3743600845336914 - discriminator_loss_train: 1.2456399202346802\n",
      "    autoencoder_loss_val: 1.4094905853271484 - discriminator_loss_val: 1.3232300281524658\n",
      "Starting epoch 536/1000\n",
      "    autoencoder_loss_train: 1.371203899383545 - discriminator_loss_train: 1.2460458278656006\n",
      "    autoencoder_loss_val: 1.4050642251968384 - discriminator_loss_val: 1.3242321014404297\n",
      "Starting epoch 537/1000\n",
      "    autoencoder_loss_train: 1.364783763885498 - discriminator_loss_train: 1.2480380535125732\n",
      "    autoencoder_loss_val: 1.3972257375717163 - discriminator_loss_val: 1.326740026473999\n",
      "Starting epoch 538/1000\n",
      "    autoencoder_loss_train: 1.349679708480835 - discriminator_loss_train: 1.2501291036605835\n",
      "    autoencoder_loss_val: 1.380699634552002 - discriminator_loss_val: 1.3289083242416382\n",
      "Starting epoch 539/1000\n",
      "    autoencoder_loss_train: 1.3387641906738281 - discriminator_loss_train: 1.2533714771270752\n",
      "    autoencoder_loss_val: 1.3692572116851807 - discriminator_loss_val: 1.3331310749053955\n",
      "Starting epoch 540/1000\n",
      "    autoencoder_loss_train: 1.330918312072754 - discriminator_loss_train: 1.2582273483276367\n",
      "    autoencoder_loss_val: 1.361222505569458 - discriminator_loss_val: 1.3394436836242676\n",
      "Starting epoch 541/1000\n",
      "    autoencoder_loss_train: 1.323495626449585 - discriminator_loss_train: 1.262721061706543\n",
      "    autoencoder_loss_val: 1.3530974388122559 - discriminator_loss_val: 1.3452388048171997\n",
      "Starting epoch 542/1000\n",
      "    autoencoder_loss_train: 1.3140722513198853 - discriminator_loss_train: 1.2642524242401123\n",
      "    autoencoder_loss_val: 1.3417532444000244 - discriminator_loss_val: 1.3475393056869507\n",
      "Starting epoch 543/1000\n",
      "    autoencoder_loss_train: 1.3150461912155151 - discriminator_loss_train: 1.2672607898712158\n",
      "    autoencoder_loss_val: 1.3411335945129395 - discriminator_loss_val: 1.3523154258728027\n",
      "Starting epoch 544/1000\n",
      "    autoencoder_loss_train: 1.3168084621429443 - discriminator_loss_train: 1.2715601921081543\n",
      "    autoencoder_loss_val: 1.3412649631500244 - discriminator_loss_val: 1.3589749336242676\n",
      "Starting epoch 545/1000\n",
      "    autoencoder_loss_train: 1.3281768560409546 - discriminator_loss_train: 1.2754652500152588\n",
      "    autoencoder_loss_val: 1.3503646850585938 - discriminator_loss_val: 1.3656847476959229\n",
      "Starting epoch 546/1000\n",
      "    autoencoder_loss_train: 1.3487313985824585 - discriminator_loss_train: 1.2779051065444946\n",
      "    autoencoder_loss_val: 1.3682103157043457 - discriminator_loss_val: 1.371343731880188\n",
      "Starting epoch 547/1000\n",
      "    autoencoder_loss_train: 1.3646111488342285 - discriminator_loss_train: 1.2780190706253052\n",
      "    autoencoder_loss_val: 1.3805736303329468 - discriminator_loss_val: 1.3735575675964355\n",
      "Starting epoch 548/1000\n",
      "    autoencoder_loss_train: 1.373591423034668 - discriminator_loss_train: 1.2790560722351074\n",
      "    autoencoder_loss_val: 1.386328935623169 - discriminator_loss_val: 1.3765928745269775\n",
      "Starting epoch 549/1000\n",
      "    autoencoder_loss_train: 1.374505877494812 - discriminator_loss_train: 1.281712293624878\n",
      "    autoencoder_loss_val: 1.3849904537200928 - discriminator_loss_val: 1.3809386491775513\n",
      "Starting epoch 550/1000\n",
      "    autoencoder_loss_train: 1.3737677335739136 - discriminator_loss_train: 1.2868874073028564\n",
      "    autoencoder_loss_val: 1.383084774017334 - discriminator_loss_val: 1.387918472290039\n",
      "Starting epoch 551/1000\n",
      "    autoencoder_loss_train: 1.3777519464492798 - discriminator_loss_train: 1.2947936058044434\n",
      "    autoencoder_loss_val: 1.386582612991333 - discriminator_loss_val: 1.398573398590088\n",
      "Starting epoch 552/1000\n",
      "    autoencoder_loss_train: 1.3824174404144287 - discriminator_loss_train: 1.304661750793457\n",
      "    autoencoder_loss_val: 1.3905792236328125 - discriminator_loss_val: 1.4109065532684326\n",
      "Starting epoch 553/1000\n",
      "    autoencoder_loss_train: 1.3931975364685059 - discriminator_loss_train: 1.3172235488891602\n",
      "    autoencoder_loss_val: 1.4010694026947021 - discriminator_loss_val: 1.4259265661239624\n",
      "Starting epoch 554/1000\n",
      "    autoencoder_loss_train: 1.4018791913986206 - discriminator_loss_train: 1.32852303981781\n",
      "    autoencoder_loss_val: 1.4096568822860718 - discriminator_loss_val: 1.4393470287322998\n",
      "Starting epoch 555/1000\n",
      "    autoencoder_loss_train: 1.3931972980499268 - discriminator_loss_train: 1.3391120433807373\n",
      "    autoencoder_loss_val: 1.4012084007263184 - discriminator_loss_val: 1.4510855674743652\n",
      "Starting epoch 556/1000\n",
      "    autoencoder_loss_train: 1.388885736465454 - discriminator_loss_train: 1.349081039428711\n",
      "    autoencoder_loss_val: 1.3969480991363525 - discriminator_loss_val: 1.46200692653656\n",
      "Starting epoch 557/1000\n",
      "    autoencoder_loss_train: 1.378832459449768 - discriminator_loss_train: 1.3582310676574707\n",
      "    autoencoder_loss_val: 1.387677550315857 - discriminator_loss_val: 1.4720442295074463\n",
      "Starting epoch 558/1000\n",
      "    autoencoder_loss_train: 1.3526381254196167 - discriminator_loss_train: 1.3636488914489746\n",
      "    autoencoder_loss_val: 1.3633832931518555 - discriminator_loss_val: 1.4774293899536133\n",
      "Starting epoch 559/1000\n",
      "    autoencoder_loss_train: 1.3281149864196777 - discriminator_loss_train: 1.366917610168457\n",
      "    autoencoder_loss_val: 1.3413997888565063 - discriminator_loss_val: 1.4800000190734863\n",
      "Starting epoch 560/1000\n",
      "    autoencoder_loss_train: 1.307715654373169 - discriminator_loss_train: 1.3702949285507202\n",
      "    autoencoder_loss_val: 1.3241584300994873 - discriminator_loss_val: 1.482283592224121\n",
      "Starting epoch 561/1000\n",
      "    autoencoder_loss_train: 1.2974495887756348 - discriminator_loss_train: 1.373591661453247\n",
      "    autoencoder_loss_val: 1.3168396949768066 - discriminator_loss_val: 1.4850202798843384\n",
      "Starting epoch 562/1000\n",
      "    autoencoder_loss_train: 1.2816177606582642 - discriminator_loss_train: 1.3741998672485352\n",
      "    autoencoder_loss_val: 1.3034327030181885 - discriminator_loss_val: 1.4842466115951538\n",
      "Starting epoch 563/1000\n",
      "    autoencoder_loss_train: 1.261301040649414 - discriminator_loss_train: 1.374535322189331\n",
      "    autoencoder_loss_val: 1.2854020595550537 - discriminator_loss_val: 1.4828388690948486\n",
      "Starting epoch 564/1000\n",
      "    autoencoder_loss_train: 1.2452809810638428 - discriminator_loss_train: 1.375504732131958\n",
      "    autoencoder_loss_val: 1.2714920043945312 - discriminator_loss_val: 1.4825496673583984\n",
      "Starting epoch 565/1000\n",
      "    autoencoder_loss_train: 1.2336537837982178 - discriminator_loss_train: 1.37418532371521\n",
      "    autoencoder_loss_val: 1.2619366645812988 - discriminator_loss_val: 1.4801945686340332\n",
      "Starting epoch 566/1000\n",
      "    autoencoder_loss_train: 1.2329301834106445 - discriminator_loss_train: 1.3699429035186768\n",
      "    autoencoder_loss_val: 1.263916015625 - discriminator_loss_val: 1.4752259254455566\n",
      "Starting epoch 567/1000\n",
      "    autoencoder_loss_train: 1.2317755222320557 - discriminator_loss_train: 1.3660035133361816\n",
      "    autoencoder_loss_val: 1.2663193941116333 - discriminator_loss_val: 1.4703551530838013\n",
      "Starting epoch 568/1000\n",
      "    autoencoder_loss_train: 1.22864830493927 - discriminator_loss_train: 1.3631795644760132\n",
      "    autoencoder_loss_val: 1.2661694288253784 - discriminator_loss_val: 1.4671480655670166\n",
      "Starting epoch 569/1000\n",
      "    autoencoder_loss_train: 1.22615647315979 - discriminator_loss_train: 1.3592934608459473\n",
      "    autoencoder_loss_val: 1.2665599584579468 - discriminator_loss_val: 1.462841510772705\n",
      "Starting epoch 570/1000\n",
      "    autoencoder_loss_train: 1.22001051902771 - discriminator_loss_train: 1.3546723127365112\n",
      "    autoencoder_loss_val: 1.2630685567855835 - discriminator_loss_val: 1.457279086112976\n",
      "Starting epoch 571/1000\n",
      "    autoencoder_loss_train: 1.2175610065460205 - discriminator_loss_train: 1.3494718074798584\n",
      "    autoencoder_loss_val: 1.2628570795059204 - discriminator_loss_val: 1.4520039558410645\n",
      "Starting epoch 572/1000\n",
      "    autoencoder_loss_train: 1.2243335247039795 - discriminator_loss_train: 1.3427637815475464\n",
      "    autoencoder_loss_val: 1.2717620134353638 - discriminator_loss_val: 1.4461901187896729\n",
      "Starting epoch 573/1000\n",
      "    autoencoder_loss_train: 1.2266449928283691 - discriminator_loss_train: 1.337239384651184\n",
      "    autoencoder_loss_val: 1.2763463258743286 - discriminator_loss_val: 1.4406764507293701\n",
      "Starting epoch 574/1000\n",
      "    autoencoder_loss_train: 1.2314541339874268 - discriminator_loss_train: 1.3304803371429443\n",
      "    autoencoder_loss_val: 1.283571481704712 - discriminator_loss_val: 1.4334664344787598\n",
      "Starting epoch 575/1000\n",
      "    autoencoder_loss_train: 1.2409694194793701 - discriminator_loss_train: 1.3258371353149414\n",
      "    autoencoder_loss_val: 1.2947901487350464 - discriminator_loss_val: 1.4289053678512573\n",
      "Starting epoch 576/1000\n",
      "    autoencoder_loss_train: 1.243053913116455 - discriminator_loss_train: 1.3244284391403198\n",
      "    autoencoder_loss_val: 1.2971317768096924 - discriminator_loss_val: 1.4276163578033447\n",
      "Starting epoch 577/1000\n",
      "    autoencoder_loss_train: 1.241219162940979 - discriminator_loss_train: 1.3258917331695557\n",
      "    autoencoder_loss_val: 1.2940433025360107 - discriminator_loss_val: 1.4287395477294922\n",
      "Starting epoch 578/1000\n",
      "    autoencoder_loss_train: 1.2474641799926758 - discriminator_loss_train: 1.327826738357544\n",
      "    autoencoder_loss_val: 1.2982006072998047 - discriminator_loss_val: 1.4302438497543335\n",
      "Starting epoch 579/1000\n",
      "    autoencoder_loss_train: 1.2563492059707642 - discriminator_loss_train: 1.3214459419250488\n",
      "    autoencoder_loss_val: 1.306767225265503 - discriminator_loss_val: 1.422430157661438\n",
      "Starting epoch 580/1000\n",
      "    autoencoder_loss_train: 1.2515842914581299 - discriminator_loss_train: 1.311598777770996\n",
      "    autoencoder_loss_val: 1.3014178276062012 - discriminator_loss_val: 1.4107770919799805\n",
      "Starting epoch 581/1000\n",
      "    autoencoder_loss_train: 1.2496545314788818 - discriminator_loss_train: 1.304804801940918\n",
      "    autoencoder_loss_val: 1.2993474006652832 - discriminator_loss_val: 1.4018349647521973\n",
      "Starting epoch 582/1000\n",
      "    autoencoder_loss_train: 1.2492423057556152 - discriminator_loss_train: 1.304018259048462\n",
      "    autoencoder_loss_val: 1.298179030418396 - discriminator_loss_val: 1.398996114730835\n",
      "Starting epoch 583/1000\n",
      "    autoencoder_loss_train: 1.2580945491790771 - discriminator_loss_train: 1.3072237968444824\n",
      "    autoencoder_loss_val: 1.3053020238876343 - discriminator_loss_val: 1.4011874198913574\n",
      "Starting epoch 584/1000\n",
      "    autoencoder_loss_train: 1.2689799070358276 - discriminator_loss_train: 1.3146204948425293\n",
      "    autoencoder_loss_val: 1.3134320974349976 - discriminator_loss_val: 1.4079020023345947\n",
      "Starting epoch 585/1000\n",
      "    autoencoder_loss_train: 1.2726759910583496 - discriminator_loss_train: 1.3229259252548218\n",
      "    autoencoder_loss_val: 1.3147344589233398 - discriminator_loss_val: 1.4149315357208252\n",
      "Starting epoch 586/1000\n",
      "    autoencoder_loss_train: 1.2598642110824585 - discriminator_loss_train: 1.3288825750350952\n",
      "    autoencoder_loss_val: 1.2994155883789062 - discriminator_loss_val: 1.4186934232711792\n",
      "Starting epoch 587/1000\n",
      "    autoencoder_loss_train: 1.2455722093582153 - discriminator_loss_train: 1.3345136642456055\n",
      "    autoencoder_loss_val: 1.2830525636672974 - discriminator_loss_val: 1.4230965375900269\n",
      "Starting epoch 588/1000\n",
      "    autoencoder_loss_train: 1.2249879837036133 - discriminator_loss_train: 1.341928482055664\n",
      "    autoencoder_loss_val: 1.2607570886611938 - discriminator_loss_val: 1.429102897644043\n",
      "Starting epoch 589/1000\n",
      "    autoencoder_loss_train: 1.2113903760910034 - discriminator_loss_train: 1.3499109745025635\n",
      "    autoencoder_loss_val: 1.2451388835906982 - discriminator_loss_val: 1.4361639022827148\n",
      "Starting epoch 590/1000\n",
      "    autoencoder_loss_train: 1.2064861059188843 - discriminator_loss_train: 1.3545036315917969\n",
      "    autoencoder_loss_val: 1.237722635269165 - discriminator_loss_val: 1.4399330615997314\n",
      "Starting epoch 591/1000\n",
      "    autoencoder_loss_train: 1.1909260749816895 - discriminator_loss_train: 1.359726905822754\n",
      "    autoencoder_loss_val: 1.2202496528625488 - discriminator_loss_val: 1.4439210891723633\n",
      "Starting epoch 592/1000\n",
      "    autoencoder_loss_train: 1.1808414459228516 - discriminator_loss_train: 1.3694217205047607\n",
      "    autoencoder_loss_val: 1.208802342414856 - discriminator_loss_val: 1.453020691871643\n",
      "Starting epoch 593/1000\n",
      "    autoencoder_loss_train: 1.171513557434082 - discriminator_loss_train: 1.378746509552002\n",
      "    autoencoder_loss_val: 1.1982698440551758 - discriminator_loss_val: 1.4616448879241943\n",
      "Starting epoch 594/1000\n",
      "    autoencoder_loss_train: 1.1619659662246704 - discriminator_loss_train: 1.3887860774993896\n",
      "    autoencoder_loss_val: 1.1874446868896484 - discriminator_loss_val: 1.4707586765289307\n",
      "Starting epoch 595/1000\n",
      "    autoencoder_loss_train: 1.1638484001159668 - discriminator_loss_train: 1.3951680660247803\n",
      "    autoencoder_loss_val: 1.1889923810958862 - discriminator_loss_val: 1.4762582778930664\n",
      "Starting epoch 596/1000\n",
      "    autoencoder_loss_train: 1.1762062311172485 - discriminator_loss_train: 1.3977909088134766\n",
      "    autoencoder_loss_val: 1.2011628150939941 - discriminator_loss_val: 1.478285789489746\n",
      "Starting epoch 597/1000\n",
      "    autoencoder_loss_train: 1.1832479238510132 - discriminator_loss_train: 1.4028081893920898\n",
      "    autoencoder_loss_val: 1.2085310220718384 - discriminator_loss_val: 1.4825141429901123\n",
      "Starting epoch 598/1000\n",
      "    autoencoder_loss_train: 1.185173511505127 - discriminator_loss_train: 1.4032729864120483\n",
      "    autoencoder_loss_val: 1.211108922958374 - discriminator_loss_val: 1.4814220666885376\n",
      "Starting epoch 599/1000\n",
      "    autoencoder_loss_train: 1.182450294494629 - discriminator_loss_train: 1.4042896032333374\n",
      "    autoencoder_loss_val: 1.20973539352417 - discriminator_loss_val: 1.480667233467102\n",
      "Starting epoch 600/1000\n",
      "    autoencoder_loss_train: 1.1717582941055298 - discriminator_loss_train: 1.4104700088500977\n",
      "    autoencoder_loss_val: 1.2000563144683838 - discriminator_loss_val: 1.4852354526519775\n",
      "Starting epoch 601/1000\n",
      "    autoencoder_loss_train: 1.178844928741455 - discriminator_loss_train: 1.408690333366394\n",
      "    autoencoder_loss_val: 1.207602620124817 - discriminator_loss_val: 1.4829164743423462\n",
      "Starting epoch 602/1000\n",
      "    autoencoder_loss_train: 1.1863796710968018 - discriminator_loss_train: 1.4060311317443848\n",
      "    autoencoder_loss_val: 1.2150824069976807 - discriminator_loss_val: 1.479703664779663\n",
      "Starting epoch 603/1000\n",
      "    autoencoder_loss_train: 1.18904447555542 - discriminator_loss_train: 1.4036023616790771\n",
      "    autoencoder_loss_val: 1.217807650566101 - discriminator_loss_val: 1.4765174388885498\n",
      "Starting epoch 604/1000\n",
      "    autoencoder_loss_train: 1.1954848766326904 - discriminator_loss_train: 1.399245262145996\n",
      "    autoencoder_loss_val: 1.224297285079956 - discriminator_loss_val: 1.471872091293335\n",
      "Starting epoch 605/1000\n",
      "    autoencoder_loss_train: 1.1995763778686523 - discriminator_loss_train: 1.397385597229004\n",
      "    autoencoder_loss_val: 1.2283360958099365 - discriminator_loss_val: 1.4700713157653809\n",
      "Starting epoch 606/1000\n",
      "    autoencoder_loss_train: 1.1962276697158813 - discriminator_loss_train: 1.395219326019287\n",
      "    autoencoder_loss_val: 1.22487211227417 - discriminator_loss_val: 1.467057704925537\n",
      "Starting epoch 607/1000\n",
      "    autoencoder_loss_train: 1.1809303760528564 - discriminator_loss_train: 1.3950672149658203\n",
      "    autoencoder_loss_val: 1.2094650268554688 - discriminator_loss_val: 1.4651446342468262\n",
      "Starting epoch 608/1000\n",
      "    autoencoder_loss_train: 1.1607866287231445 - discriminator_loss_train: 1.3980149030685425\n",
      "    autoencoder_loss_val: 1.1893306970596313 - discriminator_loss_val: 1.466027021408081\n",
      "Starting epoch 609/1000\n",
      "    autoencoder_loss_train: 1.1554313898086548 - discriminator_loss_train: 1.391013741493225\n",
      "    autoencoder_loss_val: 1.183776617050171 - discriminator_loss_val: 1.4580718278884888\n",
      "Starting epoch 610/1000\n",
      "    autoencoder_loss_train: 1.1642844676971436 - discriminator_loss_train: 1.3712974786758423\n",
      "    autoencoder_loss_val: 1.191584587097168 - discriminator_loss_val: 1.4379956722259521\n",
      "Starting epoch 611/1000\n",
      "    autoencoder_loss_train: 1.1791720390319824 - discriminator_loss_train: 1.3468449115753174\n",
      "    autoencoder_loss_val: 1.2049410343170166 - discriminator_loss_val: 1.4132558107376099\n",
      "Starting epoch 612/1000\n",
      "    autoencoder_loss_train: 1.1949747800827026 - discriminator_loss_train: 1.3246965408325195\n",
      "    autoencoder_loss_val: 1.2190406322479248 - discriminator_loss_val: 1.390952467918396\n",
      "Starting epoch 613/1000\n",
      "    autoencoder_loss_train: 1.1927849054336548 - discriminator_loss_train: 1.3111293315887451\n",
      "    autoencoder_loss_val: 1.2166870832443237 - discriminator_loss_val: 1.3771154880523682\n",
      "Starting epoch 614/1000\n",
      "    autoencoder_loss_train: 1.1708115339279175 - discriminator_loss_train: 1.3078445196151733\n",
      "    autoencoder_loss_val: 1.1961731910705566 - discriminator_loss_val: 1.373497724533081\n",
      "Starting epoch 615/1000\n",
      "    autoencoder_loss_train: 1.1446231603622437 - discriminator_loss_train: 1.3108391761779785\n",
      "    autoencoder_loss_val: 1.1718659400939941 - discriminator_loss_val: 1.3758835792541504\n",
      "Starting epoch 616/1000\n",
      "    autoencoder_loss_train: 1.1320891380310059 - discriminator_loss_train: 1.3084936141967773\n",
      "    autoencoder_loss_val: 1.1600494384765625 - discriminator_loss_val: 1.3731203079223633\n",
      "Starting epoch 617/1000\n",
      "    autoencoder_loss_train: 1.1311373710632324 - discriminator_loss_train: 1.298755168914795\n",
      "    autoencoder_loss_val: 1.1582624912261963 - discriminator_loss_val: 1.3638756275177002\n",
      "Starting epoch 618/1000\n",
      "    autoencoder_loss_train: 1.1327354907989502 - discriminator_loss_train: 1.2862287759780884\n",
      "    autoencoder_loss_val: 1.1582708358764648 - discriminator_loss_val: 1.3518116474151611\n",
      "Starting epoch 619/1000\n",
      "    autoencoder_loss_train: 1.1302024126052856 - discriminator_loss_train: 1.2751867771148682\n",
      "    autoencoder_loss_val: 1.1536414623260498 - discriminator_loss_val: 1.3408524990081787\n",
      "Starting epoch 620/1000\n",
      "    autoencoder_loss_train: 1.1099796295166016 - discriminator_loss_train: 1.2752336263656616\n",
      "    autoencoder_loss_val: 1.1327377557754517 - discriminator_loss_val: 1.3406732082366943\n",
      "Starting epoch 621/1000\n",
      "    autoencoder_loss_train: 1.0857083797454834 - discriminator_loss_train: 1.2800791263580322\n",
      "    autoencoder_loss_val: 1.108274221420288 - discriminator_loss_val: 1.3453123569488525\n",
      "Starting epoch 622/1000\n",
      "    autoencoder_loss_train: 1.0697829723358154 - discriminator_loss_train: 1.2851762771606445\n",
      "    autoencoder_loss_val: 1.0926963090896606 - discriminator_loss_val: 1.35105299949646\n",
      "Starting epoch 623/1000\n",
      "    autoencoder_loss_train: 1.0731098651885986 - discriminator_loss_train: 1.283989667892456\n",
      "    autoencoder_loss_val: 1.0950201749801636 - discriminator_loss_val: 1.3512972593307495\n",
      "Starting epoch 624/1000\n",
      "    autoencoder_loss_train: 1.0719730854034424 - discriminator_loss_train: 1.287530779838562\n",
      "    autoencoder_loss_val: 1.0931339263916016 - discriminator_loss_val: 1.356236457824707\n",
      "Starting epoch 625/1000\n",
      "    autoencoder_loss_train: 1.0619614124298096 - discriminator_loss_train: 1.2963378429412842\n",
      "    autoencoder_loss_val: 1.082607626914978 - discriminator_loss_val: 1.3654783964157104\n",
      "Starting epoch 626/1000\n",
      "    autoencoder_loss_train: 1.0576695203781128 - discriminator_loss_train: 1.3040616512298584\n",
      "    autoencoder_loss_val: 1.0776820182800293 - discriminator_loss_val: 1.3736789226531982\n",
      "Starting epoch 627/1000\n",
      "    autoencoder_loss_train: 1.0455527305603027 - discriminator_loss_train: 1.3156564235687256\n",
      "    autoencoder_loss_val: 1.065322995185852 - discriminator_loss_val: 1.384995937347412\n",
      "Starting epoch 628/1000\n",
      "    autoencoder_loss_train: 1.0286471843719482 - discriminator_loss_train: 1.33148193359375\n",
      "    autoencoder_loss_val: 1.0479460954666138 - discriminator_loss_val: 1.4001350402832031\n",
      "Starting epoch 629/1000\n",
      "    autoencoder_loss_train: 1.0188426971435547 - discriminator_loss_train: 1.3448328971862793\n",
      "    autoencoder_loss_val: 1.038325548171997 - discriminator_loss_val: 1.4133557081222534\n",
      "Starting epoch 630/1000\n",
      "    autoencoder_loss_train: 1.0085808038711548 - discriminator_loss_train: 1.357918381690979\n",
      "    autoencoder_loss_val: 1.0289325714111328 - discriminator_loss_val: 1.4260175228118896\n",
      "Starting epoch 631/1000\n",
      "    autoencoder_loss_train: 1.0085275173187256 - discriminator_loss_train: 1.366430640220642\n",
      "    autoencoder_loss_val: 1.0292699337005615 - discriminator_loss_val: 1.4343960285186768\n",
      "Starting epoch 632/1000\n",
      "    autoencoder_loss_train: 1.014898657798767 - discriminator_loss_train: 1.3705341815948486\n",
      "    autoencoder_loss_val: 1.0360448360443115 - discriminator_loss_val: 1.4385098218917847\n",
      "Starting epoch 633/1000\n",
      "    autoencoder_loss_train: 1.0235819816589355 - discriminator_loss_train: 1.3731505870819092\n",
      "    autoencoder_loss_val: 1.0450448989868164 - discriminator_loss_val: 1.441258192062378\n",
      "Starting epoch 634/1000\n",
      "    autoencoder_loss_train: 1.0406596660614014 - discriminator_loss_train: 1.3732354640960693\n",
      "    autoencoder_loss_val: 1.0624414682388306 - discriminator_loss_val: 1.4422276020050049\n",
      "Starting epoch 635/1000\n",
      "    autoencoder_loss_train: 1.053005337715149 - discriminator_loss_train: 1.3717552423477173\n",
      "    autoencoder_loss_val: 1.0758113861083984 - discriminator_loss_val: 1.4411518573760986\n",
      "Starting epoch 636/1000\n",
      "    autoencoder_loss_train: 1.0621109008789062 - discriminator_loss_train: 1.3702917098999023\n",
      "    autoencoder_loss_val: 1.0860843658447266 - discriminator_loss_val: 1.439535140991211\n",
      "Starting epoch 637/1000\n",
      "    autoencoder_loss_train: 1.0632504224777222 - discriminator_loss_train: 1.3725407123565674\n",
      "    autoencoder_loss_val: 1.0880531072616577 - discriminator_loss_val: 1.4409722089767456\n",
      "Starting epoch 638/1000\n",
      "    autoencoder_loss_train: 1.0642445087432861 - discriminator_loss_train: 1.3759047985076904\n",
      "    autoencoder_loss_val: 1.089111089706421 - discriminator_loss_val: 1.4441063404083252\n",
      "Starting epoch 639/1000\n",
      "    autoencoder_loss_train: 1.0621685981750488 - discriminator_loss_train: 1.3809053897857666\n",
      "    autoencoder_loss_val: 1.0868240594863892 - discriminator_loss_val: 1.4488227367401123\n",
      "Starting epoch 640/1000\n",
      "    autoencoder_loss_train: 1.0611660480499268 - discriminator_loss_train: 1.3842909336090088\n",
      "    autoencoder_loss_val: 1.0854662656784058 - discriminator_loss_val: 1.452016830444336\n",
      "Starting epoch 641/1000\n",
      "    autoencoder_loss_train: 1.065854549407959 - discriminator_loss_train: 1.3838469982147217\n",
      "    autoencoder_loss_val: 1.0897430181503296 - discriminator_loss_val: 1.4521045684814453\n",
      "Starting epoch 642/1000\n",
      "    autoencoder_loss_train: 1.0697224140167236 - discriminator_loss_train: 1.382875919342041\n",
      "    autoencoder_loss_val: 1.093257188796997 - discriminator_loss_val: 1.4518492221832275\n",
      "Starting epoch 643/1000\n",
      "    autoencoder_loss_train: 1.074009895324707 - discriminator_loss_train: 1.3807622194290161\n",
      "    autoencoder_loss_val: 1.0970776081085205 - discriminator_loss_val: 1.4505059719085693\n",
      "Starting epoch 644/1000\n",
      "    autoencoder_loss_train: 1.0776307582855225 - discriminator_loss_train: 1.376189112663269\n",
      "    autoencoder_loss_val: 1.100458025932312 - discriminator_loss_val: 1.446378231048584\n",
      "Starting epoch 645/1000\n",
      "    autoencoder_loss_train: 1.0772701501846313 - discriminator_loss_train: 1.3723196983337402\n",
      "    autoencoder_loss_val: 1.0997051000595093 - discriminator_loss_val: 1.442669153213501\n",
      "Starting epoch 646/1000\n",
      "    autoencoder_loss_train: 1.0776865482330322 - discriminator_loss_train: 1.3692699670791626\n",
      "    autoencoder_loss_val: 1.0993691682815552 - discriminator_loss_val: 1.4400508403778076\n",
      "Starting epoch 647/1000\n",
      "    autoencoder_loss_train: 1.0790512561798096 - discriminator_loss_train: 1.3665401935577393\n",
      "    autoencoder_loss_val: 1.099645972251892 - discriminator_loss_val: 1.4378238916397095\n",
      "Starting epoch 648/1000\n",
      "    autoencoder_loss_train: 1.0754494667053223 - discriminator_loss_train: 1.3622455596923828\n",
      "    autoencoder_loss_val: 1.0953553915023804 - discriminator_loss_val: 1.4333745241165161\n",
      "Starting epoch 649/1000\n",
      "    autoencoder_loss_train: 1.068935751914978 - discriminator_loss_train: 1.3554461002349854\n",
      "    autoencoder_loss_val: 1.088680386543274 - discriminator_loss_val: 1.4259576797485352\n",
      "Starting epoch 650/1000\n",
      "    autoencoder_loss_train: 1.063491940498352 - discriminator_loss_train: 1.350792646408081\n",
      "    autoencoder_loss_val: 1.082979440689087 - discriminator_loss_val: 1.420910358428955\n",
      "Starting epoch 651/1000\n",
      "    autoencoder_loss_train: 1.0588791370391846 - discriminator_loss_train: 1.3472633361816406\n",
      "    autoencoder_loss_val: 1.0779635906219482 - discriminator_loss_val: 1.4170019626617432\n",
      "Starting epoch 652/1000\n",
      "    autoencoder_loss_train: 1.0528700351715088 - discriminator_loss_train: 1.3423702716827393\n",
      "    autoencoder_loss_val: 1.0722841024398804 - discriminator_loss_val: 1.4114899635314941\n",
      "Starting epoch 653/1000\n",
      "    autoencoder_loss_train: 1.0400798320770264 - discriminator_loss_train: 1.3386598825454712\n",
      "    autoencoder_loss_val: 1.0602355003356934 - discriminator_loss_val: 1.4063456058502197\n",
      "Starting epoch 654/1000\n",
      "    autoencoder_loss_train: 1.0299324989318848 - discriminator_loss_train: 1.3353198766708374\n",
      "    autoencoder_loss_val: 1.05108642578125 - discriminator_loss_val: 1.4017558097839355\n",
      "Starting epoch 655/1000\n",
      "    autoencoder_loss_train: 1.027691125869751 - discriminator_loss_train: 1.33314847946167\n",
      "    autoencoder_loss_val: 1.0489044189453125 - discriminator_loss_val: 1.3992408514022827\n",
      "Starting epoch 656/1000\n",
      "    autoencoder_loss_train: 1.0301578044891357 - discriminator_loss_train: 1.3318781852722168\n",
      "    autoencoder_loss_val: 1.0509406328201294 - discriminator_loss_val: 1.3981363773345947\n",
      "Starting epoch 657/1000\n",
      "    autoencoder_loss_train: 1.028045415878296 - discriminator_loss_train: 1.3290021419525146\n",
      "    autoencoder_loss_val: 1.0489925146102905 - discriminator_loss_val: 1.3947534561157227\n",
      "Starting epoch 658/1000\n",
      "    autoencoder_loss_train: 1.021898865699768 - discriminator_loss_train: 1.3270994424819946\n",
      "    autoencoder_loss_val: 1.0433909893035889 - discriminator_loss_val: 1.3919310569763184\n",
      "Starting epoch 659/1000\n",
      "    autoencoder_loss_train: 1.0195025205612183 - discriminator_loss_train: 1.3264094591140747\n",
      "    autoencoder_loss_val: 1.0408284664154053 - discriminator_loss_val: 1.3908562660217285\n",
      "Starting epoch 660/1000\n",
      "    autoencoder_loss_train: 1.0205159187316895 - discriminator_loss_train: 1.3257908821105957\n",
      "    autoencoder_loss_val: 1.0410270690917969 - discriminator_loss_val: 1.3903493881225586\n",
      "Starting epoch 661/1000\n",
      "    autoencoder_loss_train: 1.0272860527038574 - discriminator_loss_train: 1.3241736888885498\n",
      "    autoencoder_loss_val: 1.0465294122695923 - discriminator_loss_val: 1.389554500579834\n",
      "Starting epoch 662/1000\n",
      "    autoencoder_loss_train: 1.0369163751602173 - discriminator_loss_train: 1.3223280906677246\n",
      "    autoencoder_loss_val: 1.0548675060272217 - discriminator_loss_val: 1.3885524272918701\n",
      "Starting epoch 663/1000\n",
      "    autoencoder_loss_train: 1.0440168380737305 - discriminator_loss_train: 1.3200616836547852\n",
      "    autoencoder_loss_val: 1.060755968093872 - discriminator_loss_val: 1.3867347240447998\n",
      "Starting epoch 664/1000\n",
      "    autoencoder_loss_train: 1.044935941696167 - discriminator_loss_train: 1.3169949054718018\n",
      "    autoencoder_loss_val: 1.0610849857330322 - discriminator_loss_val: 1.3833949565887451\n",
      "Starting epoch 665/1000\n",
      "    autoencoder_loss_train: 1.0395935773849487 - discriminator_loss_train: 1.315577745437622\n",
      "    autoencoder_loss_val: 1.0555636882781982 - discriminator_loss_val: 1.3808703422546387\n",
      "Starting epoch 666/1000\n",
      "    autoencoder_loss_train: 1.0326106548309326 - discriminator_loss_train: 1.3163762092590332\n",
      "    autoencoder_loss_val: 1.048534631729126 - discriminator_loss_val: 1.380238652229309\n",
      "Starting epoch 667/1000\n",
      "    autoencoder_loss_train: 1.0347552299499512 - discriminator_loss_train: 1.3149755001068115\n",
      "    autoencoder_loss_val: 1.0502955913543701 - discriminator_loss_val: 1.3787205219268799\n",
      "Starting epoch 668/1000\n",
      "    autoencoder_loss_train: 1.0516642332077026 - discriminator_loss_train: 1.3093500137329102\n",
      "    autoencoder_loss_val: 1.0658071041107178 - discriminator_loss_val: 1.3745143413543701\n",
      "Starting epoch 669/1000\n",
      "    autoencoder_loss_train: 1.0739562511444092 - discriminator_loss_train: 1.3040876388549805\n",
      "    autoencoder_loss_val: 1.086810827255249 - discriminator_loss_val: 1.3707034587860107\n",
      "Starting epoch 670/1000\n",
      "    autoencoder_loss_train: 1.0921289920806885 - discriminator_loss_train: 1.2989230155944824\n",
      "    autoencoder_loss_val: 1.103968620300293 - discriminator_loss_val: 1.366452932357788\n",
      "Starting epoch 671/1000\n",
      "    autoencoder_loss_train: 1.1020095348358154 - discriminator_loss_train: 1.2947431802749634\n",
      "    autoencoder_loss_val: 1.1130712032318115 - discriminator_loss_val: 1.3626539707183838\n",
      "Starting epoch 672/1000\n",
      "    autoencoder_loss_train: 1.1107327938079834 - discriminator_loss_train: 1.2912020683288574\n",
      "    autoencoder_loss_val: 1.1212458610534668 - discriminator_loss_val: 1.3594977855682373\n",
      "Starting epoch 673/1000\n",
      "    autoencoder_loss_train: 1.1176306009292603 - discriminator_loss_train: 1.2867369651794434\n",
      "    autoencoder_loss_val: 1.1276776790618896 - discriminator_loss_val: 1.3553121089935303\n",
      "Starting epoch 674/1000\n",
      "    autoencoder_loss_train: 1.1201395988464355 - discriminator_loss_train: 1.2828559875488281\n",
      "    autoencoder_loss_val: 1.1299409866333008 - discriminator_loss_val: 1.3514924049377441\n",
      "Starting epoch 675/1000\n",
      "    autoencoder_loss_train: 1.128895878791809 - discriminator_loss_train: 1.2777667045593262\n",
      "    autoencoder_loss_val: 1.1379591226577759 - discriminator_loss_val: 1.3468654155731201\n",
      "Starting epoch 676/1000\n",
      "    autoencoder_loss_train: 1.1371219158172607 - discriminator_loss_train: 1.2708189487457275\n",
      "    autoencoder_loss_val: 1.1453030109405518 - discriminator_loss_val: 1.3405958414077759\n",
      "Starting epoch 677/1000\n",
      "    autoencoder_loss_train: 1.148184061050415 - discriminator_loss_train: 1.262436032295227\n",
      "    autoencoder_loss_val: 1.1552107334136963 - discriminator_loss_val: 1.3331360816955566\n",
      "Starting epoch 678/1000\n",
      "    autoencoder_loss_train: 1.1585984230041504 - discriminator_loss_train: 1.2564046382904053\n",
      "    autoencoder_loss_val: 1.1647785902023315 - discriminator_loss_val: 1.3279063701629639\n",
      "Starting epoch 679/1000\n",
      "    autoencoder_loss_train: 1.1617687940597534 - discriminator_loss_train: 1.253343105316162\n",
      "    autoencoder_loss_val: 1.1678926944732666 - discriminator_loss_val: 1.3254865407943726\n",
      "Starting epoch 680/1000\n",
      "    autoencoder_loss_train: 1.1582015752792358 - discriminator_loss_train: 1.2519385814666748\n",
      "    autoencoder_loss_val: 1.1648507118225098 - discriminator_loss_val: 1.3242895603179932\n",
      "Starting epoch 681/1000\n",
      "    autoencoder_loss_train: 1.1550670862197876 - discriminator_loss_train: 1.2523722648620605\n",
      "    autoencoder_loss_val: 1.162731409072876 - discriminator_loss_val: 1.3249270915985107\n",
      "Starting epoch 682/1000\n",
      "    autoencoder_loss_train: 1.1509393453598022 - discriminator_loss_train: 1.2555158138275146\n",
      "    autoencoder_loss_val: 1.1605017185211182 - discriminator_loss_val: 1.3281913995742798\n",
      "Starting epoch 683/1000\n",
      "    autoencoder_loss_train: 1.1423918008804321 - discriminator_loss_train: 1.2600256204605103\n",
      "    autoencoder_loss_val: 1.153963327407837 - discriminator_loss_val: 1.332693338394165\n",
      "Starting epoch 684/1000\n",
      "    autoencoder_loss_train: 1.1418168544769287 - discriminator_loss_train: 1.264340877532959\n",
      "    autoencoder_loss_val: 1.1546120643615723 - discriminator_loss_val: 1.3374159336090088\n",
      "Starting epoch 685/1000\n",
      "    autoencoder_loss_train: 1.1466584205627441 - discriminator_loss_train: 1.2683501243591309\n",
      "    autoencoder_loss_val: 1.1598691940307617 - discriminator_loss_val: 1.342155933380127\n",
      "Starting epoch 686/1000\n",
      "    autoencoder_loss_train: 1.1473307609558105 - discriminator_loss_train: 1.272796392440796\n",
      "    autoencoder_loss_val: 1.1607639789581299 - discriminator_loss_val: 1.3471671342849731\n",
      "Starting epoch 687/1000\n",
      "    autoencoder_loss_train: 1.1429171562194824 - discriminator_loss_train: 1.2789634466171265\n",
      "    autoencoder_loss_val: 1.1565284729003906 - discriminator_loss_val: 1.353195071220398\n",
      "Starting epoch 688/1000\n",
      "    autoencoder_loss_train: 1.1343976259231567 - discriminator_loss_train: 1.2853209972381592\n",
      "    autoencoder_loss_val: 1.1485178470611572 - discriminator_loss_val: 1.359209418296814\n",
      "Starting epoch 689/1000\n",
      "    autoencoder_loss_train: 1.1156599521636963 - discriminator_loss_train: 1.2913165092468262\n",
      "    autoencoder_loss_val: 1.1314479112625122 - discriminator_loss_val: 1.3645410537719727\n",
      "Starting epoch 690/1000\n",
      "    autoencoder_loss_train: 1.1013953685760498 - discriminator_loss_train: 1.2978184223175049\n",
      "    autoencoder_loss_val: 1.1189769506454468 - discriminator_loss_val: 1.3706824779510498\n",
      "Starting epoch 691/1000\n",
      "    autoencoder_loss_train: 1.0913150310516357 - discriminator_loss_train: 1.3048218488693237\n",
      "    autoencoder_loss_val: 1.110332727432251 - discriminator_loss_val: 1.377258062362671\n",
      "Starting epoch 692/1000\n",
      "    autoencoder_loss_train: 1.0889984369277954 - discriminator_loss_train: 1.312103033065796\n",
      "    autoencoder_loss_val: 1.1089415550231934 - discriminator_loss_val: 1.38443922996521\n",
      "Starting epoch 693/1000\n",
      "    autoencoder_loss_train: 1.0901625156402588 - discriminator_loss_train: 1.320563554763794\n",
      "    autoencoder_loss_val: 1.1107889413833618 - discriminator_loss_val: 1.3931143283843994\n",
      "Starting epoch 694/1000\n",
      "    autoencoder_loss_train: 1.0896568298339844 - discriminator_loss_train: 1.3285925388336182\n",
      "    autoencoder_loss_val: 1.1114869117736816 - discriminator_loss_val: 1.4014558792114258\n",
      "Starting epoch 695/1000\n",
      "    autoencoder_loss_train: 1.0876761674880981 - discriminator_loss_train: 1.3356575965881348\n",
      "    autoencoder_loss_val: 1.1107667684555054 - discriminator_loss_val: 1.4086886644363403\n",
      "Starting epoch 696/1000\n",
      "    autoencoder_loss_train: 1.081945538520813 - discriminator_loss_train: 1.340825080871582\n",
      "    autoencoder_loss_val: 1.1066949367523193 - discriminator_loss_val: 1.413825273513794\n",
      "Starting epoch 697/1000\n",
      "    autoencoder_loss_train: 1.0741735696792603 - discriminator_loss_train: 1.3447023630142212\n",
      "    autoencoder_loss_val: 1.100553274154663 - discriminator_loss_val: 1.4176537990570068\n",
      "Starting epoch 698/1000\n",
      "    autoencoder_loss_train: 1.0673363208770752 - discriminator_loss_train: 1.3480769395828247\n",
      "    autoencoder_loss_val: 1.0946394205093384 - discriminator_loss_val: 1.4204931259155273\n",
      "Starting epoch 699/1000\n",
      "    autoencoder_loss_train: 1.067469596862793 - discriminator_loss_train: 1.3501315116882324\n",
      "    autoencoder_loss_val: 1.09513521194458 - discriminator_loss_val: 1.4220908880233765\n",
      "Starting epoch 700/1000\n",
      "    autoencoder_loss_train: 1.0709213018417358 - discriminator_loss_train: 1.3514814376831055\n",
      "    autoencoder_loss_val: 1.0988370180130005 - discriminator_loss_val: 1.4231455326080322\n",
      "Starting epoch 701/1000\n",
      "    autoencoder_loss_train: 1.0756001472473145 - discriminator_loss_train: 1.350513219833374\n",
      "    autoencoder_loss_val: 1.1039594411849976 - discriminator_loss_val: 1.422090768814087\n",
      "Starting epoch 702/1000\n",
      "    autoencoder_loss_train: 1.0802921056747437 - discriminator_loss_train: 1.3497419357299805\n",
      "    autoencoder_loss_val: 1.1085858345031738 - discriminator_loss_val: 1.4214130640029907\n",
      "Starting epoch 703/1000\n",
      "    autoencoder_loss_train: 1.090620994567871 - discriminator_loss_train: 1.349798321723938\n",
      "    autoencoder_loss_val: 1.1184858083724976 - discriminator_loss_val: 1.4220647811889648\n",
      "Starting epoch 704/1000\n",
      "    autoencoder_loss_train: 1.1125121116638184 - discriminator_loss_train: 1.3487462997436523\n",
      "    autoencoder_loss_val: 1.1407651901245117 - discriminator_loss_val: 1.4222443103790283\n",
      "Starting epoch 705/1000\n",
      "    autoencoder_loss_train: 1.1396957635879517 - discriminator_loss_train: 1.3451380729675293\n",
      "    autoencoder_loss_val: 1.1685607433319092 - discriminator_loss_val: 1.4196845293045044\n",
      "Starting epoch 706/1000\n",
      "    autoencoder_loss_train: 1.1649138927459717 - discriminator_loss_train: 1.340515375137329\n",
      "    autoencoder_loss_val: 1.1938486099243164 - discriminator_loss_val: 1.415977120399475\n",
      "Starting epoch 707/1000\n",
      "    autoencoder_loss_train: 1.1899415254592896 - discriminator_loss_train: 1.3350369930267334\n",
      "    autoencoder_loss_val: 1.2190701961517334 - discriminator_loss_val: 1.411461591720581\n",
      "Starting epoch 708/1000\n",
      "    autoencoder_loss_train: 1.2030434608459473 - discriminator_loss_train: 1.3291223049163818\n",
      "    autoencoder_loss_val: 1.2322678565979004 - discriminator_loss_val: 1.4059629440307617\n",
      "Starting epoch 709/1000\n",
      "    autoencoder_loss_train: 1.2194613218307495 - discriminator_loss_train: 1.3247452974319458\n",
      "    autoencoder_loss_val: 1.2485171556472778 - discriminator_loss_val: 1.4020514488220215\n",
      "Starting epoch 710/1000\n",
      "    autoencoder_loss_train: 1.2382655143737793 - discriminator_loss_train: 1.3217213153839111\n",
      "    autoencoder_loss_val: 1.2667641639709473 - discriminator_loss_val: 1.3995952606201172\n",
      "Starting epoch 711/1000\n",
      "    autoencoder_loss_train: 1.2645424604415894 - discriminator_loss_train: 1.3169912099838257\n",
      "    autoencoder_loss_val: 1.292643666267395 - discriminator_loss_val: 1.3958642482757568\n",
      "Starting epoch 712/1000\n",
      "    autoencoder_loss_train: 1.2732784748077393 - discriminator_loss_train: 1.3126311302185059\n",
      "    autoencoder_loss_val: 1.3007694482803345 - discriminator_loss_val: 1.3913843631744385\n",
      "Starting epoch 713/1000\n",
      "    autoencoder_loss_train: 1.2765634059906006 - discriminator_loss_train: 1.3068376779556274\n",
      "    autoencoder_loss_val: 1.303816795349121 - discriminator_loss_val: 1.3852602243423462\n",
      "Starting epoch 714/1000\n",
      "    autoencoder_loss_train: 1.2810111045837402 - discriminator_loss_train: 1.3002837896347046\n",
      "    autoencoder_loss_val: 1.308510661125183 - discriminator_loss_val: 1.378481149673462\n",
      "Starting epoch 715/1000\n",
      "    autoencoder_loss_train: 1.286918044090271 - discriminator_loss_train: 1.2947793006896973\n",
      "    autoencoder_loss_val: 1.3141191005706787 - discriminator_loss_val: 1.3725003004074097\n",
      "Starting epoch 716/1000\n",
      "    autoencoder_loss_train: 1.2944941520690918 - discriminator_loss_train: 1.2887952327728271\n",
      "    autoencoder_loss_val: 1.3216004371643066 - discriminator_loss_val: 1.366225004196167\n",
      "Starting epoch 717/1000\n",
      "    autoencoder_loss_train: 1.303192377090454 - discriminator_loss_train: 1.2790582180023193\n",
      "    autoencoder_loss_val: 1.3308920860290527 - discriminator_loss_val: 1.3564765453338623\n",
      "Starting epoch 718/1000\n",
      "    autoencoder_loss_train: 1.3176758289337158 - discriminator_loss_train: 1.268972396850586\n",
      "    autoencoder_loss_val: 1.346423625946045 - discriminator_loss_val: 1.3468196392059326\n",
      "Starting epoch 719/1000\n",
      "    autoencoder_loss_train: 1.3291583061218262 - discriminator_loss_train: 1.261078119277954\n",
      "    autoencoder_loss_val: 1.3590692281723022 - discriminator_loss_val: 1.339322566986084\n",
      "Starting epoch 720/1000\n",
      "    autoencoder_loss_train: 1.3304073810577393 - discriminator_loss_train: 1.2566773891448975\n",
      "    autoencoder_loss_val: 1.361372470855713 - discriminator_loss_val: 1.3350179195404053\n",
      "Starting epoch 721/1000\n",
      "    autoencoder_loss_train: 1.3145496845245361 - discriminator_loss_train: 1.2532556056976318\n",
      "    autoencoder_loss_val: 1.3459399938583374 - discriminator_loss_val: 1.3305522203445435\n",
      "Starting epoch 722/1000\n",
      "    autoencoder_loss_train: 1.292827844619751 - discriminator_loss_train: 1.2522943019866943\n",
      "    autoencoder_loss_val: 1.323883056640625 - discriminator_loss_val: 1.3282181024551392\n",
      "Starting epoch 723/1000\n",
      "    autoencoder_loss_train: 1.260717749595642 - discriminator_loss_train: 1.2537580728530884\n",
      "    autoencoder_loss_val: 1.2907130718231201 - discriminator_loss_val: 1.3274685144424438\n",
      "Starting epoch 724/1000\n",
      "    autoencoder_loss_train: 1.235612392425537 - discriminator_loss_train: 1.2575018405914307\n",
      "    autoencoder_loss_val: 1.2643394470214844 - discriminator_loss_val: 1.329493522644043\n",
      "Starting epoch 725/1000\n",
      "    autoencoder_loss_train: 1.2284884452819824 - discriminator_loss_train: 1.2589848041534424\n",
      "    autoencoder_loss_val: 1.2566488981246948 - discriminator_loss_val: 1.3305325508117676\n",
      "Starting epoch 726/1000\n",
      "    autoencoder_loss_train: 1.2245301008224487 - discriminator_loss_train: 1.2612214088439941\n",
      "    autoencoder_loss_val: 1.2518893480300903 - discriminator_loss_val: 1.3326889276504517\n",
      "Starting epoch 727/1000\n",
      "    autoencoder_loss_train: 1.211430311203003 - discriminator_loss_train: 1.263127326965332\n",
      "    autoencoder_loss_val: 1.2387230396270752 - discriminator_loss_val: 1.3341994285583496\n",
      "Starting epoch 728/1000\n",
      "    autoencoder_loss_train: 1.1961627006530762 - discriminator_loss_train: 1.2672020196914673\n",
      "    autoencoder_loss_val: 1.223624587059021 - discriminator_loss_val: 1.3380919694900513\n",
      "Starting epoch 729/1000\n",
      "    autoencoder_loss_train: 1.1808874607086182 - discriminator_loss_train: 1.273240566253662\n",
      "    autoencoder_loss_val: 1.2080144882202148 - discriminator_loss_val: 1.343862533569336\n",
      "Starting epoch 730/1000\n",
      "    autoencoder_loss_train: 1.1552767753601074 - discriminator_loss_train: 1.2823936939239502\n",
      "    autoencoder_loss_val: 1.181675910949707 - discriminator_loss_val: 1.3522748947143555\n",
      "Starting epoch 731/1000\n",
      "    autoencoder_loss_train: 1.1315879821777344 - discriminator_loss_train: 1.291898250579834\n",
      "    autoencoder_loss_val: 1.1569205522537231 - discriminator_loss_val: 1.3607416152954102\n",
      "Starting epoch 732/1000\n",
      "    autoencoder_loss_train: 1.1100561618804932 - discriminator_loss_train: 1.3018779754638672\n",
      "    autoencoder_loss_val: 1.1346170902252197 - discriminator_loss_val: 1.3702552318572998\n",
      "Starting epoch 733/1000\n",
      "    autoencoder_loss_train: 1.0971050262451172 - discriminator_loss_train: 1.3105452060699463\n",
      "    autoencoder_loss_val: 1.1204482316970825 - discriminator_loss_val: 1.3789176940917969\n",
      "Starting epoch 734/1000\n",
      "    autoencoder_loss_train: 1.0862219333648682 - discriminator_loss_train: 1.3192391395568848\n",
      "    autoencoder_loss_val: 1.108130693435669 - discriminator_loss_val: 1.3877240419387817\n",
      "Starting epoch 735/1000\n",
      "    autoencoder_loss_train: 1.0732473134994507 - discriminator_loss_train: 1.3262053728103638\n",
      "    autoencoder_loss_val: 1.0940607786178589 - discriminator_loss_val: 1.3941363096237183\n",
      "Starting epoch 736/1000\n",
      "    autoencoder_loss_train: 1.060051441192627 - discriminator_loss_train: 1.3326504230499268\n",
      "    autoencoder_loss_val: 1.0803284645080566 - discriminator_loss_val: 1.3999640941619873\n",
      "Starting epoch 737/1000\n",
      "    autoencoder_loss_train: 1.0469337701797485 - discriminator_loss_train: 1.3391106128692627\n",
      "    autoencoder_loss_val: 1.0670238733291626 - discriminator_loss_val: 1.4060181379318237\n",
      "Starting epoch 738/1000\n",
      "    autoencoder_loss_train: 1.0451682806015015 - discriminator_loss_train: 1.3451390266418457\n",
      "    autoencoder_loss_val: 1.0653676986694336 - discriminator_loss_val: 1.412419319152832\n",
      "Starting epoch 739/1000\n",
      "    autoencoder_loss_train: 1.0489528179168701 - discriminator_loss_train: 1.349768042564392\n",
      "    autoencoder_loss_val: 1.06899094581604 - discriminator_loss_val: 1.417598843574524\n",
      "Starting epoch 740/1000\n",
      "    autoencoder_loss_train: 1.0552964210510254 - discriminator_loss_train: 1.3513420820236206\n",
      "    autoencoder_loss_val: 1.0750012397766113 - discriminator_loss_val: 1.4194536209106445\n",
      "Starting epoch 741/1000\n",
      "    autoencoder_loss_train: 1.0597147941589355 - discriminator_loss_train: 1.3508878946304321\n",
      "    autoencoder_loss_val: 1.0790526866912842 - discriminator_loss_val: 1.418820858001709\n",
      "Starting epoch 742/1000\n",
      "    autoencoder_loss_train: 1.0665115118026733 - discriminator_loss_train: 1.3492162227630615\n",
      "    autoencoder_loss_val: 1.0855227708816528 - discriminator_loss_val: 1.4172508716583252\n",
      "Starting epoch 743/1000\n",
      "    autoencoder_loss_train: 1.075660228729248 - discriminator_loss_train: 1.3469815254211426\n",
      "    autoencoder_loss_val: 1.0952047109603882 - discriminator_loss_val: 1.4156603813171387\n",
      "Starting epoch 744/1000\n",
      "    autoencoder_loss_train: 1.0800796747207642 - discriminator_loss_train: 1.3446555137634277\n",
      "    autoencoder_loss_val: 1.101204514503479 - discriminator_loss_val: 1.4134559631347656\n",
      "Starting epoch 745/1000\n",
      "    autoencoder_loss_train: 1.0781989097595215 - discriminator_loss_train: 1.3421194553375244\n",
      "    autoencoder_loss_val: 1.100294828414917 - discriminator_loss_val: 1.4106452465057373\n",
      "Starting epoch 746/1000\n",
      "    autoencoder_loss_train: 1.0721752643585205 - discriminator_loss_train: 1.33909273147583\n",
      "    autoencoder_loss_val: 1.0947810411453247 - discriminator_loss_val: 1.40677809715271\n",
      "Starting epoch 747/1000\n",
      "    autoencoder_loss_train: 1.0714709758758545 - discriminator_loss_train: 1.336719036102295\n",
      "    autoencoder_loss_val: 1.0938409566879272 - discriminator_loss_val: 1.403612732887268\n",
      "Starting epoch 748/1000\n",
      "    autoencoder_loss_train: 1.073396921157837 - discriminator_loss_train: 1.3399362564086914\n",
      "    autoencoder_loss_val: 1.0955449342727661 - discriminator_loss_val: 1.4059566259384155\n",
      "Starting epoch 749/1000\n",
      "    autoencoder_loss_train: 1.0815168619155884 - discriminator_loss_train: 1.3438509702682495\n",
      "    autoencoder_loss_val: 1.1036769151687622 - discriminator_loss_val: 1.4094098806381226\n",
      "Starting epoch 750/1000\n",
      "    autoencoder_loss_train: 1.0957175493240356 - discriminator_loss_train: 1.343289852142334\n",
      "    autoencoder_loss_val: 1.1179869174957275 - discriminator_loss_val: 1.4093270301818848\n",
      "Starting epoch 751/1000\n",
      "    autoencoder_loss_train: 1.1082487106323242 - discriminator_loss_train: 1.3402135372161865\n",
      "    autoencoder_loss_val: 1.1316399574279785 - discriminator_loss_val: 1.407268762588501\n",
      "Starting epoch 752/1000\n",
      "    autoencoder_loss_train: 1.1226388216018677 - discriminator_loss_train: 1.3342411518096924\n",
      "    autoencoder_loss_val: 1.1470482349395752 - discriminator_loss_val: 1.4026775360107422\n",
      "Starting epoch 753/1000\n",
      "    autoencoder_loss_train: 1.1354461908340454 - discriminator_loss_train: 1.325693130493164\n",
      "    autoencoder_loss_val: 1.1610374450683594 - discriminator_loss_val: 1.395557165145874\n",
      "Starting epoch 754/1000\n",
      "    autoencoder_loss_train: 1.140675663948059 - discriminator_loss_train: 1.3172905445098877\n",
      "    autoencoder_loss_val: 1.1675512790679932 - discriminator_loss_val: 1.3878389596939087\n",
      "Starting epoch 755/1000\n",
      "    autoencoder_loss_train: 1.1409225463867188 - discriminator_loss_train: 1.3080792427062988\n",
      "    autoencoder_loss_val: 1.1685876846313477 - discriminator_loss_val: 1.3786420822143555\n",
      "Starting epoch 756/1000\n",
      "    autoencoder_loss_train: 1.1508697271347046 - discriminator_loss_train: 1.2969956398010254\n",
      "    autoencoder_loss_val: 1.1778192520141602 - discriminator_loss_val: 1.3683466911315918\n",
      "Starting epoch 757/1000\n",
      "    autoencoder_loss_train: 1.1636425256729126 - discriminator_loss_train: 1.2879078388214111\n",
      "    autoencoder_loss_val: 1.1892139911651611 - discriminator_loss_val: 1.3600654602050781\n",
      "Starting epoch 758/1000\n",
      "    autoencoder_loss_train: 1.1725447177886963 - discriminator_loss_train: 1.28145432472229\n",
      "    autoencoder_loss_val: 1.1969687938690186 - discriminator_loss_val: 1.3541250228881836\n",
      "Starting epoch 759/1000\n",
      "    autoencoder_loss_train: 1.169469952583313 - discriminator_loss_train: 1.2770475149154663\n",
      "    autoencoder_loss_val: 1.193650245666504 - discriminator_loss_val: 1.3497493267059326\n",
      "Starting epoch 760/1000\n",
      "    autoencoder_loss_train: 1.1610010862350464 - discriminator_loss_train: 1.2727701663970947\n",
      "    autoencoder_loss_val: 1.1856528520584106 - discriminator_loss_val: 1.3453059196472168\n",
      "Starting epoch 761/1000\n",
      "    autoencoder_loss_train: 1.1468589305877686 - discriminator_loss_train: 1.2671465873718262\n",
      "    autoencoder_loss_val: 1.1728333234786987 - discriminator_loss_val: 1.3391227722167969\n",
      "Starting epoch 762/1000\n",
      "    autoencoder_loss_train: 1.1375631093978882 - discriminator_loss_train: 1.2639362812042236\n",
      "    autoencoder_loss_val: 1.164409875869751 - discriminator_loss_val: 1.335477590560913\n",
      "Starting epoch 763/1000\n",
      "    autoencoder_loss_train: 1.1335777044296265 - discriminator_loss_train: 1.260634422302246\n",
      "    autoencoder_loss_val: 1.1603691577911377 - discriminator_loss_val: 1.3321119546890259\n",
      "Starting epoch 764/1000\n",
      "    autoencoder_loss_train: 1.1297630071640015 - discriminator_loss_train: 1.259145736694336\n",
      "    autoencoder_loss_val: 1.1564850807189941 - discriminator_loss_val: 1.3308244943618774\n",
      "Starting epoch 765/1000\n",
      "    autoencoder_loss_train: 1.1250333786010742 - discriminator_loss_train: 1.2565500736236572\n",
      "    autoencoder_loss_val: 1.152144193649292 - discriminator_loss_val: 1.3282225131988525\n",
      "Starting epoch 766/1000\n",
      "    autoencoder_loss_train: 1.1294597387313843 - discriminator_loss_train: 1.2461379766464233\n",
      "    autoencoder_loss_val: 1.1573758125305176 - discriminator_loss_val: 1.3180794715881348\n",
      "Starting epoch 767/1000\n",
      "    autoencoder_loss_train: 1.137420892715454 - discriminator_loss_train: 1.2370259761810303\n",
      "    autoencoder_loss_val: 1.1659801006317139 - discriminator_loss_val: 1.309413194656372\n",
      "Starting epoch 768/1000\n",
      "    autoencoder_loss_train: 1.1425564289093018 - discriminator_loss_train: 1.23564875125885\n",
      "    autoencoder_loss_val: 1.1715493202209473 - discriminator_loss_val: 1.3088278770446777\n",
      "Starting epoch 769/1000\n",
      "    autoencoder_loss_train: 1.1430654525756836 - discriminator_loss_train: 1.2411563396453857\n",
      "    autoencoder_loss_val: 1.1721296310424805 - discriminator_loss_val: 1.314976692199707\n",
      "Starting epoch 770/1000\n",
      "    autoencoder_loss_train: 1.1442084312438965 - discriminator_loss_train: 1.2455419301986694\n",
      "    autoencoder_loss_val: 1.1741218566894531 - discriminator_loss_val: 1.3197135925292969\n",
      "Starting epoch 771/1000\n",
      "    autoencoder_loss_train: 1.1498122215270996 - discriminator_loss_train: 1.2483787536621094\n",
      "    autoencoder_loss_val: 1.18115234375 - discriminator_loss_val: 1.3232965469360352\n",
      "Starting epoch 772/1000\n",
      "    autoencoder_loss_train: 1.1439965963363647 - discriminator_loss_train: 1.2566429376602173\n",
      "    autoencoder_loss_val: 1.1767232418060303 - discriminator_loss_val: 1.3317649364471436\n",
      "Starting epoch 773/1000\n",
      "    autoencoder_loss_train: 1.1314678192138672 - discriminator_loss_train: 1.2667784690856934\n",
      "    autoencoder_loss_val: 1.1652299165725708 - discriminator_loss_val: 1.341648817062378\n",
      "Starting epoch 774/1000\n",
      "    autoencoder_loss_train: 1.1175916194915771 - discriminator_loss_train: 1.278331995010376\n",
      "    autoencoder_loss_val: 1.1516252756118774 - discriminator_loss_val: 1.3526915311813354\n",
      "Starting epoch 775/1000\n",
      "    autoencoder_loss_train: 1.1100742816925049 - discriminator_loss_train: 1.287609338760376\n",
      "    autoencoder_loss_val: 1.1440587043762207 - discriminator_loss_val: 1.3618360757827759\n",
      "Starting epoch 776/1000\n",
      "    autoencoder_loss_train: 1.1031773090362549 - discriminator_loss_train: 1.299269676208496\n",
      "    autoencoder_loss_val: 1.1370269060134888 - discriminator_loss_val: 1.3737940788269043\n",
      "Starting epoch 777/1000\n",
      "    autoencoder_loss_train: 1.1009188890457153 - discriminator_loss_train: 1.312711477279663\n",
      "    autoencoder_loss_val: 1.1341077089309692 - discriminator_loss_val: 1.3880540132522583\n",
      "Starting epoch 778/1000\n",
      "    autoencoder_loss_train: 1.10189950466156 - discriminator_loss_train: 1.3240820169448853\n",
      "    autoencoder_loss_val: 1.13463294506073 - discriminator_loss_val: 1.4009783267974854\n",
      "Starting epoch 779/1000\n",
      "    autoencoder_loss_train: 1.1072156429290771 - discriminator_loss_train: 1.3362350463867188\n",
      "    autoencoder_loss_val: 1.139748215675354 - discriminator_loss_val: 1.4149057865142822\n",
      "Starting epoch 780/1000\n",
      "    autoencoder_loss_train: 1.1102001667022705 - discriminator_loss_train: 1.3489494323730469\n",
      "    autoencoder_loss_val: 1.1431034803390503 - discriminator_loss_val: 1.429060459136963\n",
      "Starting epoch 781/1000\n",
      "    autoencoder_loss_train: 1.1081894636154175 - discriminator_loss_train: 1.3619089126586914\n",
      "    autoencoder_loss_val: 1.1412928104400635 - discriminator_loss_val: 1.442671298980713\n",
      "Starting epoch 782/1000\n",
      "    autoencoder_loss_train: 1.1010152101516724 - discriminator_loss_train: 1.373689889907837\n",
      "    autoencoder_loss_val: 1.133547067642212 - discriminator_loss_val: 1.4540443420410156\n",
      "Starting epoch 783/1000\n",
      "    autoencoder_loss_train: 1.1004576683044434 - discriminator_loss_train: 1.3832792043685913\n",
      "    autoencoder_loss_val: 1.1324989795684814 - discriminator_loss_val: 1.4634101390838623\n",
      "Starting epoch 784/1000\n",
      "    autoencoder_loss_train: 1.0986852645874023 - discriminator_loss_train: 1.3894587755203247\n",
      "    autoencoder_loss_val: 1.1313320398330688 - discriminator_loss_val: 1.4693918228149414\n",
      "Starting epoch 785/1000\n",
      "    autoencoder_loss_train: 1.0995566844940186 - discriminator_loss_train: 1.3918681144714355\n",
      "    autoencoder_loss_val: 1.1331864595413208 - discriminator_loss_val: 1.4714882373809814\n",
      "Starting epoch 786/1000\n",
      "    autoencoder_loss_train: 1.1047204732894897 - discriminator_loss_train: 1.3898468017578125\n",
      "    autoencoder_loss_val: 1.1384128332138062 - discriminator_loss_val: 1.4689853191375732\n",
      "Starting epoch 787/1000\n",
      "    autoencoder_loss_train: 1.1207565069198608 - discriminator_loss_train: 1.3844883441925049\n",
      "    autoencoder_loss_val: 1.1540606021881104 - discriminator_loss_val: 1.4634534120559692\n",
      "Starting epoch 788/1000\n",
      "    autoencoder_loss_train: 1.1361973285675049 - discriminator_loss_train: 1.3767142295837402\n",
      "    autoencoder_loss_val: 1.1692713499069214 - discriminator_loss_val: 1.4557141065597534\n",
      "Starting epoch 789/1000\n",
      "    autoencoder_loss_train: 1.1440743207931519 - discriminator_loss_train: 1.367842197418213\n",
      "    autoencoder_loss_val: 1.177602767944336 - discriminator_loss_val: 1.446887731552124\n",
      "Starting epoch 790/1000\n",
      "    autoencoder_loss_train: 1.1458628177642822 - discriminator_loss_train: 1.3587157726287842\n",
      "    autoencoder_loss_val: 1.1800041198730469 - discriminator_loss_val: 1.4376959800720215\n",
      "Starting epoch 791/1000\n",
      "    autoencoder_loss_train: 1.1500657796859741 - discriminator_loss_train: 1.3493168354034424\n",
      "    autoencoder_loss_val: 1.1844806671142578 - discriminator_loss_val: 1.4277851581573486\n",
      "Starting epoch 792/1000\n",
      "    autoencoder_loss_train: 1.1657840013504028 - discriminator_loss_train: 1.3365124464035034\n",
      "    autoencoder_loss_val: 1.1992690563201904 - discriminator_loss_val: 1.4152288436889648\n",
      "Starting epoch 793/1000\n",
      "    autoencoder_loss_train: 1.1877236366271973 - discriminator_loss_train: 1.3245937824249268\n",
      "    autoencoder_loss_val: 1.2199311256408691 - discriminator_loss_val: 1.4039294719696045\n",
      "Starting epoch 794/1000\n",
      "    autoencoder_loss_train: 1.209458351135254 - discriminator_loss_train: 1.3114501237869263\n",
      "    autoencoder_loss_val: 1.2404977083206177 - discriminator_loss_val: 1.39071786403656\n",
      "Starting epoch 795/1000\n",
      "    autoencoder_loss_train: 1.2257729768753052 - discriminator_loss_train: 1.2957890033721924\n",
      "    autoencoder_loss_val: 1.2562034130096436 - discriminator_loss_val: 1.3744537830352783\n",
      "Starting epoch 796/1000\n",
      "    autoencoder_loss_train: 1.241442322731018 - discriminator_loss_train: 1.2827576398849487\n",
      "    autoencoder_loss_val: 1.2717080116271973 - discriminator_loss_val: 1.3610484600067139\n",
      "Starting epoch 797/1000\n",
      "    autoencoder_loss_train: 1.2622244358062744 - discriminator_loss_train: 1.274634838104248\n",
      "    autoencoder_loss_val: 1.2923146486282349 - discriminator_loss_val: 1.3530988693237305\n",
      "Starting epoch 798/1000\n",
      "    autoencoder_loss_train: 1.2829986810684204 - discriminator_loss_train: 1.2679297924041748\n",
      "    autoencoder_loss_val: 1.3127472400665283 - discriminator_loss_val: 1.3462141752243042\n",
      "Starting epoch 799/1000\n",
      "    autoencoder_loss_train: 1.2922406196594238 - discriminator_loss_train: 1.2637498378753662\n",
      "    autoencoder_loss_val: 1.3211846351623535 - discriminator_loss_val: 1.3414753675460815\n",
      "Starting epoch 800/1000\n",
      "    autoencoder_loss_train: 1.296988606452942 - discriminator_loss_train: 1.264268159866333\n",
      "    autoencoder_loss_val: 1.3242735862731934 - discriminator_loss_val: 1.341907024383545\n",
      "Starting epoch 801/1000\n",
      "    autoencoder_loss_train: 1.298616886138916 - discriminator_loss_train: 1.2677863836288452\n",
      "    autoencoder_loss_val: 1.3236126899719238 - discriminator_loss_val: 1.3454654216766357\n",
      "Starting epoch 802/1000\n",
      "    autoencoder_loss_train: 1.294325590133667 - discriminator_loss_train: 1.2738416194915771\n",
      "    autoencoder_loss_val: 1.3177294731140137 - discriminator_loss_val: 1.3516674041748047\n",
      "Starting epoch 803/1000\n",
      "    autoencoder_loss_train: 1.2766999006271362 - discriminator_loss_train: 1.281779408454895\n",
      "    autoencoder_loss_val: 1.2991671562194824 - discriminator_loss_val: 1.3590929508209229\n",
      "Starting epoch 804/1000\n",
      "    autoencoder_loss_train: 1.25596022605896 - discriminator_loss_train: 1.2890093326568604\n",
      "    autoencoder_loss_val: 1.2782917022705078 - discriminator_loss_val: 1.3655965328216553\n",
      "Starting epoch 805/1000\n",
      "    autoencoder_loss_train: 1.2375437021255493 - discriminator_loss_train: 1.2958686351776123\n",
      "    autoencoder_loss_val: 1.25985848903656 - discriminator_loss_val: 1.3714544773101807\n",
      "Starting epoch 806/1000\n",
      "    autoencoder_loss_train: 1.2284752130508423 - discriminator_loss_train: 1.300823450088501\n",
      "    autoencoder_loss_val: 1.250056266784668 - discriminator_loss_val: 1.3756146430969238\n",
      "Starting epoch 807/1000\n",
      "    autoencoder_loss_train: 1.2165336608886719 - discriminator_loss_train: 1.3026401996612549\n",
      "    autoencoder_loss_val: 1.2372130155563354 - discriminator_loss_val: 1.375962257385254\n",
      "Starting epoch 808/1000\n",
      "    autoencoder_loss_train: 1.2023247480392456 - discriminator_loss_train: 1.3052195310592651\n",
      "    autoencoder_loss_val: 1.2214953899383545 - discriminator_loss_val: 1.3768677711486816\n",
      "Starting epoch 809/1000\n",
      "    autoencoder_loss_train: 1.1857397556304932 - discriminator_loss_train: 1.30330491065979\n",
      "    autoencoder_loss_val: 1.2039793729782104 - discriminator_loss_val: 1.3726669549942017\n",
      "Starting epoch 810/1000\n",
      "    autoencoder_loss_train: 1.1647918224334717 - discriminator_loss_train: 1.3006113767623901\n",
      "    autoencoder_loss_val: 1.1824020147323608 - discriminator_loss_val: 1.367332100868225\n",
      "Starting epoch 811/1000\n",
      "    autoencoder_loss_train: 1.141457438468933 - discriminator_loss_train: 1.2965826988220215\n",
      "    autoencoder_loss_val: 1.1590520143508911 - discriminator_loss_val: 1.3605793714523315\n",
      "Starting epoch 812/1000\n",
      "    autoencoder_loss_train: 1.1248559951782227 - discriminator_loss_train: 1.2943308353424072\n",
      "    autoencoder_loss_val: 1.141993761062622 - discriminator_loss_val: 1.3556132316589355\n",
      "Starting epoch 813/1000\n",
      "    autoencoder_loss_train: 1.1135627031326294 - discriminator_loss_train: 1.2932798862457275\n",
      "    autoencoder_loss_val: 1.130759358406067 - discriminator_loss_val: 1.352139949798584\n",
      "Starting epoch 814/1000\n",
      "    autoencoder_loss_train: 1.1092853546142578 - discriminator_loss_train: 1.2940075397491455\n",
      "    autoencoder_loss_val: 1.1263999938964844 - discriminator_loss_val: 1.3509447574615479\n",
      "Starting epoch 815/1000\n",
      "    autoencoder_loss_train: 1.1087286472320557 - discriminator_loss_train: 1.2959926128387451\n",
      "    autoencoder_loss_val: 1.12417733669281 - discriminator_loss_val: 1.3513810634613037\n",
      "Starting epoch 816/1000\n",
      "    autoencoder_loss_train: 1.114999532699585 - discriminator_loss_train: 1.2972420454025269\n",
      "    autoencoder_loss_val: 1.1285984516143799 - discriminator_loss_val: 1.351602554321289\n",
      "Starting epoch 817/1000\n",
      "    autoencoder_loss_train: 1.1236166954040527 - discriminator_loss_train: 1.2967110872268677\n",
      "    autoencoder_loss_val: 1.136389136314392 - discriminator_loss_val: 1.3500635623931885\n",
      "Starting epoch 818/1000\n",
      "    autoencoder_loss_train: 1.1301774978637695 - discriminator_loss_train: 1.2967073917388916\n",
      "    autoencoder_loss_val: 1.143779993057251 - discriminator_loss_val: 1.349501609802246\n",
      "Starting epoch 819/1000\n",
      "    autoencoder_loss_train: 1.1340234279632568 - discriminator_loss_train: 1.2986640930175781\n",
      "    autoencoder_loss_val: 1.1470494270324707 - discriminator_loss_val: 1.3513803482055664\n",
      "Starting epoch 820/1000\n",
      "    autoencoder_loss_train: 1.1395293474197388 - discriminator_loss_train: 1.2996889352798462\n",
      "    autoencoder_loss_val: 1.1507327556610107 - discriminator_loss_val: 1.3525649309158325\n",
      "Starting epoch 821/1000\n",
      "    autoencoder_loss_train: 1.147118330001831 - discriminator_loss_train: 1.300180435180664\n",
      "    autoencoder_loss_val: 1.1566169261932373 - discriminator_loss_val: 1.3533320426940918\n",
      "Starting epoch 822/1000\n",
      "    autoencoder_loss_train: 1.1550712585449219 - discriminator_loss_train: 1.2987391948699951\n",
      "    autoencoder_loss_val: 1.1634678840637207 - discriminator_loss_val: 1.3525409698486328\n",
      "Starting epoch 823/1000\n",
      "    autoencoder_loss_train: 1.164554238319397 - discriminator_loss_train: 1.2947888374328613\n",
      "    autoencoder_loss_val: 1.172027349472046 - discriminator_loss_val: 1.3497239351272583\n",
      "Starting epoch 824/1000\n",
      "    autoencoder_loss_train: 1.1756675243377686 - discriminator_loss_train: 1.2916510105133057\n",
      "    autoencoder_loss_val: 1.18271005153656 - discriminator_loss_val: 1.3477907180786133\n",
      "Starting epoch 825/1000\n",
      "    autoencoder_loss_train: 1.1882705688476562 - discriminator_loss_train: 1.291983962059021\n",
      "    autoencoder_loss_val: 1.1943833827972412 - discriminator_loss_val: 1.3488152027130127\n",
      "Starting epoch 826/1000\n",
      "    autoencoder_loss_train: 1.2076958417892456 - discriminator_loss_train: 1.2887167930603027\n",
      "    autoencoder_loss_val: 1.211980938911438 - discriminator_loss_val: 1.3465707302093506\n",
      "Starting epoch 827/1000\n",
      "    autoencoder_loss_train: 1.2370988130569458 - discriminator_loss_train: 1.2816781997680664\n",
      "    autoencoder_loss_val: 1.238950490951538 - discriminator_loss_val: 1.341160774230957\n",
      "Starting epoch 828/1000\n",
      "    autoencoder_loss_train: 1.2726967334747314 - discriminator_loss_train: 1.2774015665054321\n",
      "    autoencoder_loss_val: 1.2722628116607666 - discriminator_loss_val: 1.338362216949463\n",
      "Starting epoch 829/1000\n",
      "    autoencoder_loss_train: 1.3072565793991089 - discriminator_loss_train: 1.2734477519989014\n",
      "    autoencoder_loss_val: 1.3056560754776 - discriminator_loss_val: 1.3356190919876099\n",
      "Starting epoch 830/1000\n",
      "    autoencoder_loss_train: 1.3393802642822266 - discriminator_loss_train: 1.269070029258728\n",
      "    autoencoder_loss_val: 1.3371813297271729 - discriminator_loss_val: 1.332112193107605\n",
      "Starting epoch 831/1000\n",
      "    autoencoder_loss_train: 1.3738210201263428 - discriminator_loss_train: 1.2599494457244873\n",
      "    autoencoder_loss_val: 1.3706132173538208 - discriminator_loss_val: 1.3233572244644165\n",
      "Starting epoch 832/1000\n",
      "    autoencoder_loss_train: 1.3988800048828125 - discriminator_loss_train: 1.2585365772247314\n",
      "    autoencoder_loss_val: 1.3940025568008423 - discriminator_loss_val: 1.3217542171478271\n",
      "Starting epoch 833/1000\n",
      "    autoencoder_loss_train: 1.4178029298782349 - discriminator_loss_train: 1.254268765449524\n",
      "    autoencoder_loss_val: 1.4117640256881714 - discriminator_loss_val: 1.3170547485351562\n",
      "Starting epoch 834/1000\n",
      "    autoencoder_loss_train: 1.426918625831604 - discriminator_loss_train: 1.2468961477279663\n",
      "    autoencoder_loss_val: 1.4204075336456299 - discriminator_loss_val: 1.309700846672058\n",
      "Starting epoch 835/1000\n",
      "    autoencoder_loss_train: 1.4236366748809814 - discriminator_loss_train: 1.2371225357055664\n",
      "    autoencoder_loss_val: 1.4176281690597534 - discriminator_loss_val: 1.3001387119293213\n",
      "Starting epoch 836/1000\n",
      "    autoencoder_loss_train: 1.4171593189239502 - discriminator_loss_train: 1.2255516052246094\n",
      "    autoencoder_loss_val: 1.4121358394622803 - discriminator_loss_val: 1.2890877723693848\n",
      "Starting epoch 837/1000\n",
      "    autoencoder_loss_train: 1.4007132053375244 - discriminator_loss_train: 1.2154005765914917\n",
      "    autoencoder_loss_val: 1.3976244926452637 - discriminator_loss_val: 1.279093861579895\n",
      "Starting epoch 838/1000\n",
      "    autoencoder_loss_train: 1.3893871307373047 - discriminator_loss_train: 1.2062433958053589\n",
      "    autoencoder_loss_val: 1.3879647254943848 - discriminator_loss_val: 1.2703732252120972\n",
      "Starting epoch 839/1000\n",
      "    autoencoder_loss_train: 1.379408597946167 - discriminator_loss_train: 1.2074555158615112\n",
      "    autoencoder_loss_val: 1.3785526752471924 - discriminator_loss_val: 1.2719452381134033\n",
      "Starting epoch 840/1000\n",
      "    autoencoder_loss_train: 1.3471848964691162 - discriminator_loss_train: 1.210401177406311\n",
      "    autoencoder_loss_val: 1.3483545780181885 - discriminator_loss_val: 1.2744988203048706\n",
      "Starting epoch 841/1000\n",
      "    autoencoder_loss_train: 1.2916674613952637 - discriminator_loss_train: 1.2191565036773682\n",
      "    autoencoder_loss_val: 1.2964622974395752 - discriminator_loss_val: 1.282411813735962\n",
      "Starting epoch 842/1000\n",
      "    autoencoder_loss_train: 1.2453502416610718 - discriminator_loss_train: 1.2338895797729492\n",
      "    autoencoder_loss_val: 1.25272798538208 - discriminator_loss_val: 1.296390414237976\n",
      "Starting epoch 843/1000\n",
      "    autoencoder_loss_train: 1.2188587188720703 - discriminator_loss_train: 1.2473713159561157\n",
      "    autoencoder_loss_val: 1.2271020412445068 - discriminator_loss_val: 1.3098500967025757\n",
      "Starting epoch 844/1000\n",
      "    autoencoder_loss_train: 1.2000620365142822 - discriminator_loss_train: 1.261796236038208\n",
      "    autoencoder_loss_val: 1.2083542346954346 - discriminator_loss_val: 1.324698567390442\n",
      "Starting epoch 845/1000\n",
      "    autoencoder_loss_train: 1.169442057609558 - discriminator_loss_train: 1.2781727313995361\n",
      "    autoencoder_loss_val: 1.1792258024215698 - discriminator_loss_val: 1.3413805961608887\n",
      "Starting epoch 846/1000\n",
      "    autoencoder_loss_train: 1.1441946029663086 - discriminator_loss_train: 1.2965223789215088\n",
      "    autoencoder_loss_val: 1.1547247171401978 - discriminator_loss_val: 1.3598413467407227\n",
      "Starting epoch 847/1000\n",
      "    autoencoder_loss_train: 1.1256049871444702 - discriminator_loss_train: 1.3148797750473022\n",
      "    autoencoder_loss_val: 1.1368939876556396 - discriminator_loss_val: 1.377814769744873\n",
      "Starting epoch 848/1000\n",
      "    autoencoder_loss_train: 1.1079939603805542 - discriminator_loss_train: 1.333078145980835\n",
      "    autoencoder_loss_val: 1.1207919120788574 - discriminator_loss_val: 1.39534592628479\n",
      "Starting epoch 849/1000\n",
      "    autoencoder_loss_train: 1.1049599647521973 - discriminator_loss_train: 1.350749135017395\n",
      "    autoencoder_loss_val: 1.1178457736968994 - discriminator_loss_val: 1.4120171070098877\n",
      "Starting epoch 850/1000\n",
      "    autoencoder_loss_train: 1.1046981811523438 - discriminator_loss_train: 1.369668960571289\n",
      "    autoencoder_loss_val: 1.1172523498535156 - discriminator_loss_val: 1.4294707775115967\n",
      "Starting epoch 851/1000\n",
      "    autoencoder_loss_train: 1.1121940612792969 - discriminator_loss_train: 1.386960744857788\n",
      "    autoencoder_loss_val: 1.124758005142212 - discriminator_loss_val: 1.4449925422668457\n",
      "Starting epoch 852/1000\n",
      "    autoencoder_loss_train: 1.125108242034912 - discriminator_loss_train: 1.4000495672225952\n",
      "    autoencoder_loss_val: 1.1383095979690552 - discriminator_loss_val: 1.4561033248901367\n",
      "Starting epoch 853/1000\n",
      "    autoencoder_loss_train: 1.1445894241333008 - discriminator_loss_train: 1.4081734418869019\n",
      "    autoencoder_loss_val: 1.1585139036178589 - discriminator_loss_val: 1.4625294208526611\n",
      "Starting epoch 854/1000\n",
      "    autoencoder_loss_train: 1.159621238708496 - discriminator_loss_train: 1.4212923049926758\n",
      "    autoencoder_loss_val: 1.173307180404663 - discriminator_loss_val: 1.4740511178970337\n",
      "Starting epoch 855/1000\n",
      "    autoencoder_loss_train: 1.1715214252471924 - discriminator_loss_train: 1.4371100664138794\n",
      "    autoencoder_loss_val: 1.184694528579712 - discriminator_loss_val: 1.488619327545166\n",
      "Starting epoch 856/1000\n",
      "    autoencoder_loss_train: 1.1757259368896484 - discriminator_loss_train: 1.455489158630371\n",
      "    autoencoder_loss_val: 1.1880306005477905 - discriminator_loss_val: 1.5064780712127686\n",
      "Starting epoch 857/1000\n",
      "    autoencoder_loss_train: 1.1816986799240112 - discriminator_loss_train: 1.471785068511963\n",
      "    autoencoder_loss_val: 1.193044900894165 - discriminator_loss_val: 1.5230779647827148\n",
      "Starting epoch 858/1000\n",
      "    autoencoder_loss_train: 1.1988961696624756 - discriminator_loss_train: 1.4727447032928467\n",
      "    autoencoder_loss_val: 1.210096836090088 - discriminator_loss_val: 1.524888038635254\n",
      "Starting epoch 859/1000\n",
      "    autoencoder_loss_train: 1.2160379886627197 - discriminator_loss_train: 1.463045358657837\n",
      "    autoencoder_loss_val: 1.2276785373687744 - discriminator_loss_val: 1.5163273811340332\n",
      "Starting epoch 860/1000\n",
      "    autoencoder_loss_train: 1.2257918119430542 - discriminator_loss_train: 1.4577140808105469\n",
      "    autoencoder_loss_val: 1.2372500896453857 - discriminator_loss_val: 1.5123471021652222\n",
      "Starting epoch 861/1000\n",
      "    autoencoder_loss_train: 1.2266321182250977 - discriminator_loss_train: 1.455070972442627\n",
      "    autoencoder_loss_val: 1.237654447555542 - discriminator_loss_val: 1.5115282535552979\n",
      "Starting epoch 862/1000\n",
      "    autoencoder_loss_train: 1.2313885688781738 - discriminator_loss_train: 1.4506440162658691\n",
      "    autoencoder_loss_val: 1.2423686981201172 - discriminator_loss_val: 1.5099027156829834\n",
      "Starting epoch 863/1000\n",
      "    autoencoder_loss_train: 1.2467044591903687 - discriminator_loss_train: 1.4437994956970215\n",
      "    autoencoder_loss_val: 1.257650375366211 - discriminator_loss_val: 1.5066267251968384\n",
      "Starting epoch 864/1000\n",
      "    autoencoder_loss_train: 1.2552499771118164 - discriminator_loss_train: 1.4377658367156982\n",
      "    autoencoder_loss_val: 1.2664921283721924 - discriminator_loss_val: 1.5037875175476074\n",
      "Starting epoch 865/1000\n",
      "    autoencoder_loss_train: 1.2546463012695312 - discriminator_loss_train: 1.4308695793151855\n",
      "    autoencoder_loss_val: 1.2662763595581055 - discriminator_loss_val: 1.4995732307434082\n",
      "Starting epoch 866/1000\n",
      "    autoencoder_loss_train: 1.2511101961135864 - discriminator_loss_train: 1.4156662225723267\n",
      "    autoencoder_loss_val: 1.2636244297027588 - discriminator_loss_val: 1.486422061920166\n",
      "Starting epoch 867/1000\n",
      "    autoencoder_loss_train: 1.2315199375152588 - discriminator_loss_train: 1.396721601486206\n",
      "    autoencoder_loss_val: 1.2454724311828613 - discriminator_loss_val: 1.4684011936187744\n",
      "Starting epoch 868/1000\n",
      "    autoencoder_loss_train: 1.2090991735458374 - discriminator_loss_train: 1.3807473182678223\n",
      "    autoencoder_loss_val: 1.22383451461792 - discriminator_loss_val: 1.4531522989273071\n",
      "Starting epoch 869/1000\n",
      "    autoencoder_loss_train: 1.2077065706253052 - discriminator_loss_train: 1.3679206371307373\n",
      "    autoencoder_loss_val: 1.221964955329895 - discriminator_loss_val: 1.4422287940979004\n",
      "Starting epoch 870/1000\n",
      "    autoencoder_loss_train: 1.208611249923706 - discriminator_loss_train: 1.3582730293273926\n",
      "    autoencoder_loss_val: 1.2223970890045166 - discriminator_loss_val: 1.4340214729309082\n",
      "Starting epoch 871/1000\n",
      "    autoencoder_loss_train: 1.200474739074707 - discriminator_loss_train: 1.3450579643249512\n",
      "    autoencoder_loss_val: 1.2150212526321411 - discriminator_loss_val: 1.4204998016357422\n",
      "Starting epoch 872/1000\n",
      "    autoencoder_loss_train: 1.181653380393982 - discriminator_loss_train: 1.330672264099121\n",
      "    autoencoder_loss_val: 1.1975221633911133 - discriminator_loss_val: 1.40433669090271\n",
      "Starting epoch 873/1000\n",
      "    autoencoder_loss_train: 1.1741943359375 - discriminator_loss_train: 1.317653775215149\n",
      "    autoencoder_loss_val: 1.190617561340332 - discriminator_loss_val: 1.3900184631347656\n",
      "Starting epoch 874/1000\n",
      "    autoencoder_loss_train: 1.1761268377304077 - discriminator_loss_train: 1.3074061870574951\n",
      "    autoencoder_loss_val: 1.1923060417175293 - discriminator_loss_val: 1.3787462711334229\n",
      "Starting epoch 875/1000\n",
      "    autoencoder_loss_train: 1.1928181648254395 - discriminator_loss_train: 1.3018579483032227\n",
      "    autoencoder_loss_val: 1.2073588371276855 - discriminator_loss_val: 1.373070240020752\n",
      "Starting epoch 876/1000\n",
      "    autoencoder_loss_train: 1.2085723876953125 - discriminator_loss_train: 1.2987219095230103\n",
      "    autoencoder_loss_val: 1.221377968788147 - discriminator_loss_val: 1.3692703247070312\n",
      "Starting epoch 877/1000\n",
      "    autoencoder_loss_train: 1.2018519639968872 - discriminator_loss_train: 1.294100284576416\n",
      "    autoencoder_loss_val: 1.2145466804504395 - discriminator_loss_val: 1.3628461360931396\n",
      "Starting epoch 878/1000\n",
      "    autoencoder_loss_train: 1.1896848678588867 - discriminator_loss_train: 1.2938194274902344\n",
      "    autoencoder_loss_val: 1.202856421470642 - discriminator_loss_val: 1.35988450050354\n",
      "Starting epoch 879/1000\n",
      "    autoencoder_loss_train: 1.175779104232788 - discriminator_loss_train: 1.3013484477996826\n",
      "    autoencoder_loss_val: 1.188701868057251 - discriminator_loss_val: 1.3638179302215576\n",
      "Starting epoch 880/1000\n",
      "    autoencoder_loss_train: 1.157410740852356 - discriminator_loss_train: 1.3184759616851807\n",
      "    autoencoder_loss_val: 1.169092059135437 - discriminator_loss_val: 1.3778095245361328\n",
      "Starting epoch 881/1000\n",
      "    autoencoder_loss_train: 1.1286464929580688 - discriminator_loss_train: 1.3437364101409912\n",
      "    autoencoder_loss_val: 1.1392310857772827 - discriminator_loss_val: 1.4000067710876465\n",
      "Starting epoch 882/1000\n",
      "    autoencoder_loss_train: 1.099206805229187 - discriminator_loss_train: 1.3722554445266724\n",
      "    autoencoder_loss_val: 1.1088947057724 - discriminator_loss_val: 1.4257726669311523\n",
      "Starting epoch 883/1000\n",
      "    autoencoder_loss_train: 1.0794689655303955 - discriminator_loss_train: 1.3964645862579346\n",
      "    autoencoder_loss_val: 1.0899250507354736 - discriminator_loss_val: 1.4480780363082886\n",
      "Starting epoch 884/1000\n",
      "    autoencoder_loss_train: 1.0691792964935303 - discriminator_loss_train: 1.4181127548217773\n",
      "    autoencoder_loss_val: 1.0808452367782593 - discriminator_loss_val: 1.4683890342712402\n",
      "Starting epoch 885/1000\n",
      "    autoencoder_loss_train: 1.055071473121643 - discriminator_loss_train: 1.4437116384506226\n",
      "    autoencoder_loss_val: 1.0671772956848145 - discriminator_loss_val: 1.492927074432373\n",
      "Starting epoch 886/1000\n",
      "    autoencoder_loss_train: 1.0410548448562622 - discriminator_loss_train: 1.4678707122802734\n",
      "    autoencoder_loss_val: 1.0538606643676758 - discriminator_loss_val: 1.516479253768921\n",
      "Starting epoch 887/1000\n",
      "    autoencoder_loss_train: 1.024842619895935 - discriminator_loss_train: 1.4882533550262451\n",
      "    autoencoder_loss_val: 1.0386149883270264 - discriminator_loss_val: 1.5361348390579224\n",
      "Starting epoch 888/1000\n",
      "    autoencoder_loss_train: 1.0197607278823853 - discriminator_loss_train: 1.5018497705459595\n",
      "    autoencoder_loss_val: 1.0349688529968262 - discriminator_loss_val: 1.549984335899353\n",
      "Starting epoch 889/1000\n",
      "    autoencoder_loss_train: 1.0215890407562256 - discriminator_loss_train: 1.5082377195358276\n",
      "    autoencoder_loss_val: 1.0387042760849 - discriminator_loss_val: 1.5569937229156494\n",
      "Starting epoch 890/1000\n",
      "    autoencoder_loss_train: 1.0335314273834229 - discriminator_loss_train: 1.5076532363891602\n",
      "    autoencoder_loss_val: 1.053456425666809 - discriminator_loss_val: 1.557851791381836\n",
      "Starting epoch 891/1000\n",
      "    autoencoder_loss_train: 1.0474047660827637 - discriminator_loss_train: 1.5032598972320557\n",
      "    autoencoder_loss_val: 1.070119023323059 - discriminator_loss_val: 1.5541110038757324\n",
      "Starting epoch 892/1000\n",
      "    autoencoder_loss_train: 1.0605560541152954 - discriminator_loss_train: 1.4943058490753174\n",
      "    autoencoder_loss_val: 1.0852118730545044 - discriminator_loss_val: 1.5456619262695312\n",
      "Starting epoch 893/1000\n",
      "    autoencoder_loss_train: 1.0717899799346924 - discriminator_loss_train: 1.4757194519042969\n",
      "    autoencoder_loss_val: 1.0968372821807861 - discriminator_loss_val: 1.5271291732788086\n",
      "Starting epoch 894/1000\n",
      "    autoencoder_loss_train: 1.088275671005249 - discriminator_loss_train: 1.4503812789916992\n",
      "    autoencoder_loss_val: 1.1130540370941162 - discriminator_loss_val: 1.50165855884552\n",
      "Starting epoch 895/1000\n",
      "    autoencoder_loss_train: 1.102985143661499 - discriminator_loss_train: 1.4263951778411865\n",
      "    autoencoder_loss_val: 1.1284327507019043 - discriminator_loss_val: 1.4774324893951416\n",
      "Starting epoch 896/1000\n",
      "    autoencoder_loss_train: 1.1282598972320557 - discriminator_loss_train: 1.4019560813903809\n",
      "    autoencoder_loss_val: 1.1531803607940674 - discriminator_loss_val: 1.4526591300964355\n",
      "Starting epoch 897/1000\n",
      "    autoencoder_loss_train: 1.1645996570587158 - discriminator_loss_train: 1.3770508766174316\n",
      "    autoencoder_loss_val: 1.1881053447723389 - discriminator_loss_val: 1.4279043674468994\n",
      "Starting epoch 898/1000\n",
      "    autoencoder_loss_train: 1.1980645656585693 - discriminator_loss_train: 1.3551934957504272\n",
      "    autoencoder_loss_val: 1.2201287746429443 - discriminator_loss_val: 1.4060728549957275\n",
      "Starting epoch 899/1000\n",
      "    autoencoder_loss_train: 1.221185326576233 - discriminator_loss_train: 1.3357303142547607\n",
      "    autoencoder_loss_val: 1.2419977188110352 - discriminator_loss_val: 1.386021614074707\n",
      "Starting epoch 900/1000\n",
      "    autoencoder_loss_train: 1.2381863594055176 - discriminator_loss_train: 1.3195207118988037\n",
      "    autoencoder_loss_val: 1.257652759552002 - discriminator_loss_val: 1.369101881980896\n",
      "Starting epoch 901/1000\n",
      "    autoencoder_loss_train: 1.2585821151733398 - discriminator_loss_train: 1.3031766414642334\n",
      "    autoencoder_loss_val: 1.2756531238555908 - discriminator_loss_val: 1.3516552448272705\n",
      "Starting epoch 902/1000\n",
      "    autoencoder_loss_train: 1.265883207321167 - discriminator_loss_train: 1.2917439937591553\n",
      "    autoencoder_loss_val: 1.28092360496521 - discriminator_loss_val: 1.339279055595398\n",
      "Starting epoch 903/1000\n",
      "    autoencoder_loss_train: 1.2604308128356934 - discriminator_loss_train: 1.2870631217956543\n",
      "    autoencoder_loss_val: 1.2745168209075928 - discriminator_loss_val: 1.3344048261642456\n",
      "Starting epoch 904/1000\n",
      "    autoencoder_loss_train: 1.2626001834869385 - discriminator_loss_train: 1.2825243473052979\n",
      "    autoencoder_loss_val: 1.2747950553894043 - discriminator_loss_val: 1.3296787738800049\n",
      "Starting epoch 905/1000\n",
      "    autoencoder_loss_train: 1.2819963693618774 - discriminator_loss_train: 1.2743818759918213\n",
      "    autoencoder_loss_val: 1.290690541267395 - discriminator_loss_val: 1.320831060409546\n",
      "Starting epoch 906/1000\n",
      "    autoencoder_loss_train: 1.2971607446670532 - discriminator_loss_train: 1.2692365646362305\n",
      "    autoencoder_loss_val: 1.302649974822998 - discriminator_loss_val: 1.315070629119873\n",
      "Starting epoch 907/1000\n",
      "    autoencoder_loss_train: 1.2982202768325806 - discriminator_loss_train: 1.2712862491607666\n",
      "    autoencoder_loss_val: 1.3018505573272705 - discriminator_loss_val: 1.3174418210983276\n",
      "Starting epoch 908/1000\n",
      "    autoencoder_loss_train: 1.2849518060684204 - discriminator_loss_train: 1.2801752090454102\n",
      "    autoencoder_loss_val: 1.2884337902069092 - discriminator_loss_val: 1.3274695873260498\n",
      "Starting epoch 909/1000\n",
      "    autoencoder_loss_train: 1.2669878005981445 - discriminator_loss_train: 1.2933075428009033\n",
      "    autoencoder_loss_val: 1.2709702253341675 - discriminator_loss_val: 1.3421666622161865\n",
      "Starting epoch 910/1000\n",
      "    autoencoder_loss_train: 1.2598357200622559 - discriminator_loss_train: 1.3028619289398193\n",
      "    autoencoder_loss_val: 1.2635414600372314 - discriminator_loss_val: 1.3527605533599854\n",
      "Starting epoch 911/1000\n",
      "    autoencoder_loss_train: 1.261389970779419 - discriminator_loss_train: 1.3029563426971436\n",
      "    autoencoder_loss_val: 1.2628145217895508 - discriminator_loss_val: 1.3529303073883057\n",
      "Starting epoch 912/1000\n",
      "    autoencoder_loss_train: 1.2608366012573242 - discriminator_loss_train: 1.2995388507843018\n",
      "    autoencoder_loss_val: 1.2594597339630127 - discriminator_loss_val: 1.3491466045379639\n",
      "Starting epoch 913/1000\n",
      "    autoencoder_loss_train: 1.2524807453155518 - discriminator_loss_train: 1.2989119291305542\n",
      "    autoencoder_loss_val: 1.2485605478286743 - discriminator_loss_val: 1.3482701778411865\n",
      "Starting epoch 914/1000\n",
      "    autoencoder_loss_train: 1.221101999282837 - discriminator_loss_train: 1.3097517490386963\n",
      "    autoencoder_loss_val: 1.2166112661361694 - discriminator_loss_val: 1.3596396446228027\n",
      "Starting epoch 915/1000\n",
      "    autoencoder_loss_train: 1.1888386011123657 - discriminator_loss_train: 1.3262300491333008\n",
      "    autoencoder_loss_val: 1.1848777532577515 - discriminator_loss_val: 1.377181887626648\n",
      "Starting epoch 916/1000\n",
      "    autoencoder_loss_train: 1.1589088439941406 - discriminator_loss_train: 1.3427762985229492\n",
      "    autoencoder_loss_val: 1.1554867029190063 - discriminator_loss_val: 1.394711971282959\n",
      "Starting epoch 917/1000\n",
      "    autoencoder_loss_train: 1.1438528299331665 - discriminator_loss_train: 1.352911114692688\n",
      "    autoencoder_loss_val: 1.139892578125 - discriminator_loss_val: 1.4053360223770142\n",
      "Starting epoch 918/1000\n",
      "    autoencoder_loss_train: 1.129736304283142 - discriminator_loss_train: 1.3586490154266357\n",
      "    autoencoder_loss_val: 1.1247189044952393 - discriminator_loss_val: 1.411262035369873\n",
      "Starting epoch 919/1000\n",
      "    autoencoder_loss_train: 1.1094253063201904 - discriminator_loss_train: 1.3701837062835693\n",
      "    autoencoder_loss_val: 1.104616403579712 - discriminator_loss_val: 1.4234278202056885\n",
      "Starting epoch 920/1000\n",
      "    autoencoder_loss_train: 1.0828917026519775 - discriminator_loss_train: 1.3845919370651245\n",
      "    autoencoder_loss_val: 1.0797079801559448 - discriminator_loss_val: 1.4388028383255005\n",
      "Starting epoch 921/1000\n",
      "    autoencoder_loss_train: 1.0626904964447021 - discriminator_loss_train: 1.3948688507080078\n",
      "    autoencoder_loss_val: 1.061431884765625 - discriminator_loss_val: 1.4497158527374268\n",
      "Starting epoch 922/1000\n",
      "    autoencoder_loss_train: 1.038832187652588 - discriminator_loss_train: 1.3978533744812012\n",
      "    autoencoder_loss_val: 1.0389379262924194 - discriminator_loss_val: 1.4521403312683105\n",
      "Starting epoch 923/1000\n",
      "    autoencoder_loss_train: 1.015451431274414 - discriminator_loss_train: 1.3992366790771484\n",
      "    autoencoder_loss_val: 1.017003059387207 - discriminator_loss_val: 1.45240318775177\n",
      "Starting epoch 924/1000\n",
      "    autoencoder_loss_train: 1.0044893026351929 - discriminator_loss_train: 1.3992516994476318\n",
      "    autoencoder_loss_val: 1.007649302482605 - discriminator_loss_val: 1.4515819549560547\n",
      "Starting epoch 925/1000\n",
      "    autoencoder_loss_train: 0.9980295300483704 - discriminator_loss_train: 1.3995187282562256\n",
      "    autoencoder_loss_val: 1.003224492073059 - discriminator_loss_val: 1.4516398906707764\n",
      "Starting epoch 926/1000\n",
      "    autoencoder_loss_train: 0.9923338890075684 - discriminator_loss_train: 1.3975213766098022\n",
      "    autoencoder_loss_val: 0.9995501041412354 - discriminator_loss_val: 1.4494682550430298\n",
      "Starting epoch 927/1000\n",
      "    autoencoder_loss_train: 0.9884412884712219 - discriminator_loss_train: 1.3938510417938232\n",
      "    autoencoder_loss_val: 0.9978209733963013 - discriminator_loss_val: 1.4457545280456543\n",
      "Starting epoch 928/1000\n",
      "    autoencoder_loss_train: 0.9806752800941467 - discriminator_loss_train: 1.3877010345458984\n",
      "    autoencoder_loss_val: 0.9914931058883667 - discriminator_loss_val: 1.4390051364898682\n",
      "Starting epoch 929/1000\n",
      "    autoencoder_loss_train: 0.9783627390861511 - discriminator_loss_train: 1.3804757595062256\n",
      "    autoencoder_loss_val: 0.990972638130188 - discriminator_loss_val: 1.431142807006836\n",
      "Starting epoch 930/1000\n",
      "    autoencoder_loss_train: 0.9750196933746338 - discriminator_loss_train: 1.3712294101715088\n",
      "    autoencoder_loss_val: 0.9896414279937744 - discriminator_loss_val: 1.4211411476135254\n",
      "Starting epoch 931/1000\n",
      "    autoencoder_loss_train: 0.9735562801361084 - discriminator_loss_train: 1.360559344291687\n",
      "    autoencoder_loss_val: 0.9897364377975464 - discriminator_loss_val: 1.4095017910003662\n",
      "Starting epoch 932/1000\n",
      "    autoencoder_loss_train: 0.9742375612258911 - discriminator_loss_train: 1.3484387397766113\n",
      "    autoencoder_loss_val: 0.9911175966262817 - discriminator_loss_val: 1.3958806991577148\n",
      "Starting epoch 933/1000\n",
      "    autoencoder_loss_train: 0.9811457395553589 - discriminator_loss_train: 1.335397481918335\n",
      "    autoencoder_loss_val: 0.9987629652023315 - discriminator_loss_val: 1.3815969228744507\n",
      "Starting epoch 934/1000\n",
      "    autoencoder_loss_train: 0.9915955066680908 - discriminator_loss_train: 1.3205924034118652\n",
      "    autoencoder_loss_val: 1.0110421180725098 - discriminator_loss_val: 1.3658523559570312\n",
      "Starting epoch 935/1000\n",
      "    autoencoder_loss_train: 1.0041193962097168 - discriminator_loss_train: 1.3047136068344116\n",
      "    autoencoder_loss_val: 1.0257856845855713 - discriminator_loss_val: 1.3492450714111328\n",
      "Starting epoch 936/1000\n",
      "    autoencoder_loss_train: 1.0124801397323608 - discriminator_loss_train: 1.2907711267471313\n",
      "    autoencoder_loss_val: 1.0357757806777954 - discriminator_loss_val: 1.3344557285308838\n",
      "Starting epoch 937/1000\n",
      "    autoencoder_loss_train: 1.009181022644043 - discriminator_loss_train: 1.2813453674316406\n",
      "    autoencoder_loss_val: 1.0332025289535522 - discriminator_loss_val: 1.3238234519958496\n",
      "Starting epoch 938/1000\n",
      "    autoencoder_loss_train: 0.9966025352478027 - discriminator_loss_train: 1.2748425006866455\n",
      "    autoencoder_loss_val: 1.021378755569458 - discriminator_loss_val: 1.3158420324325562\n",
      "Starting epoch 939/1000\n",
      "    autoencoder_loss_train: 0.994502067565918 - discriminator_loss_train: 1.2668452262878418\n",
      "    autoencoder_loss_val: 1.0207401514053345 - discriminator_loss_val: 1.3070056438446045\n",
      "Starting epoch 940/1000\n",
      "    autoencoder_loss_train: 1.0007038116455078 - discriminator_loss_train: 1.2589777708053589\n",
      "    autoencoder_loss_val: 1.0283455848693848 - discriminator_loss_val: 1.2990124225616455\n",
      "Starting epoch 941/1000\n",
      "    autoencoder_loss_train: 1.016151785850525 - discriminator_loss_train: 1.2498645782470703\n",
      "    autoencoder_loss_val: 1.0455421209335327 - discriminator_loss_val: 1.2903101444244385\n",
      "Starting epoch 942/1000\n",
      "    autoencoder_loss_train: 1.0328686237335205 - discriminator_loss_train: 1.2418603897094727\n",
      "    autoencoder_loss_val: 1.064299464225769 - discriminator_loss_val: 1.2831416130065918\n",
      "Starting epoch 943/1000\n",
      "    autoencoder_loss_train: 1.0426828861236572 - discriminator_loss_train: 1.2385077476501465\n",
      "    autoencoder_loss_val: 1.0755045413970947 - discriminator_loss_val: 1.2803905010223389\n",
      "Starting epoch 944/1000\n",
      "    autoencoder_loss_train: 1.0469226837158203 - discriminator_loss_train: 1.240170955657959\n",
      "    autoencoder_loss_val: 1.0805068016052246 - discriminator_loss_val: 1.282517671585083\n",
      "Starting epoch 945/1000\n",
      "    autoencoder_loss_train: 1.042258858680725 - discriminator_loss_train: 1.2423127889633179\n",
      "    autoencoder_loss_val: 1.0770106315612793 - discriminator_loss_val: 1.2843235731124878\n",
      "Starting epoch 946/1000\n",
      "    autoencoder_loss_train: 1.0344994068145752 - discriminator_loss_train: 1.2472789287567139\n",
      "    autoencoder_loss_val: 1.0703175067901611 - discriminator_loss_val: 1.2890450954437256\n",
      "Starting epoch 947/1000\n",
      "    autoencoder_loss_train: 1.02798593044281 - discriminator_loss_train: 1.2514064311981201\n",
      "    autoencoder_loss_val: 1.0651854276657104 - discriminator_loss_val: 1.292916178703308\n",
      "Starting epoch 948/1000\n",
      "    autoencoder_loss_train: 1.0203557014465332 - discriminator_loss_train: 1.2612437009811401\n",
      "    autoencoder_loss_val: 1.0579791069030762 - discriminator_loss_val: 1.3029627799987793\n",
      "Starting epoch 949/1000\n",
      "    autoencoder_loss_train: 1.0086276531219482 - discriminator_loss_train: 1.2758562564849854\n",
      "    autoencoder_loss_val: 1.046187162399292 - discriminator_loss_val: 1.3180845975875854\n",
      "Starting epoch 950/1000\n",
      "    autoencoder_loss_train: 0.9944682121276855 - discriminator_loss_train: 1.295259952545166\n",
      "    autoencoder_loss_val: 1.0314253568649292 - discriminator_loss_val: 1.3383921384811401\n",
      "Starting epoch 951/1000\n",
      "    autoencoder_loss_train: 0.9800958037376404 - discriminator_loss_train: 1.3212350606918335\n",
      "    autoencoder_loss_val: 1.0160276889801025 - discriminator_loss_val: 1.3660398721694946\n",
      "Starting epoch 952/1000\n",
      "    autoencoder_loss_train: 0.9722362160682678 - discriminator_loss_train: 1.3450924158096313\n",
      "    autoencoder_loss_val: 1.0077954530715942 - discriminator_loss_val: 1.3917441368103027\n",
      "Starting epoch 953/1000\n",
      "    autoencoder_loss_train: 0.9698919057846069 - discriminator_loss_train: 1.368281602859497\n",
      "    autoencoder_loss_val: 1.0049304962158203 - discriminator_loss_val: 1.4170770645141602\n",
      "Starting epoch 954/1000\n",
      "    autoencoder_loss_train: 0.9718931913375854 - discriminator_loss_train: 1.3920493125915527\n",
      "    autoencoder_loss_val: 1.0062240362167358 - discriminator_loss_val: 1.443057656288147\n",
      "Starting epoch 955/1000\n",
      "    autoencoder_loss_train: 0.9741635322570801 - discriminator_loss_train: 1.4106764793395996\n",
      "    autoencoder_loss_val: 1.0081324577331543 - discriminator_loss_val: 1.4632070064544678\n",
      "Starting epoch 956/1000\n",
      "    autoencoder_loss_train: 0.9784338474273682 - discriminator_loss_train: 1.4265995025634766\n",
      "    autoencoder_loss_val: 1.0120545625686646 - discriminator_loss_val: 1.480386734008789\n",
      "Starting epoch 957/1000\n",
      "    autoencoder_loss_train: 0.9781753420829773 - discriminator_loss_train: 1.4454089403152466\n",
      "    autoencoder_loss_val: 1.0108022689819336 - discriminator_loss_val: 1.5003329515457153\n",
      "Starting epoch 958/1000\n",
      "    autoencoder_loss_train: 0.9778574705123901 - discriminator_loss_train: 1.4668018817901611\n",
      "    autoencoder_loss_val: 1.0088715553283691 - discriminator_loss_val: 1.522935152053833\n",
      "Starting epoch 959/1000\n",
      "    autoencoder_loss_train: 0.9750705361366272 - discriminator_loss_train: 1.4899801015853882\n",
      "    autoencoder_loss_val: 1.0038368701934814 - discriminator_loss_val: 1.5469573736190796\n",
      "Starting epoch 960/1000\n",
      "    autoencoder_loss_train: 0.976715624332428 - discriminator_loss_train: 1.504878044128418\n",
      "    autoencoder_loss_val: 1.0037732124328613 - discriminator_loss_val: 1.5619087219238281\n",
      "Starting epoch 961/1000\n",
      "    autoencoder_loss_train: 0.9803315997123718 - discriminator_loss_train: 1.5096070766448975\n",
      "    autoencoder_loss_val: 1.0061368942260742 - discriminator_loss_val: 1.5658974647521973\n",
      "Starting epoch 962/1000\n",
      "    autoencoder_loss_train: 0.9907634854316711 - discriminator_loss_train: 1.5010632276535034\n",
      "    autoencoder_loss_val: 1.0162746906280518 - discriminator_loss_val: 1.5564196109771729\n",
      "Starting epoch 963/1000\n",
      "    autoencoder_loss_train: 1.0042768716812134 - discriminator_loss_train: 1.491018295288086\n",
      "    autoencoder_loss_val: 1.0295014381408691 - discriminator_loss_val: 1.5456058979034424\n",
      "Starting epoch 964/1000\n",
      "    autoencoder_loss_train: 1.014351487159729 - discriminator_loss_train: 1.4806385040283203\n",
      "    autoencoder_loss_val: 1.039219617843628 - discriminator_loss_val: 1.534380316734314\n",
      "Starting epoch 965/1000\n",
      "    autoencoder_loss_train: 1.0268973112106323 - discriminator_loss_train: 1.4700050354003906\n",
      "    autoencoder_loss_val: 1.05124831199646 - discriminator_loss_val: 1.5230112075805664\n",
      "Starting epoch 966/1000\n",
      "    autoencoder_loss_train: 1.0361661911010742 - discriminator_loss_train: 1.4591097831726074\n",
      "    autoencoder_loss_val: 1.0596798658370972 - discriminator_loss_val: 1.511117696762085\n",
      "Starting epoch 967/1000\n",
      "    autoencoder_loss_train: 1.0389115810394287 - discriminator_loss_train: 1.445325493812561\n",
      "    autoencoder_loss_val: 1.0615792274475098 - discriminator_loss_val: 1.4957789182662964\n",
      "Starting epoch 968/1000\n",
      "    autoencoder_loss_train: 1.0336889028549194 - discriminator_loss_train: 1.426738977432251\n",
      "    autoencoder_loss_val: 1.0557961463928223 - discriminator_loss_val: 1.4751038551330566\n",
      "Starting epoch 969/1000\n",
      "    autoencoder_loss_train: 1.0361909866333008 - discriminator_loss_train: 1.4048361778259277\n",
      "    autoencoder_loss_val: 1.0584824085235596 - discriminator_loss_val: 1.451277494430542\n",
      "Starting epoch 970/1000\n",
      "    autoencoder_loss_train: 1.0369336605072021 - discriminator_loss_train: 1.384078025817871\n",
      "    autoencoder_loss_val: 1.0588892698287964 - discriminator_loss_val: 1.4286619424819946\n",
      "Starting epoch 971/1000\n",
      "    autoencoder_loss_train: 1.0360331535339355 - discriminator_loss_train: 1.3628098964691162\n",
      "    autoencoder_loss_val: 1.0574675798416138 - discriminator_loss_val: 1.405724048614502\n",
      "Starting epoch 972/1000\n",
      "    autoencoder_loss_train: 1.0321378707885742 - discriminator_loss_train: 1.3430869579315186\n",
      "    autoencoder_loss_val: 1.0521408319473267 - discriminator_loss_val: 1.3843897581100464\n",
      "Starting epoch 973/1000\n",
      "    autoencoder_loss_train: 1.0355381965637207 - discriminator_loss_train: 1.324479103088379\n",
      "    autoencoder_loss_val: 1.0533632040023804 - discriminator_loss_val: 1.3648946285247803\n",
      "Starting epoch 974/1000\n",
      "    autoencoder_loss_train: 1.0422511100769043 - discriminator_loss_train: 1.3061816692352295\n",
      "    autoencoder_loss_val: 1.0576388835906982 - discriminator_loss_val: 1.3461132049560547\n",
      "Starting epoch 975/1000\n",
      "    autoencoder_loss_train: 1.0554375648498535 - discriminator_loss_train: 1.2888528108596802\n",
      "    autoencoder_loss_val: 1.067635178565979 - discriminator_loss_val: 1.3287187814712524\n",
      "Starting epoch 976/1000\n",
      "    autoencoder_loss_train: 1.074806571006775 - discriminator_loss_train: 1.2725321054458618\n",
      "    autoencoder_loss_val: 1.083706021308899 - discriminator_loss_val: 1.3126697540283203\n",
      "Starting epoch 977/1000\n",
      "    autoencoder_loss_train: 1.0923047065734863 - discriminator_loss_train: 1.2578600645065308\n",
      "    autoencoder_loss_val: 1.0983295440673828 - discriminator_loss_val: 1.2980166673660278\n",
      "Starting epoch 978/1000\n",
      "    autoencoder_loss_train: 1.104172945022583 - discriminator_loss_train: 1.2475225925445557\n",
      "    autoencoder_loss_val: 1.1082756519317627 - discriminator_loss_val: 1.2878599166870117\n",
      "Starting epoch 979/1000\n",
      "    autoencoder_loss_train: 1.1100934743881226 - discriminator_loss_train: 1.243520975112915\n",
      "    autoencoder_loss_val: 1.1126519441604614 - discriminator_loss_val: 1.284162163734436\n",
      "Starting epoch 980/1000\n",
      "    autoencoder_loss_train: 1.1142046451568604 - discriminator_loss_train: 1.2435188293457031\n",
      "    autoencoder_loss_val: 1.1151738166809082 - discriminator_loss_val: 1.2845947742462158\n",
      "Starting epoch 981/1000\n",
      "    autoencoder_loss_train: 1.1280750036239624 - discriminator_loss_train: 1.240398645401001\n",
      "    autoencoder_loss_val: 1.126067876815796 - discriminator_loss_val: 1.2822258472442627\n",
      "Starting epoch 982/1000\n",
      "    autoencoder_loss_train: 1.1352012157440186 - discriminator_loss_train: 1.2347215414047241\n",
      "    autoencoder_loss_val: 1.1298588514328003 - discriminator_loss_val: 1.27717924118042\n",
      "Starting epoch 983/1000\n",
      "    autoencoder_loss_train: 1.1501446962356567 - discriminator_loss_train: 1.2206647396087646\n",
      "    autoencoder_loss_val: 1.1402088403701782 - discriminator_loss_val: 1.2634782791137695\n",
      "Starting epoch 984/1000\n",
      "    autoencoder_loss_train: 1.1665937900543213 - discriminator_loss_train: 1.2124370336532593\n",
      "    autoencoder_loss_val: 1.1533191204071045 - discriminator_loss_val: 1.25576913356781\n",
      "Starting epoch 985/1000\n",
      "    autoencoder_loss_train: 1.1734503507614136 - discriminator_loss_train: 1.2135043144226074\n",
      "    autoencoder_loss_val: 1.1588249206542969 - discriminator_loss_val: 1.2576569318771362\n",
      "Starting epoch 986/1000\n",
      "    autoencoder_loss_train: 1.1729249954223633 - discriminator_loss_train: 1.224362850189209\n",
      "    autoencoder_loss_val: 1.158125400543213 - discriminator_loss_val: 1.2696564197540283\n",
      "Starting epoch 987/1000\n",
      "    autoencoder_loss_train: 1.1690078973770142 - discriminator_loss_train: 1.2434864044189453\n",
      "    autoencoder_loss_val: 1.154698133468628 - discriminator_loss_val: 1.2901479005813599\n",
      "Starting epoch 988/1000\n",
      "    autoencoder_loss_train: 1.157404899597168 - discriminator_loss_train: 1.2647078037261963\n",
      "    autoencoder_loss_val: 1.1435353755950928 - discriminator_loss_val: 1.3128869533538818\n",
      "Starting epoch 989/1000\n",
      "    autoencoder_loss_train: 1.1497281789779663 - discriminator_loss_train: 1.282982587814331\n",
      "    autoencoder_loss_val: 1.1354879140853882 - discriminator_loss_val: 1.3327317237854004\n",
      "Starting epoch 990/1000\n",
      "    autoencoder_loss_train: 1.1399908065795898 - discriminator_loss_train: 1.3054819107055664\n",
      "    autoencoder_loss_val: 1.1258554458618164 - discriminator_loss_val: 1.356807827949524\n",
      "Starting epoch 991/1000\n",
      "    autoencoder_loss_train: 1.1346995830535889 - discriminator_loss_train: 1.3257601261138916\n",
      "    autoencoder_loss_val: 1.1202051639556885 - discriminator_loss_val: 1.3785735368728638\n",
      "Starting epoch 992/1000\n",
      "    autoencoder_loss_train: 1.1349701881408691 - discriminator_loss_train: 1.3455253839492798\n",
      "    autoencoder_loss_val: 1.119982123374939 - discriminator_loss_val: 1.400057315826416\n",
      "Starting epoch 993/1000\n",
      "    autoencoder_loss_train: 1.1299885511398315 - discriminator_loss_train: 1.3650853633880615\n",
      "    autoencoder_loss_val: 1.1151067018508911 - discriminator_loss_val: 1.4214892387390137\n",
      "Starting epoch 994/1000\n",
      "    autoencoder_loss_train: 1.1194514036178589 - discriminator_loss_train: 1.3853647708892822\n",
      "    autoencoder_loss_val: 1.104971170425415 - discriminator_loss_val: 1.4436060190200806\n",
      "Starting epoch 995/1000\n",
      "    autoencoder_loss_train: 1.1158392429351807 - discriminator_loss_train: 1.395630121231079\n",
      "    autoencoder_loss_val: 1.1007829904556274 - discriminator_loss_val: 1.4548038244247437\n",
      "Starting epoch 996/1000\n",
      "    autoencoder_loss_train: 1.1132094860076904 - discriminator_loss_train: 1.4007892608642578\n",
      "    autoencoder_loss_val: 1.0976247787475586 - discriminator_loss_val: 1.4604849815368652\n",
      "Starting epoch 997/1000\n",
      "    autoencoder_loss_train: 1.1036566495895386 - discriminator_loss_train: 1.4146406650543213\n",
      "    autoencoder_loss_val: 1.089191198348999 - discriminator_loss_val: 1.4754400253295898\n",
      "Starting epoch 998/1000\n",
      "    autoencoder_loss_train: 1.0895040035247803 - discriminator_loss_train: 1.4343756437301636\n",
      "    autoencoder_loss_val: 1.077761173248291 - discriminator_loss_val: 1.4965029954910278\n",
      "Starting epoch 999/1000\n",
      "    autoencoder_loss_train: 1.0833005905151367 - discriminator_loss_train: 1.4462738037109375\n",
      "    autoencoder_loss_val: 1.073593258857727 - discriminator_loss_val: 1.509568452835083\n",
      "Starting epoch 1000/1000\n",
      "    autoencoder_loss_train: 1.0780341625213623 - discriminator_loss_train: 1.445327639579773\n",
      "    autoencoder_loss_val: 1.0694658756256104 - discriminator_loss_val: 1.509209156036377\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24880\\3521841964.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mautoencoder_losses_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_losses_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoencoder_losses_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_losses_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GAN_denoising_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_attacked_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unattacked_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_attacked_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unattacked_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_attacked_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unattacked_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_attacked_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unattacked_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./trained_gan'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive\\Documents\\notes\\525\\CS525_Projects\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive\\Documents\\notes\\525\\CS525_Projects\\.venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3683\u001b[0m         \u001b[1;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3684\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3685\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m   3686\u001b[0m                 \u001b[1;34m\"You must compile your model before \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3687\u001b[0m                 \u001b[1;34m\"training/testing. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "# conv = create_fully_conv_denoising_autoencoder(X_attacked[0].shape)\n",
    "# conv.fit(X_attacked_train, X_unattacked_train, epochs = num_epochs, batch_size = 256, shuffle = True, validation_data = (X_attacked_test, X_unattacked_test))\n",
    "# conv.save('./trained_fully_conv')\n",
    "\n",
    "# connect = create_fully_conn_denoising_autoencoder(X_attacked[0].shape)\n",
    "# connect.fit(X_attacked_train, X_unattacked_train, epochs = num_epochs, batch_size = 256, shuffle = True, validation_data = (X_attacked_test, X_unattacked_test))\n",
    "# connect.save('./trained_fully_con')\n",
    "\n",
    "autoencoder, discriminator = create_GAN_denoising_autoencoder(X_attacked[0].shape)\n",
    "autoencoder_losses_train, discriminator_losses_train, autoencoder_losses_val, discriminator_losses_val = train_GAN_denoising_autoencoder(X_attacked_train, X_unattacked_train, X_attacked_test, X_unattacked_test, num_epochs, 256, autoencoder, discriminator)\n",
    "gan = autoencoder\n",
    "# history = gan.fit(X_attacked_train, X_unattacked_train, epochs = num_epochs, batch_size = 256, shuffle = True, validation_data = (X_attacked_test, X_unattacked_test))\n",
    "gan.save('./trained_gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
